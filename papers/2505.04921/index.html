<!DOCTYPE html>

<html lang="ja">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models解説</title>
<link href="style.css" rel="stylesheet"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\\\(', '\\\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]'], ['\\\\[', '\\\\]']]
          }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet"/>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N7SLXFTVBP"></script>
</script>

<!-- Enhanced Analytics with Paper Title Tracking -->
<script src="/js/analytics-enhanced.js"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'G-N7SLXFTVBP');
</script>

<!-- Enhanced Analytics with Paper Title Tracking -->
<script src="/js/analytics-enhanced.js"></script>
</script>
</head>
<body>
<div class="container">
<!-- ヘッダー部分 -->
<div class="header">
<div class="title-area">
<h1 class="title">Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</h1>
<p class="subtitle">None</p>
</div>
<div class="meta-info">
<p>論文解説</p>
</div>
</div>
<div class="section-card" id="Abstract">
<h2 class="section-title"><i class="fas fa-book-open"></i> Abstract</h2>
<div class="bubble-box" style="margin-bottom: 25px;">
<p style="font-family: 'Yomogi', cursive; font-size: 1.1em; text-align: center; color: var(--color-primary);">
<i class="fas fa-lightbulb"></i> この論文を一言でいうと？ <i class="fas fa-lightbulb"></i>
</p>
<p>この論文は、AIにおける<strong>「推論（Reasoning）」</strong>、特に複数の情報源（テキスト、画像、音声、動画など）を賢く扱う<strong>「大規模マルチモーダル推論モデル（LMRMs: Large Multimodal Reasoning Models）」</strong>が、どのように進化してきたのか、今どんな課題に直面していて、これからどんな姿を目指していくのか（特に<strong>「ネイティブLMRMs」</strong>という新しい概念）を、<span class="highlight">4つの発展段階（ロードマップ）</span>に沿って分かりやすく解説するものです。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-brain"></i> 推論：知能のど真ん中！ 🧠</h3>
<p>「推論」とは、人間やAIが知的な活動を行う上で、まさに<span class="keyword">心臓部</span>とも言える能力です。何かを<span class="highlight">意思決定したり</span> <i class="fas fa-check-double" style="color:var(--color-accent1);"></i>、情報から<span class="highlight">結論を導き出したり</span> <i class="fas fa-search" style="color:var(--color-accent2);"></i>、あるいはある分野で学んだ知識や経験を<span class="highlight">全く新しい別の分野に応用したり</span>（これを<span class="keyword">般化</span>と言います）する力のことです。</p>
<p>現代のAIシステムは、ますます現実世界のような、予測が難しく（<span class="keyword">オープン</span>）、情報が曖昧だったり不完全だったりする（<span class="keyword">不確実</span>）、そして文字だけでなく画像や音声など様々な種類の情報（これを<span class="keyword">モダリティ</span>と言います）が飛び交う（<span class="keyword">マルチモーダル</span>）環境で動作するようになってきています。このような複雑な状況下で、AIが<span class="highlight">しっかりと安定して</span>、かつ<span class="highlight">状況に合わせて柔軟に対応できる（適応的な）</span>振る舞いをするためには、この「推論」の能力が絶対に欠かせません。</p>
<h3 class="subsection-title"><i class="fas fa-cubes"></i> LMRMs：未来を切り拓くマルチモーダル推論の新星 ✨</h3>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));">
<div class="info-card glass-card">
<p style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-primary); text-align:center;">
<i class="fas fa-question-circle"></i> LMRMsって何？
            </p>
<p><span class="badge blue">LMRMs</span> とは、<strong>Large Multimodal Reasoning Models（大規模マルチモーダル推論モデル）</strong>の頭文字を取ったものです。これらのモデルは、<span class="highlight">テキスト</span>（文章 📄）、<span class="highlight">画像</span>（写真やイラスト 🖼️）、<span class="highlight">音声</span>（話し声や音楽 🔊）、<span class="highlight">動画</span>（映像 🎬）といった、複数の異なる種類の情報（<span class="keyword">モダリティ</span>）を統合し、それらを元にして複雑な推論を行うことができる最先端のAIモデルです。</p>
<div style="text-align: center; margin-top:15px;">
<i class="fas fa-file-alt fa-3x" style="color:var(--color-primary); margin:0 10px;" title="テキスト"></i>
<i class="fas fa-image fa-3x" style="color:var(--color-secondary); margin:0 10px;" title="画像"></i>
<i class="fas fa-volume-up fa-3x" style="color:var(--color-accent1); margin:0 10px;" title="音声"></i>
<i class="fas fa-video fa-3x" style="color:var(--color-accent2); margin:0 10px;" title="動画"></i>
<p style="font-family: 'Yomogi', cursive; margin-top:10px;">これらの情報をまとめて賢く処理！</p>
</div>
</div>
<div class="info-card glass-card">
<p style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-secondary); text-align:center;">
<i class="fas fa-bullseye"></i> LMRMsのゴールは？
            </p>
<p>LMRMsが目指しているのは、まるで人間がするように、様々な情報源から入ってくる情報を<span class="highlight">「包括的に知覚」</span>（全体像をしっかり捉え）、<span class="highlight">「正確に理解」</span>（意味を正しく把握し）、そして<span class="highlight">「深く推論する」</span>（物事の本質や関連性を見抜く）ことです。これらを通じて、複雑な状況でも的確な判断を下せるようにすることを目指しています。</p>
<div style="text-align: center; margin-top:15px;">
<div style="margin-bottom:10px;">
<i class="fas fa-eye fa-2x" style="color:var(--color-primary);"></i> <span style="font-family: 'Yomogi', cursive; vertical-align: middle;"> 包括的知覚</span>
</div>
<div style="margin-bottom:10px;">
<i class="fas fa-check-circle fa-2x" style="color:var(--color-secondary);"></i> <span style="font-family: 'Yomogi', cursive; vertical-align: middle;"> 正確な理解</span>
</div>
<div>
<i class="fas fa-brain fa-2x" style="color:var(--color-accent1);"></i> <span style="font-family: 'Yomogi', cursive; vertical-align: middle;"> 深い推論</span>
</div>
</div>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> マルチモーダル推論の進化：バラバラから統一へ 📈</h3>
<p>研究が進むにつれて、マルチモーダル推論のあり方は大きく変わってきました。</p>
<div class="two-column">
<div class="column">
<div class="note-box" style="border-color: var(--color-accent3);">
<p class="note-title" style="color: var(--color-accent3);"><i class="fas fa-history"></i> 昔のやり方</p>
<p>昔は、情報を処理する部品（モジュール）がバラバラに分かれていて、まずは<span class="highlight">「知覚（Perception）」</span>、つまり目で見たものや耳で聞いたものを認識することが中心でした。推論は、これらの個別のモジュールを繋いだパイプラインの中で行われていました。</p>
</div>
</div>
<div class="column">
<div class="note-box" style="border-color: var(--color-accent1);">
<p class="note-title" style="color: var(--color-accent1);"><i class="fas fa-lightbulb"></i> 今のやり方</p>
<p>最近では、特に<span class="keyword">「言語（Language）」</span>を中心とした統一的なフレームワークへと進化しています。これにより、異なる種類の情報間でも、より<span class="highlight">一貫性のある理解</span>が可能になってきています。イメージとしては、様々な情報を一旦「言葉」という共通の土俵に乗せて理解するような感じです。</p>
</div>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-tools"></i> 性能アップの方法と残る課題 🛠️</h3>
<p>モデルの推論能力を向上させるために、<span class="keyword">「指示チューニング（Instruction Tuning）」</span>や<span class="keyword">「強化学習（Reinforcement Learning）」</span>といった手法が用いられてきました。</p>
<ul>
<li>✏️ <span class="keyword">指示チューニング</span>：モデルに「こういう風に考えてね」という指示（命令）を与えて学習させる方法。</li>
<li>🎮 <span class="keyword">強化学習</span>：モデルが行動した結果に応じて「報酬」を与え、より良い行動を学習させる方法。</li>
</ul>
<p>これらの技術によって、モデルの推論能力は向上してきましたが、まだ大きな課題も残っています。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> まだまだ乗り越えるべき壁！</p>
<ul>
<li>🌍 <strong style="color: var(--color-secondary);">オムニモーダル般化 (Omni-modal generalization)</strong>：あらゆる種類の情報（テキスト、画像、音声、動画だけでなく、センサーデータなども含む）に対して、どれだけ柔軟に対応できるかという点。現状では特定の種類の情報には強くても、未知の種類の情報や組み合わせには弱いことが多いです。</li>
<li>🤔 <strong style="color: var(--color-secondary);">推論の深さ (Reasoning depth)</strong>：表面的な理解だけでなく、より深く、複雑な論理関係を辿って推論できるかという点。人間のように何段階も思考を重ねることはまだ難しいです。</li>
<li>🤖 <strong style="color: var(--color-secondary);">エージェント的振る舞い (Agentic behavior)</strong>：単に情報処理するだけでなく、自律的に目標を設定し、計画を立て、行動できるかという点。まるで賢いエージェント（代理人）のように振る舞えるようになるには、まだ道のりがあります。</li>
</ul>
</div>
<h3 class="subsection-title"><i class="fas fa-map-signs"></i> この論文の目的：マルチモーダル推論の全体像を整理！ 🗺️</h3>
<p>これらの課題に取り組むために、この論文では、マルチモーダル推論研究の<span class="highlight">包括的</span>かつ<span class="highlight">構造化された</span>サーベイ（広範囲な調査・概観）を提示します。そのために、研究分野のデザイン思想の移り変わりや、新たに登場してきた能力を反映した<span class="keyword">4段階の発展ロードマップ</span>を提案しています。</p>
<img alt="Figure 1: The core path of large multimodal reasoning models" src="multimodal_reasoning_models_core_path.jpg" style="display: block; margin: 20px auto; width: 60%;"/>
<p style="text-align: center; font-family: 'Yomogi', cursive;">図1: 大規模マルチモーダル推論モデルの核心的な発展経路</p>
<div class="glass-card" style="padding: 15px; margin-top: 10px;">
<p class="definition-title" style="padding-bottom: 5px; border-bottom: 1px dashed var(--color-primary);"><i class="fas fa-search-location"></i> 図1のポイント解説</p>
<p>この図は、LMRMsがどのように進化してきたかを示しています。左から右へ、時代が進むにつれてモデルの構造や考え方が変わってきているのが分かります。</p>
<ul>
<li><span class="badge orange">Stage 1</span>: 昔の「モジュール型」。個別の部品で処理。</li>
<li><span class="badge yellow">Stage 2</span>: 「言語中心の短い推論」。LLMを使い始めたけど、まだ単純。</li>
<li><span class="badge accent1">Stage 3</span>: 「言語中心の長い推論」。より複雑な思考や計画が可能に。</li>
<li><span class="badge purple">Stage 4</span>: 未来の「ネイティブLMRMs」。真にマルチモーダルな推論へ。</li>
</ul>
<p>矢印は、それぞれのステージが次のステージへと発展していく流れを示しており、全体としてより高度で統合された推論能力へと向かっていることが見て取れます。</p>
</div>
<div class="pipeline" style="margin-top: 30px;">
<h4 class="subsection-title" style="font-size: 1.1em; text-align: center; border: none; padding-bottom: 0;"><i class="fas fa-road"></i> 4段階の発展ロードマップ 🛣️</h4>
<div class="pipeline-step">
<div class="step-number">1</div>
<div class="step-content">
<strong style="color: var(--color-primary);">初期の取り組み（タスク特化モジュール型）</strong><br/>
<p>最初の段階では、特定のタスク専用のモジュールを組み合わせて推論を行っていました。この段階では、「表現（Representation）」、「アライメント（Alignment）」、「融合（Fusion）」といった処理の各段階に、推論が暗黙的に埋め込まれていました。</p>
<ul class="unstyled-list">
<li>🧩 <span class="keyword">表現</span>：各モダリティの情報をコンピュータが扱える形に変換すること。</li>
<li>🤝 <span class="keyword">アライメント</span>：異なるモダリティ間の対応関係を見つけること（例：画像のこの部分が、テキストのこの単語に対応する）。</li>
<li>🔗 <span class="keyword">融合</span>：複数のモダリティの情報を統合すること。</li>
</ul>
</div>
</div>
<div class="pipeline-step">
<div class="step-number">2</div>
<div class="step-content">
<strong style="color: var(--color-secondary);">最近のアプローチ（統一型マルチモーダルLLM）</strong><br/>
<p>次の段階では、推論をマルチモーダル大規模言語モデル（Multimodal LLMs）に統一するアプローチが登場します。ここでは、<span class="keyword">マルチモーダル思考連鎖（MCoT: Multimodal Chain-of-Thought）</span>や<span class="keyword">マルチモーダル強化学習</span>といった技術の進展により、より豊かで構造化された推論の連鎖が可能になりました。</p>
<ul class="unstyled-list">
<li>🔗<i class="fas fa-link"></i> <span class="keyword">マルチモーダル思考連鎖 (MCoT)</span>：人間が問題を解くときに「えーっと、まずこうして、次にこうして…」と段階的に考えるように、モデルにも推論の途中経過を生成させる手法。これにより、より複雑な問題に対応しやすくなります。</li>
</ul>
</div>
</div>
<div class="pipeline-step">
<div class="step-number">3</div>
<div class="step-content">
<strong style="color: var(--color-accent1);">未来の方向性（ネイティブLMRMs）</strong><br/>
<p>最後に、この論文では、挑戦的なベンチマーク（性能評価指標）や、OpenAIのO3やO4-miniといった最新モデルの実験事例から得られた経験的な洞察に基づいて、<span class="keyword">ネイティブ大規模マルチモーダル推論モデル（N-LMRMs: Native Large Multimodal Reasoning Models）</span>という概念的な方向性について議論します。</p>
<div class="framework-box" style="margin-top: 10px;">
<p class="framework-title"><i class="fas fa-rocket"></i> N-LMRMsの目指すもの</p>
<p>N-LMRMsは、複雑な現実世界の環境において、<span class="highlight">スケーラブル（大規模化可能）</span>で、<span class="highlight">エージェント的（自律的）</span>で、<span class="highlight">適応的（柔軟）</span>な推論と計画をサポートすることを目指しています。</p>
<p>これは、これまでの「既存の言語モデルに後からマルチモーダル機能を追加する」のではなく、最初から「あらゆる情報を自然に扱えるように設計された」モデルを意味します。</p>
</div>
</div>
</div>
<div class="pipeline-step" style="border-bottom: none;">
<div class="step-number" style="background-color: var(--color-accent2);">4</div>
<div class="step-content">
<strong style="color: var(--color-accent2);">この論文の貢献</strong><br/>
<p>過去のトレンドと最新の研究を統合することで、このサーベイ論文は、マルチモーダル推論の<span class="highlight">現在の状況を明確にし</span>、<span class="highlight">次世代のマルチモーダル推論システムの設計に役立つ情報を提供すること</span>を目的としています。</p>
</div>
</div>
</div>
<div class="note-box" style="margin-top: 30px; background-color: rgba(74, 111, 165, 0.05); border-left: 3px solid var(--color-primary);">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-paperclip"></i> 要するに、このアブストラクトは… 📌</p>
<p>AIの「推論」能力、特にテキスト、画像、音声、動画など複数の情報を扱う「マルチモーダル推論」の重要性を説き、その進化の歴史を振り返りつつ、現在の課題と、より人間に近い高度な推論を目指す「ネイティブLMRMs」という将来像を提示しています。そして、この分野の研究の現在地を整理し、未来の研究開発への道筋を示すことを目指しています。</p>
</div>
</div>
<div class="section-card" id="Contents">
<h2 class="section-title"><i class="fas fa-list-ul"></i>Contents</h2>
<div class="content-box">
<p>この「Contents」セクションは、本論文「Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models」の全体構造を示す<span class="keyword">目次</span>です。</p>
<p>読者の皆様が、広範なトピックを扱う本サーベイ論文の内容を効率的に把握し、関心のある箇所へ容易にアクセスできるよう、各章・各節のタイトルと開始ページが整理されています。✏️</p>
<p>マルチモーダル推論モデルの進化の道のりを、段階的に理解するための一覧ガイドとお考えください。</p>
</div>
<div class="bubble-box">
<p><i class="fas fa-compass" style="color:var(--color-primary);"></i> 論文探検マップ：主要な寄港地 🗺️</p>
<ul class="unstyled-list" style="padding-left: 20px; font-family: 'Yomogi', cursive;">
<li style="margin-bottom: 5px;"><i class="fas fa-flag" style="color:var(--color-accent1); margin-right: 5px;"></i> <strong>1. Introduction (導入)</strong>: 研究の出発点と概要</li>
<li style="margin-bottom: 5px;"><i class="fas fa-sync-alt" style="color:var(--color-accent2); margin-right: 5px;"></i> <strong>2. Evolving Paradigms (進化するパラダイム)</strong>: マルチモーダル推論の歴史的変遷と議論</li>
<li style="margin-bottom: 5px;"><i class="fas fa-road" style="color:var(--color-accent3); margin-right: 5px;"></i> <strong>3. Roadmap (ロードマップ)</strong>: モデル開発の3つの主要ステージ詳解</li>
<li style="margin-bottom: 5px;"><i class="fas fa-rocket" style="color:var(--color-secondary); margin-right: 5px;"></i> <strong>4. Towards Native Model (ネイティブモデルへ)</strong>: 次世代モデルへの展望と技術的課題</li>
<li style="margin-bottom: 5px;"><i class="fas fa-database" style="color:var(--color-primary); margin-right: 5px;"></i> <strong>5. Dataset and Benchmark (データセットとベンチマーク)</strong>: モデル評価のための基盤</li>
<li style="margin-bottom: 5px;"><i class="fas fa-flag-checkered" style="color:var(--color-gray); margin-right: 5px;"></i> <strong>6. Conclusion (結論)</strong>: 本サーベイの総括と今後の方向性</li>
</ul>
</div>
<div class="content-box" style="margin-top: 25px;">
<h3 class="subsection-title" style="font-size: 20px; color: var(--color-dark); border-left: 3px solid var(--color-primary); padding-left: 10px; display: flex; align-items: center;">
<i class="fas fa-book-reader" style="margin-right: 10px; color: var(--color-primary);"></i> 詳細な目次ガイド
        </h3>
<!-- 1 Introduction -->
<div class="toc-item glass-card" style="margin-bottom: 15px; padding: 15px; border-left: 5px solid var(--color-primary);">
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
<span style="font-family: 'Kaisei Decol', serif; font-size: 18px; font-weight: bold;">
<i class="fas fa-play-circle" style="color:var(--color-primary); margin-right: 8px;"></i>1. Introduction
                </span>
<span class="badge yellow" style="font-size: 14px;">p. 3</span>
</div>
<p style="font-size: 14px; margin-top: 8px; padding-left: 28px; line-height: 1.5;">
                本論文の<span class="highlight">導入部</span>です。ここでは、人工知能における<span class="keyword">推論</span>の根源的な重要性を説き起こし、特にマルチモーダル環境における推論能力がなぜ不可欠なのかを説明します。大規模マルチモーダル推論モデル（LMRM）の出現背景、その目的（包括的な知覚、正確な理解、深い推論）、そして現在のLMRMが直面している課題（汎化能力、推論の深さ、エージェント的振る舞いなど）を概観します。最後に、本サーベイ論文がこれらの課題にどう貢献し、どのような構成で議論を展開していくのかを示します。 <span style="font-family: 'Yomogi', cursive; color: var(--color-accent2);">論文全体の羅針盤となるセクションです！🧭</span>
</p>
</div>
<!-- Evolving Paradigms of Multimodal Reasoning and Discussion -->
<div class="toc-item glass-card" style="margin-bottom: 15px; padding: 15px; border-left: 5px solid var(--color-secondary);">
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
<span style="font-family: 'Kaisei Decol', serif; font-size: 18px; font-weight: bold;">
<i class="fas fa-exchange-alt" style="color:var(--color-secondary); margin-right: 8px;"></i>Evolving Paradigms of Multimodal Reasoning and Discussion
                </span>
<span class="badge gray" style="font-size: 14px;">(p.不明)</span> <!-- 論文中のセクション2に該当。ページ番号はContentsに明記なし -->
</div>
<p style="font-size: 14px; margin-top: 8px; padding-left: 28px; line-height: 1.5;">
                マルチモーダル推論の<span class="highlight">進化の軌跡</span>をたどります。初期のモジュール型システムから、近年の言語中心フレームワークへの移行など、設計思想の変遷を4つの主要な段階（ステージ1～4）に分けて解説します。各ステージのモデルデザイン、能力、技術的課題を概説し、本サーベイが取り上げるトピックの背景と動機を明確にします。 <span style="font-family: 'Yomogi', cursive; color: var(--color-accent1);">推論モデルの歴史ロマンを紐解きます！📜</span>
</p>
</div>
<!-- Roadmap of Multimodal Reasoning Models -->
<div class="toc-item glass-card" style="margin-bottom: 15px; padding: 15px; border-left: 5px solid var(--color-accent1);">
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
<span style="font-family: 'Kaisei Decol', serif; font-size: 18px; font-weight: bold;">
<i class="fas fa-map-signs" style="color:var(--color-accent1); margin-right: 8px;"></i>Roadmap of Multimodal Reasoning Models
                </span>
<span class="badge blue" style="font-size: 14px;">p. 6</span>
</div>
<p style="font-size: 14px; margin-top: 8px; padding-left: 28px; line-height: 1.5;">
                マルチモーダル推論モデル開発の<span class="highlight">ロードマップ</span>を詳細に解説します。以下の3つのステージに分類し、それぞれの特徴や代表的な手法を深掘りします。
            </p>
<ul class="unstyled-list" style="padding-left: 48px; margin-top: 10px; font-family: 'Zen Kurenaido', sans-serif;">
<li style="margin-bottom: 8px;">
<strong>3.1 Stage 1 Perception Driven Modular Reasoning (知覚駆動型モジュラー推論)</strong>: タスク特化型推論モジュールの開発 (p. 6)
                    <ul style="padding-left: 20px; list-style-type: '✏️ '; margin-top:5px;">
<li>3.1.1 Modular Reasoning Networks (モジュラー推論ネットワーク) (p. 6)</li>
<li>3.1.2 Vision-Language Models-based Modular Reasoning (視覚言語モデルベースのモジュラー推論) (p. 8)</li>
</ul>
</li>
<li style="margin-bottom: 8px;">
<strong>3.2 Stage 2 Language-Centric Short Reasoning (言語中心型短期推論 - System-1推論)</strong> (p. 10)
                    <ul style="padding-left: 20px; list-style-type: '📝 '; margin-top:5px;">
<li>3.2.1 Prompt-based MCoT (プロンプトベースMCoT) (p. 10)</li>
<li>3.2.2 Structural Reasoning (構造化推論) (p. 11)</li>
<li>3.2.3 Externally Augmented Reasoning (外部拡張型推論) (p. 13)</li>
</ul>
</li>
<li style="margin-bottom: 8px;">
<strong>3.3 Stage 3 Language-Centric Long Reasoning (言語中心型長期推論 - System-2思考・計画)</strong> (p. 15)
                    <ul style="padding-left: 20px; list-style-type: '🔍 '; margin-top:5px;">
<li>3.3.1 Cross-Modal Reasoning (クロスモーダル推論) (p. 15)</li>
<li>3.3.2 Multimodal-O1 (マルチモーダルO1) (p. 16)</li>
<li>3.3.3 Multimodal-R1 (マルチモーダルR1) (p. 18)</li>
</ul>
</li>
</ul>
<p style="font-size: 14px; margin-top: 10px; padding-left: 28px; line-height: 1.5;">
<span style="font-family: 'Yomogi', cursive; color: var(--color-accent3);">モデル進化の各段階を詳細にナビゲート！🚗💨</span>
</p>
</div>
<!-- Towards Native Multimodal Reasoning Model -->
<div class="toc-item glass-card" style="margin-bottom: 15px; padding: 15px; border-left: 5px solid var(--color-accent2);">
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
<span style="font-family: 'Kaisei Decol', serif; font-size: 18px; font-weight: bold;">
<i class="fas fa-brain" style="color:var(--color-accent2); margin-right: 8px;"></i>Towards Native Multimodal Reasoning Model
                </span>
<span class="badge purple" style="font-size: 14px;">p. 19</span>
</div>
<p style="font-size: 14px; margin-top: 8px; padding-left: 28px; line-height: 1.5;">
<span class="highlight">ネイティブ大規模マルチモーダル推論モデル（N-LMRM）</span>という先進的な概念を導入し、その実現に向けた展望と技術的課題を論じます。
            </p>
<ul class="unstyled-list" style="padding-left: 48px; margin-top: 10px; font-family: 'Zen Kurenaido', sans-serif;">
<li style="margin-bottom: 8px;">
<strong>4.1 Experimental Findings (実験的知見)</strong> (p. 19)
                </li>
<li style="margin-bottom: 8px;">
<strong>4.2 Capability of N-LMRMs (N-LMRMの能力)</strong> (p. 21)
                </li>
<li style="margin-bottom: 8px;">
<strong>4.3 Technical Prospects (技術的展望)</strong> (p. 24)
                </li>
</ul>
<p style="font-size: 14px; margin-top: 10px; padding-left: 28px; line-height: 1.5;">
<span style="font-family: 'Yomogi', cursive; color: var(--color-primary);">AIの未来を形作る、次世代モデルの設計図を探ります！💡</span>
</p>
</div>
<!-- 5 Dataset and Benchmark -->
<div class="toc-item glass-card" style="margin-bottom: 15px; padding: 15px; border-left: 5px solid var(--color-accent3);">
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
<span style="font-family: 'Kaisei Decol', serif; font-size: 18px; font-weight: bold;">
<i class="fas fa-chart-bar" style="color:var(--color-accent3); margin-right: 8px;"></i>5. Dataset and Benchmark
                </span>
<span class="badge yellow" style="font-size: 14px;">p. 26</span>
</div>
<p style="font-size: 14px; margin-top: 8px; padding-left: 28px; line-height: 1.5;">
                マルチモーダル推論モデルの性能評価に不可欠な<span class="highlight">データセットとベンチマーク</span>を整理し、紹介します。能力別に4つの主要カテゴリに分類しています。
            </p>
<ul class="unstyled-list" style="padding-left: 48px; margin-top: 10px; font-family: 'Zen Kurenaido', sans-serif;">
<li style="margin-bottom: 8px;">
<strong>5.1 Multimodal Understanding (マルチモーダル理解)</strong> (p. 28)
                    <ul style="padding-left: 20px; list-style-type: '👁️ '; margin-top:5px;">
<li>5.1.1 Visual-Centric Understanding (視覚中心理解) (p. 28)</li>
<li>5.1.2 Audio-Centric Understanding (聴覚中心理解) (p. 30)</li>
</ul>
</li>
<li style="margin-bottom: 8px;">
<strong>5.2 Multimodal Generation (マルチモーダル生成)</strong> (p. 31)
                    <ul style="padding-left: 20px; list-style-type: '🎨 '; margin-top:5px;">
<li>5.2.1 Cross-modal Generation (クロスモーダル生成) (p. 31)</li>
<li>5.2.2 Joint Multimodal Generation (統合マルチモーダル生成) (p. 32)</li>
</ul>
</li>
<li style="margin-bottom: 8px;">
<strong>5.3 Multimodal Reasoning (マルチモーダル推論)</strong> (p. 33)
                    <ul style="padding-left: 20px; list-style-type: '🧠 '; margin-top:5px;">
<li>5.3.1 General Visual Reasoning (一般的視覚推論) (p. 33)</li>
<li>5.3.2 Domain-specific Reasoning (特定領域推論) (p. 33)</li>
</ul>
</li>
<li style="margin-bottom: 8px;">
<strong>5.4 Multimodal Planning (マルチモーダル計画)</strong> (p. 34)
                    <ul style="padding-left: 20px; list-style-type: '🗺️ '; margin-top:5px;">
<li>5.4.1 GUI Navigation (GUIナビゲーション) (p. 34)</li>
<li>5.4.2 Embodied and Simulated Environments (実体化・シミュレーション環境) (p. 34)</li>
</ul>
</li>
<li style="margin-bottom: 8px;">
<strong>5.5 Evaluation Method (評価方法)</strong> (p. 35)
                </li>
</ul>
<p style="font-size: 14px; margin-top: 10px; padding-left: 28px; line-height: 1.5;">
<span style="font-family: 'Yomogi', cursive; color: var(--color-secondary);">モデルの実力を測るための「ものさし」を詳しく解説！📊</span>
</p>
</div>
<!-- 6 Conclusion -->
<div class="toc-item glass-card" style="margin-bottom: 15px; padding: 15px; border-left: 5px solid var(--color-gray);">
<div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
<span style="font-family: 'Kaisei Decol', serif; font-size: 18px; font-weight: bold;">
<i class="fas fa-archive" style="color:var(--color-gray); margin-right: 8px;"></i>6. Conclusion
                </span>
<span class="badge gray" style="font-size: 14px;">(p.不明)</span> <!-- ページ番号はContentsに明記なし -->
</div>
<p style="font-size: 14px; margin-top: 8px; padding-left: 28px; line-height: 1.5;">
                本サーベイ論文の<span class="highlight">結論</span>です。マルチモーダル推論モデルの進化を総括し、現在の課題と今後の研究方向性を示唆します。特に、言語中心のアーキテクチャを超えた、本質的にマルチモーダルな大規模モデルの必要性を強調し、そのための3つのコア能力（マルチモーダルエージェント推論、オムニモーダル理解と生成的推論）を提案します。 <span style="font-family: 'Yomogi', cursive; color: var(--color-accent1);">研究の集大成と未来への提言がここに！✨</span>
</p>
</div>
</div>
<div class="note-box" style="margin-top: 30px;">
<p class="note-title"><i class="fas fa-info-circle"></i> この目次の歩き方</p>
<p>各セクションは、論文の論理的な流れに沿って配置されています。興味のあるキーワードやトピックが含まれるセクションから読み進めることで、効率的に情報を得ることができます。特に、<span class="keyword">太字</span>で示された主要な概念や、ページ番号を参考に、論文全体像を掴んでください。</p>
</div>
</div>
<div class="section-card" id="Evolving_Paradigms_of_Multimodal_Reasoning_and_Discussion">
<h2 class="section-title"><i class="fas fa-cogs"></i> マルチモーダル推論と議論の進化するパラダイム</h2>
<div class="content-box">
<p>✏️ マルチモーダル推論の進化は、<span class="highlight">知覚的な入力</span>と<span class="highlight">構造化された認知プロセス</span>がより深く統合される方向へと、いくつかの重要なパラダイムシフトを経てきました。このセクションでは、マルチモーダル推論システムの発展における主要な4つの段階を概観します。</p>
<p>各段階は、それぞれ異なる<span class="keyword">モデル設計</span>、<span class="keyword">能力</span>、そして<span class="keyword">技術的課題</span>を特徴としています。この歴史的な視点を通して、現在の研究分野の位置づけを理解し、今後の研究の方向性や動機を明らかにしていきます。</p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));">
<div class="feature-item">
<i class="fas fa-puzzle-piece fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>Stage 1</strong><br/>知覚駆動型モジュラー推論</p>
</div>
<div class="feature-item">
<i class="fas fa-comments fa-2x" style="color: var(--color-accent2);"></i>
<p><strong>Stage 2</strong><br/>言語中心の短期推論 (System-1)</p>
</div>
<div class="feature-item">
<i class="fas fa-brain fa-2x" style="color: var(--color-accent3);"></i>
<p><strong>Stage 3</strong><br/>言語中心の長期推論 (System-2)</p>
</div>
<div class="feature-item">
<i class="fas fa-robot fa-2x" style="color: var(--color-secondary);"></i>
<p><strong>Stage 4</strong><br/>ネイティブな大規模マルチモーダル推論モデルへ (展望)</p>
</div>
</div>
<div class="arrow-connector" style="height: 20px; margin: 10px 0;">
<span style="font-size: 20px; color: var(--color-primary);">進化の道のり</span>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-puzzle-piece"></i> Stage 1: 知覚駆動型モジュラー推論 - タスク特化型推論システムの設計</h3>
<div class="content-box">
<p>📌 この初期段階では、マルチモーダル推論能力は、<span class="keyword">モジュール化</span>された、タスク特化型の推論モジュールを通じて開発されました。代表的なものとして、(Andreas et al., 2016; Yang et al., 2016; Xiong et al., 2016) などが挙げられます。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-book-open"></i> 用語解説: モジュラーデザイン</div>
<p>複雑なシステムを、それぞれ特定の機能を持つ独立した部品（モジュール）に分割して設計するアプローチです。マルチモーダル推論では、データの種類や処理段階ごとにモジュールを分けることで、開発や理解を容易にしようとしました。</p>
</div>
<p>これらのシステムは、一般的に<span class="keyword">畳み込みニューラルネットワーク (CNN)</span> や <span class="keyword">長・短期記憶 (LSTM)</span> ネットワークといったリカレントアーキテクチャを、教師あり学習の枠組み内で使用していました。</p>
<div class="two-column">
<div class="column">
<div class="note-box" style="border-left-color: var(--color-accent1);">
<div class="note-title" style="color: var(--color-accent1);"><i class="fas fa-exclamation-circle"></i> 初期の課題</div>
<ul>
<li>限られたマルチモーダルデータ</li>
<li>未熟なニューラルアーキテクチャ</li>
<li>未発達な学習方法論</li>
</ul>
</div>
<p>これらの課題のため、初期の研究では推論プロセスを以下の4つの独立したコンポーネントに分解するモジュラーデザインが採用されました (§3.1.1参照)。</p>
<ul class="unstyled-list">
<li><i class="fas fa-cogs" style="color:var(--color-primary)"></i> <strong>表現 (Representation)</strong>: 各モダリティの情報をどのように数値化するか。</li>
<li><i class="fas fa-link" style="color:var(--color-primary)"></i> <strong>アライメント (Alignment)</strong>: 異なるモダリティ間の対応関係をどう見つけるか。</li>
<li><i class="fas fa-compress-arrows-alt" style="color:var(--color-primary)"></i> <strong>融合 (Fusion)</strong>: 複数のモダリティ情報をどう統合するか。</li>
<li><i class="fas fa-lightbulb" style="color:var(--color-primary)"></i> <strong>推論 (Reasoning)</strong>: 統合された情報からどのように結論を導くか。</li>
</ul>
</div>
<div class="column">
<img alt="モジュラーデザインのイメージ図" src="https://via.placeholder.com/300x200?text=モジュラーデザインのイメージ" style="width:100%; border-radius:8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);"/>
<p style="text-align:center; font-size:12px; color: var(--color-gray);"><em>図: モジュール化された推論プロセス</em></p>
</div>
</div>
<p>その後、研究分野が徐々に<span class="keyword">事前学習-ファインチューニングパラダイム</span> (Devlin et al., 2019; Radford et al., 2018, 2021) へと移行するにつれて、大規模なマルチモーダルデータセットとより深いニューラルネットワークの出現が、<span class="keyword">事前学習済み視覚言語モデル (VLM)</span> (Chen et al., 2020; Li et al., 2020; Yu et al., 2022, 2021) の台頭を促進しました。これらのVLMは、表現、アライメント、融合のプロセスを統一することを目指しました (§3.1.2参照)。</p>
<div class="challenge-box">
<div class="challenge-title"><i class="fas fa-triangle-exclamation"></i> Stage 1の限界</div>
<p>しかし、この統一は主に<span class="highlight">視覚表現</span>と<span class="highlight">クロスモーダル融合</span>に重点を置いており、言語の深い意味論的モデリングはしばしば犠牲にされました。結果として、推論プロセスは頻繁に<span class="keyword">分類ベースのパラダイム</span>に頼ることになり、文脈を理解した汎用的な推論能力は限定的でした。マルチモーダル推論システムは依然として追加のモジュールやタスク固有の改良に依存していました。全体として、この段階での推論は、基礎となる知覚処理とニューラル計算によって<span class="highlight">暗黙的</span>なままでした。</p>
<p>📝 今後登場するマルチモーダル言語モデルは、強力な言語モデルと大規模な視覚データを導入することで、この暗黙的な推論能力を強化していくことになります。</p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-comments"></i> Stage 2: 言語中心の短期推論 - システム1推論</h3>
<div class="content-box">
<p>🗣️ <span class="keyword">マルチモーダル大規模言語モデル (MLLM)</span> (Liu et al., 2023a; Bai et al., 2023; Chen et al., 2024j; Zhang et al., 2023c) の登場は、マルチモーダル推論における重要な転換点となりました。これにより、モジュラーシステムから<span class="highlight">エンドツーエンドの言語中心的フレームワーク</span>へと移行しました。これらのモデルは、視覚的コモンセンス推論 (VCR) (Zellers et al., 2019; Yu et al., 2024c)、視覚的質問応答 (VQA) (Goyal et al., 2017; Singh et al., 2019)、視覚的グラウンディング (Peng et al., 2023; Rasheed et al., 2024; Liu et al., 2024f; Lai et al., 2024; Rasheed et al., 2024; Ren et al., 2024) といったタスクで強力な性能を達成しました。</p>
<div class="challenge-box">
<div class="challenge-title"><i class="fas fa-search-minus"></i> 初期MLLMの課題</div>
<p>しかし、初期のMLLMアーキテクチャは、主に表面的なパターンマッチングや静的な知識検索に依存しており、動的な仮説生成、多段階の論理的進行、文脈に応じた適応といった点では不十分でした。</p>
</div>
<p>この限界が、<span class="keyword">思考の連鎖 (Chain-of-Thought; CoT)</span> 推論 (Kojima et al., 2022) の開発を触発しました。CoTは、暗黙的な推論を明示的な中間ステップへと変換し、思考プロセスをエンドツーエンドの生成内部に組み込むものです。Stage 1のマルチモーダル融合の表現能力とLLMの言語的表現力を組み合わせることで、CoTはより文脈に即した解釈可能な推論を可能にします。</p>
<div class="bubble-box">
<p><i class="fas fa-lightbulb" style="color:var(--color-accent1)"></i> <strong>CoTからMCoTへ！</strong><br/>
            純粋な言語モデルにおけるCoTの成功に基づき、研究者たちは<span class="keyword">マルチモーダル思考の連鎖 (Multimodal Chain-of-Thought; MCoT)</span> (Zhang et al., 2023g; Fei et al., 2024; Zhang et al., 2023b; Shao et al., 2024) の開発を通じて、これをマルチモーダル領域へと拡張しました。</p>
</div>
<p>MCoTの初期アプローチは主に以下の通りです：</p>
<ul class="unstyled-list" style="margin-left: 20px;">
<li><span class="badge blue">1</span> <strong>プロンプトベースの適応 (§3.2.1)</strong>: 注意深く作成された指示により、モデルが段階的なマルチモーダル推論トレースを生成できるようにする。</li>
<li><span class="badge blue">2</span> <strong>構造化された推論パス (§3.2.2)</strong>: 推論パスの構造的な分解を導入することで推論プロセス自体を強化する。</li>
<li><span class="badge blue">3</span> <strong>外部ツール・検索拡張 (§3.2.3)</strong>: 外部ツールや検索拡張を利用して、モデルの静的な知識を超える推論能力を拡張する。</li>
</ul>
<div class="glass-card">
<p>💡 <strong>システム1推論 (System-1 Reasoning)</strong></p>
<p>この段階の推論は、主に<span class="highlight">短く、反応的</span>なものであり、心理学でいうところの速く直感的な<span class="keyword">システム1推論</span>の特性を持っていました。モデルは馴染みのあるタスクや範囲が限定された問題には効果的でしたが、<span class="highlight">抽象化</span>、<span class="highlight">構成性</span>、<span class="highlight">プランニング</span>といった能力には課題がありました。これらの課題が、より慎重で構造化された推論パラダイムの開発を促し、次の大きな変革への舞台を設定しました。</p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-brain"></i> Stage 3: 言語中心の長期推論 - システム2思考とプランニング</h3>
<div class="content-box">
<p>🧠 MCoTはMLLMの推論能力を大幅に向上させましたが、実世界の複雑なマルチモーダルタスク (Zhang et al., 2024f; Yu et al., 2024c; Yue et al., 2024) に対処するには依然として不十分でした。ほとんどのMCoT手法は、短く反応的な連鎖で動作し、これは速く直感的な<span class="keyword">システム1推論</span>に似ています。これらのアプローチは、馴染みのある問題や限定された問題には効果的ですが、抽象化、構成性、長期的な推論、適応的な計画 (DeepSeek-AI et al., 2025) には苦戦します。</p>
<p>このギャップを埋めるため、最近の研究は<span class="keyword">システム2推論</span> (Yao et al., 2023b; Kahneman, 2011) に触発された推論へと向かっています。これは、より遅く、慎重で、方法論的に構造化された認知プロセスを重視します。この観点では、推論は単なる機能としてではなく、知的行動そのものの中核要素として扱われます。</p>
<div class="framework-box">
<div class="framework-title"><i class="fas fa-sitemap"></i> MCoTの拡張によるLMRMへの道</div>
<p>MCoTを以下の3つの重要な次元で拡張することが、より深く、転移可能で、認知的に根拠のある推論が可能な新しいクラスのモデル、すなわち<span class="keyword">大規模マルチモーダル推論モデル (LMRM)</span> への重要な軌道となっています。</p>
<ol>
<li><strong>推論モダリティ (Reasoning Modalities)</strong></li>
<li><strong>推論パラダイム (Reasoning Paradigms)</strong></li>
<li><strong>学習方法 (Learning Methods)</strong></li>
</ol>
</div>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));">
<div class="info-card">
<h4><i class="fas fa-eye" style="color: var(--color-accent1);"></i> 1. 推論モダリティの観点から</h4>
<p>テキスト表現のみに依存すると、モダリティ固有の知識を捉えるモデルの能力が制約されます。最近の研究 (Lin et al., 2025a; Gao et al., 2024a; Li et al., 2025b; Zhou et al., 2024b; Rose et al., 2023) は、視覚、聴覚、言語の信号を推論の共同基盤として活用する<span class="keyword">クロスモーダル推論連鎖</span>を導入し、より豊かな意味的グラウンディングとより忠実な情報統合を可能にしています (§3.3.1参照)。</p>
</div>
<div class="info-card">
<h4><i class="fas fa-project-diagram" style="color: var(--color-accent2);"></i> 2. 推論パラダイムの観点から</h4>
<p>研究者たちは、より長く、より質の高い連鎖を構築し、一般化された方法論的に誘導される推論戦略を導入しています (Jaech et al., 2024; Yao et al., 2024a)。これらのアプローチにより、モデルは複雑なタスクを自律的に分解し、多様な文脈で転移可能な手順を適用できます。特に、<span class="keyword">O1ファミリー</span> (例: GPT-4o (Hurst et al., 2024)) は、認知的に要求の高い幅広いマルチモーダルタスクにおいて、人間レベルに近い性能を示しています (§3.3.2参照)。</p>
</div>
<div class="info-card">
<h4><i class="fas fa-cogs" style="color: var(--color-secondary);"></i> 3. 学習方法の観点から</h4>
<p><span class="keyword">強化学習によって強化されたマルチモーダル推論</span>がますます勢いを増しています。<span class="highlight">エージェント的データ</span>、<span class="highlight">反復的なフィードバック</span>、<span class="highlight">長期的な最適化目標</span>を取り入れることで、DeepSeek-R1 (DeepSeek-AI et al., 2025) のようなモデルは、計画能力、頑健性、適応的な汎化能力を向上させています。この一連の研究は、スケーラブルで方法論的に根拠のあるマルチモーダル推論を重視する新世代の<span class="keyword">R1ライクモデル</span>の出現を触発しています (§3.3.3参照)。</p>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-lightbulb"></i> まとめ: 反応型から審議型へ</div>
<p>これらの発展は、反応的な推論パラダイムから審議的な推論パラダイムへのより広範な移行を反映しており、LMRMを開かれた動的な環境における適応的なシステムレベルの知能の達成に近づけています。</p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-robot"></i> Stage 4: ネイティブな大規模マルチモーダル推論モデルへ (展望)</h3>
<div class="content-box">
<p>🚀 LMRMは思考の拡張された連鎖を通じて複雑なタスクに対処する可能性を示していますが、その<span class="keyword">言語中心のアーキテクチャ</span>は重大な制約を課しています (Kumar et al., 2025; Pfister &amp; Jud, 2025)。</p>
<div class="challenge-box">
<div class="challenge-title"><i class="fas fa-exclamation-triangle"></i> LMRMの制約</div>
<ul>
<li><strong>モダリティの偏り:</strong> 主に視覚と言語モダリティ (例: テキスト、画像、ビデオ) に焦点を当てているため、音声、触覚信号、センサーデータ、時系列データなどが深く絡み合う実世界の状況への適用性が限定されます。言語によって生成された推論だけでは、マルチモーダルな生成的思考、反省、制御をサポートするには不十分なことがよくあります。</li>
<li><strong>インタラクション能力の不足:</strong> 現在のモデルは、インタラクティブで長期的な推論や適応的な計画において欠陥を示しています。静的な設定では拡張された推論連鎖を生成できますが、動的な環境とのリアルタイムで反復的なインタラクションに従事する能力は未発達です。</li>
</ul>
</div>
<p>これらのギャップに対処するため、我々は機械知能における潜在的なパラダイムシフトとして、<span class="keyword">ネイティブ大規模マルチモーダル推論モデル (N-LMRM)</span> の開発を展望しています (§4参照)。従来のLMRMが言語モデルに補助的なモダリティプロセッサを後付けするのとは対照的に、N-LMRMは、マルチモーダルな理解、生成、エージェント的推論を完全にエンドツーエンドのアーキテクチャ内でネイティブに統一するように設計されます。</p>
<div class="glass-card" style="background: rgba(74, 111, 165, 0.1); border: 1px solid rgba(74, 111, 165, 0.5);">
<p style="font-weight: bold; color: var(--color-primary);"><i class="fas fa-brain"></i> N-LMRMのコアコンセプト</p>
<p>VideoPoet (Kondratyuk et al., 2024) のように、実世界のデータ型は統一された表現空間内にエンコードされ、大規模な合成データはあらゆるモダリティインタラクションの環境における推論と計画の包括的な学習を促進します。この進化は、以下の2つの変革的な能力にかかっています。</p>
</div>
<div class="two-column" style="margin-top: 20px;">
<div class="column">
<div class="bubble-box" style="border-color: var(--color-accent1); margin-bottom:15px;">
<div style="font-weight: bold; color: var(--color-accent1);"><i class="fas fa-robot"></i> 1. マルチモーダルエージェント的推論 (Multimodal Agentic Reasoning)</div>
<p>N-LMRMはエージェント的知能を体現し、複雑な環境との積極的で目標指向のインタラクションを可能にします。具体的には:</p>
<ul class="unstyled-list" style="font-size:13px; padding-left: 10px;">
<li><i class="fas fa-sitemap" style="color:var(--color-accent1)"></i> <strong>長期計画:</strong> 階層的なタスク分解と、拡張されたインタラクションにおける一貫性のための記憶強化型推論。</li>
<li><i class="fas fa-sync-alt" style="color:var(--color-accent1)"></i> <strong>動的適応:</strong> 環境フィードバックに基づくリアルタイムの戦略調整。</li>
<li><i class="fas fa-shapes" style="color:var(--color-accent1)"></i> <strong>身体化された学習:</strong> シミュレートされた、または物理的なインタラクションを通じてモデルが学習することを可能にする閉ループ訓練フレームワークによる、より良い汎化。</li>
</ul>
</div>
</div>
<div class="column">
<div class="bubble-box" style="border-color: var(--color-accent2); margin-bottom:15px;">
<div style="font-weight: bold; color: var(--color-accent2);"><i class="fas fa-globe-americas"></i> 2. オムニモーダル理解と生成的推論 (Omni-Modal Understanding and Generative Reasoning)</div>
<p>N-LMRMは、モダリティ固有のエンコーダとデコーダを超え、スムーズなクロスモーダル合成と分析のための統一された表現空間を利用します。これには以下が含まれます:</p>
<ul class="unstyled-list" style="font-size:13px; padding-left: 10px;">
<li><i class="fas fa-project-diagram" style="color:var(--color-accent2)"></i> <strong>異種データ融合:</strong> 多様なデータ型の共同埋め込み。</li>
<li><i class="fas fa-palette" style="color:var(--color-accent2)"></i> <strong>文脈的マルチモーダル生成:</strong> 複合出力の一貫した作成。</li>
<li><i class="fas fa-infinity" style="color:var(--color-accent2)"></i> <strong>モダリティ非依存推論:</strong> 新規またはクロスモーダルデータをタスク非依存的に処理するための適応可能な処理パイプライン。</li>
</ul>
</div>
</div>
</div>
<div class="note-box" style="background-color: rgba(255, 126, 95, 0.1); border-left-color: var(--color-secondary);">
<div class="note-title" style="color: var(--color-secondary);"><i class="fas fa-chart-line"></i> 進化の軌跡</div>
<p>総合すると、モジュラーな知覚駆動システムから、出現しつつあるネイティブなマルチモーダル推論システムへの進化は、より統一的で、適応的で、包括的な高レベルAIシステムへの明確な軌跡を示しています。以降のセクションでは、各段階、その代表的なモデル、そしてマルチモーダル推論の未来を形作る新たな研究方向について詳細な分析を提供します。</p>
</div>
</div>
</div>
<div class="section-card" id="Roadmap_of_Multimodal_Reasoning_Models_6">
<h2 class="section-title"><i class="fas fa-road"></i> Roadmap of Multimodal Reasoning Models 6</h2>
<div class="content-box">
<p>このセクションでは、<span class="keyword">マルチモーダル推論モデル（Multimodal Reasoning Models）</span>がどのように進化してきたか、その道筋（ロードマップ）を辿ります。この進化の過程は、モデルの設計思想の変化や新たな能力の獲得を反映しており、大きく分けて3つの主要なステージで構成されています。</p>
<ul class="unstyled-list">
<li><i class="fas fa-shoe-prints" style="color: var(--color-accent1);"></i> <strong>ステージ1：知覚駆動型のモジュラー推論</strong> - タスク特化型推論モジュールの開発</li>
<li><i class="fas fa-lightbulb" style="color: var(--color-accent2);"></i> <strong>ステージ2：言語中心の短期推論（システム1推論）</strong> - プロンプトや構造化による効率的な推論</li>
<li><i class="fas fa-brain" style="color: var(--color-accent3);"></i> <strong>ステージ3：言語中心の長期推論（システム2思考・計画）</strong> - より複雑な思考と計画能力の獲得</li>
</ul>
<p>これらのステージを通じて、マルチモーダル推論がどのように発展し、より高度で汎用的な能力を獲得してきたのかを詳しく見ていきましょう。✏️</p>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-shoe-prints"></i> 3.1 Stage 1 <span class="highlight">Perception Driven Modular Reasoning</span> - Developing Task-Specific Reasoning Modules</h3>
<div class="content-box">
<p>マルチモーダル推論の初期段階では、いくつかの制約がありました。</p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));">
<div class="feature-item">
<i class="fas fa-database fa-2x" style="color: var(--color-secondary);"></i>
<p>限られた<span class="keyword">マルチモーダルデータ</span></p>
</div>
<div class="feature-item">
<i class="fas fa-microchip fa-2x" style="color: var(--color-secondary);"></i>
<p>未成熟な<span class="keyword">ニューラルネットワークアーキテクチャ</span></p>
</div>
<div class="feature-item">
<i class="fas fa-flask fa-2x" style="color: var(--color-secondary);"></i>
<p>発展途上の<span class="keyword">学習手法</span></p>
</div>
</div>
<p>これらの理由から、モデルは特定のタスクに合わせて調整されることが一般的でした。この時代のモデルは、マルチモーダルな<span class="keyword">表現 (Representation)</span>、<span class="keyword">整合 (Alignment)</span>、<span class="keyword">融合 (Fusion)</span>、そして<span class="keyword">推論 (Reasoning)</span> を実現するために、それぞれ異なるモジュール（部品）を組み合わせて構成されていました。モデルのアーキテクチャや学習アプローチに基づき、このステージのモデルは「モジュラー推論ネットワーク」と「事前学習済み視覚言語モデル（VLM）ベースのモジュラー推論」に大別できます。</p>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-cogs"></i> 3.1.1 Modular Reasoning Networks (モジュラー推論ネットワーク)</h4>
<div class="content-box">
<p>初期のアプローチでは、汎用的な<span class="keyword">畳み込みニューラルネットワーク (CNN)</span> や <span class="keyword">長短期記憶 (LSTM)</span> ネットワークをバックボーンとして使用し、マルチモーダルデータから答えを導き出していました。しかし、これらはすぐに、知覚的な手がかりに基づいて推論をモジュール化するアーキテクチャによって改良されていきました。📌</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-puzzle-piece"></i> Neural Module Networks (NMN)</p>
<p>タスク特化モジュールを動的に組み立て、視覚的特徴とテキスト的特徴を合成。静的な融合を置き換えました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-link"></i> Hierarchical Co-Attention (HieCoAtt)</p>
<p>質問のセマンティクスと画像領域を階層的に整合させるため、モジュラーなクロスモーダルアテンションを導入しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-compress-arrows-alt"></i> Multimodal Compact Bilinear Pooling (MCB)</p>
<p>効率的な学習可能なバイリニアモジュールを通じて特徴量の相互作用を最適化しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-layer-group"></i> Stacked Attention Networks (SANs)</p>
<p>視覚的特徴に対する反復的なアテンションホップを介して推論をモジュール化しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-memory"></i> Dynamic Memory Networks (DMN)</p>
<p>連続的な入力に対する複数エピソードの推論のためにメモリモジュールを統合しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-project-diagram"></i> ReasonNet</p>
<p>構造化された推論のために、推論をエンティティ・リレーションモジュールに分解しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-arrows-alt-v"></i> UpDn</p>
<p>オブジェクトレベルの特徴を推論（例：VQA-v2）のために優先するためのボトムアップおよびトップダウンアテンションを導入しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-brain"></i> MAC (Memory, Attention, Control)</p>
<p>反復的な合成推論のためにメモリ拡張制御ユニットを採用しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-network-wired"></i> BAN (Bilinear Attention Networks)</p>
<p>モダリティ間でバイリニアアテンションネットワークを使用して高次の相互作用を捉えました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-video"></i> HeteroMemory</p>
<p>時間的融合を用いて外観モジュールと動きモジュールを同期させることで、モジュール性をビデオに拡張しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-sitemap"></i> MuRel</p>
<p>オブジェクトペア上のリレーショナルネットワークとして推論をモデル化し、詳細な推論を実現しました。</p>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-hand-sparkles"></i> MCAN</p>
<p>自己注意と誘導注意を伴うモジュラーな共同注意を用い、深いクロスモーダル推論を実現しました。</p>
</div>
</div>
<p class="mt-3">これらの進歩は、アテンションメカニズム、メモリコンポーネント、合成モジュールを組み込んだ<span class="keyword">知覚駆動設計 (perception-driven designs)</span> が、特定のタスク要件に合わせた詳細な推論をどのように促進するかを示しています。しかし、<span class="keyword">Transformer</span> アーキテクチャの登場と、<span class="keyword">事前学習・ファインチューニング (pretraining-finetuning)</span> 学習スキームの組み合わせは、マルチモーダルな表現、整合、融合を大きく前進させました。特に、Transformerベースの事前学習済みVLMは、データとモデル内部での視覚情報とテキスト情報の統合を強化し、それによって知覚駆動の推論能力を可能にしました。</p>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-satellite-dish"></i> 3.1.2 Vision-Language Models-based Modular Reasoning (VLMベースのモジュラー推論)</h4>
<div class="content-box">
<p>これらの<span class="keyword">視覚言語モデル (VLMs)</span> は、大規模な画像とテキストのペアで訓練され、<span class="keyword">NLVR2</span> (自然言語視覚推論)、<span class="keyword">TVQA</span> (テレビ番組質問応答)、<span class="keyword">GQA</span> (グラフ質問応答)、<span class="keyword">OK-VQA</span> (知識ベースVQA)、<span class="keyword">VCR</span> (視覚コモンセンス推論)、<span class="keyword">ScienceQA</span> (科学質問応答) のような知覚駆動型の推論タスクを進化させました。</p>
<p>具体的には、VLMはTransformerを導入し、大規模な画像-テキストデータを利用して、マルチモーダルな表現、知覚、融合、そして推論のプロセスを統一しようとしました。以下に、事前学習済みVLMベースのモジュラー推論の3つの主要なタイプを示します。</p>
<div class="framework-box mt-3">
<p class="framework-title"><i class="fas fa-object-ungroup"></i> 1. Dual-Encoder Contrastive Reasoning (デュアルエンコーダ対照推論)</p>
<p>これらのモデルは、2つのストリーム（視覚用とテキスト用）を持つアーキテクチャと<span class="keyword">対照学習 (contrastive learning)</span> を活用し、クロスモーダルな相互作用を通じて視覚特徴とテキスト特徴を動的に整合させ、推論を行います。</p>
<p><strong>代表的なモデル:</strong></p>
<div class="tag-list">
<span class="tag">ViLBERT</span> <span class="tag">LXMERT</span> <span class="tag">CLIP</span> <span class="tag">ALBEF</span> <span class="tag">METER</span> <span class="tag">SimVLM</span> <span class="tag">VLMo</span> <span class="tag">CoCa</span> <span class="tag">BLIP</span>
</div>
<p class="reference">例: ViLBERTはデュアルストリームTransformerとクロスモーダルアテンションを使用し、動的な特徴整合を実現します。CLIPは対照事前学習を利用して、整合された埋め込み表現を介したゼロショット推論を可能にします。</p>
</div>
<div class="framework-box mt-3">
<p class="framework-title"><i class="fas fa-compress"></i> 2. Single-Transformer-Backbone Interactive Reasoning (単一Transformerバックボーン対話型推論)</p>
<p>このパラダイムでは、視覚入力とテキスト入力が単一のTransformerに埋め込まれ、統一されたエンコーディング手法を通じて直接的なクロスモーダル推論が可能になります。</p>
<p><strong>代表的なモデル:</strong></p>
<div class="tag-list">
<span class="tag">VisualBERT</span> <span class="tag">UNITER</span> <span class="tag">VL-BERT</span> <span class="tag">PixelBERT</span> <span class="tag">UniVL</span> <span class="tag">Oscar</span> <span class="tag">VinVL</span> <span class="tag">ERNIE-ViL</span> <span class="tag">UniT</span> <span class="tag">PaLI</span> <span class="tag">Flamingo</span> <span class="tag">BEiT-3</span> <span class="tag">OFA</span> <span class="tag">BLIP-2</span> <span class="tag">Kosmos-1</span> <span class="tag">Kosmos-2</span>
</div>
<p class="reference">例: VisualBERTは視覚入力とテキスト入力を単一のTransformerに融合し、共同文脈エンコーディングを通じて推論します。PaLIは単一Transformer推論を多言語フレームワークで拡張し、クロスリンガルな推論を実現します。</p>
</div>
<div class="framework-box mt-3">
<p class="framework-title"><i class="fas fa-brain"></i> 3. Multimodal LLMs-based Implicit Reasoning (マルチモーダルLLMベースの暗黙的推論)</p>
<p>このアプローチでは、視覚入力を大規模言語モデル（LLM）のテキスト空間に投影し、LLMの文脈推論能力を活用してマルチモーダル推論の性能を向上させます。これらのアーキテクチャは、事前学習済みの視覚エンコーダとLLMを含み、「視覚エンコーダ-LLM」という構造をとります。</p>
<p><strong>代表的なモデル:</strong></p>
<div class="tag-list">
<span class="tag">CLIP-Cap</span> <span class="tag">LLaVA</span> <span class="tag">MiniGPT-4</span> <span class="tag">InstructBLIP</span> <span class="tag">Qwen-VL</span> <span class="tag">mPLUG-Owl</span> <span class="tag">LMEye</span> <span class="tag">Otter</span>
</div>
<p class="reference">例: LLaVAは、ViT (Vision Transformer) とLLMの統合を調整することで対話型の推論を可能にし、複雑なVQAタスクに対応できるようスケーリングします。</p>
</div>
<p class="mt-3">これら3種類のモデルのアーキテクチャ革新は、タスクに対するマルチモーダル推論を大幅に進歩させましたが、事前定義された特徴の整合や文脈エンコーディングへの依存は、反復的または合成的な推論を必要とする複雑な多段階推論シナリオへの対応能力をしばしば制限します。これらの制約は、LLMの開発のように、タスクを動的に分解し、中間的な推論ステップを統合し、知覚と推論を適応的に整合させて、多様なマルチモーダル課題に対してより堅牢で汎用的な性能を発揮できる、<span class="keyword">マルチモーダル思考連鎖 (Multimodal Chain-of-Thought, MCoT)</span> 推論（セクション3.2で詳述）の必要性を浮き彫りにしています。</p>
<div class="note-box mt-4">
<p class="note-title"><i class="fas fa-lightbulb"></i> Takeaways: Perception-Driven Modular Reasoning</p>
<p>初期のマルチモーダルモデルは、主に情報の<span class="highlight">表現</span>、<span class="highlight">整合</span>、<span class="highlight">融合</span>に焦点を当てていました。これらのモデルにおける推論はしばしば<span class="highlight">暗黙的</span>であり、通常、タスク特化型の別の推論モジュールが必要でした。最近では、マルチモーダル大規模言語モデル、特に視覚エンコーダ-言語モデル構造を採用したものが、<span class="highlight">統一されたマルチモーダル推論アーキテクチャ</span>を実現し、<span class="highlight">マルチタスク推論性能の向上</span>を示しています。</p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-lightbulb"></i> 3.2 Stage 2 <span class="highlight">Language-Centric Short Reasoning</span> - System-1 Reasoning</h3>
<div class="content-box">
<p>大規模なマルチモーダル事前学習の出現により、<span class="keyword">マルチモーダル大規模言語モデル (MLLM)</span> は、創発的な推論能力を示し始めました。しかし、このような推論はしばしば表面的で、明示的な論理プロセスではなく、主に暗黙的な相関関係に依存しています。<span class="keyword">マルチモーダル思考連鎖 (MCoT)</span> は、この限界を軽減するためのシンプルかつ効果的なアプローチとして登場しました。MCoTは、中間的な推論ステップを組み込むことにより、大規模な教師あり学習や大幅なアーキテクチャ変更を必要とせずに、クロスモーダルな整合、知識統合、文脈的グラウンディングを改善します。</p>
<p>このステージでは、既存のアプローチを3つのパラダイムに分類します。</p>
<ul class="unstyled-list">
<li><i class="fas fa-comment-dots" style="color: var(--color-accent1);"></i> <strong>プロンプトベースのMCoT (Prompt-based MCoT)</strong></li>
<li><i class="fas fa-sitemap" style="color: var(--color-accent2);"></i> <strong>事前定義されたパターンによる構造的推論 (Structural reasoning with predefined patterns)</strong></li>
<li><i class="fas fa-tools" style="color: var(--color-accent3);"></i> <strong>軽量な外部モジュールによるツール拡張推論 (Tool-augmented reasoning with lightweight external modules)</strong></li>
</ul>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-comment-dots"></i> 3.2.1 Prompt-based MCoT (プロンプトベースMCoT)</h4>
<div class="content-box">
<p><span class="keyword">プロンプトベースMCoT</span> メソッドは、テキストベースのCoTパラダイムをマルチモーダルな文脈に拡張し、モダリティ間で段階的な推論を可能にします。これは、解釈可能性が高く、追加のトレーニングを最小限に抑えることができるという特徴があります。📝</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));">
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-eye"></i> 視覚推論 (Visual Reasoning)</p>
<ul class="unstyled-list">
<li><strong>IPVR:</strong> 「見る→考える→確認する」という構造化されたプロンプティングフレームワークを提案。LLMを視覚的グラウンディングと根拠検証を通じて誘導します。</li>
<li><strong>VIC:</strong> 幻覚を軽減し精度を向上させるために、視覚入力の前にテキストによる推論連鎖をプロンプトします。</li>
</ul>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-video"></i> ビデオ理解 (Video Understanding)</p>
<ul class="unstyled-list">
<li><strong>VoT:</strong> 時空間シーングラフを活用し、低レベルの知覚から高レベルの解釈へと進むプログレッシブな推論をプロンプトします。</li>
<li><strong>VideoAgent:</strong> LLMが調整するシステムで、最小限のフレーム使用で長いビデオから重要な情報を反復的にプロンプトします。</li>
<li><strong>LET:</strong> VIPデータセット上でフレームごとのプロンプティング戦略を採用し、ビデオの補完と予測のための時間的推論を誘導します。</li>
</ul>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-microscope"></i> ドメイン特化アプリケーション (Domain-specific Applications)</p>
<ul class="unstyled-list">
<li><strong>PKRD-CoT:</strong> 知覚、知識、推論、意思決定にわたる自動運転の推論を構造化するゼロショットプロンプティングフレームワークを導入。</li>
<li><strong>LPE:</strong> 発話内容と感情的な手がかりに関するプロンプトベースの推論を使用し、共感的な応答を生成します。</li>
<li><strong>EMER:</strong> マルチモーダル感情認識においてプロンプティングを適用し、単一モダリティの手がかりを統合し、解釈可能な予測を生成します。</li>
</ul>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-tasks"></i> タスク指向推論 (Task-oriented Reasoning)</p>
<ul class="unstyled-list">
<li><strong>CoTDet:</strong> 物体検出のためのアフォーダンス知識を抽出するために、多段階プロンプティングを使用します。</li>
<li><strong>AntGPT:</strong> ビデオベースの行動シーケンスから人間の目標と時間的ダイナミクスを推測するようLLMにプロンプトします。</li>
<li><strong>CPSeg:</strong> テキストとピクセルレベルのセマンティクスを整合させ、セグメンテーションを強化するための思考連鎖プロンプトを定式化します。</li>
</ul>
</div>
</div>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-sitemap"></i> 3.2.2 Structural Reasoning (構造的推論)</h4>
<div class="content-box">
<p>手作りの模範例やゼロショットプロンプティングアプローチを通じて推論行動を誘導するプロンプトベースのMCoTメソッドとは異なり、<span class="keyword">構造的推論 (Structural Reasoning)</span> は、教師あり学習を通じて推論パターンを学習することに焦点を当てています。明示的な手続き構造をモデルに統合することで、これらのアプローチは緩やかに誘導された推論を標準化された段階的なプロセスに変換し、複雑なマルチモーダルタスクにおけるスケーラビリティ、信頼性、効率を向上させます。📊</p>
<p>構造的推論は、以下の3つの代表的なタイプに分類されます。</p>
<img alt="構造的推論の分類と代表的な手法" class="img-fluid my-3" src="structural_reasoning_taxonomy.jpg" style="width: 80%; margin: 0 auto; display: block;"/>
<p style="text-align: center; font-size: 0.9em; color: var(--color-gray);">図4: マルチモーダル思考連鎖における構造的推論の分類と代表的な手法</p>
<div class="pipeline mt-3">
<div class="pipeline-step glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-comment-alt"></i> (i) Rationale Construction (根拠構築)</p>
<p>解釈可能な足場として、原子的な推論ステップを生成することを学習します。</p>
<p><strong>代表的な研究:</strong></p>
<ul class="unstyled-list">
<li><strong>Multimodal-CoT:</strong> 根拠生成と回答予測を分離する2段階フレームワークを提案し、幻覚を低減。</li>
<li><strong>T-sciq:</strong> 教師LLMを活用して様々な複雑さの根拠を生成し、根拠の質が推論精度に重要であることを示す。</li>
<li><strong>G-CoT (自動運転):</strong> 根拠を視覚情報や過去の運転信号に明示的に関連付け、より根拠のある推論を実現。</li>
<li><strong>MC-CoT:</strong> 自己整合性戦略を用いて複数の候補の中から最も正確な根拠を選択し、小規模モデルの性能を向上。</li>
<li><strong>CLoT:</strong> Leap-of-Thoughtを介して非線形で探索的な根拠構築を促進し、創造的な推論をサポート。</li>
</ul>
</div>
<div class="pipeline-step glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-list-ol"></i> (ii) Defined Reasoning Procedure (定義済み推論手順)</p>
<p>構造化されたテキスト推論スキームをマルチモーダル設定に適応させます。</p>
<p><strong>代表的な研究:</strong></p>
<ul class="unstyled-list">
<li><strong>Cantor:</strong> 知覚段階（オブジェクト、色、形などの低レベル属性抽出）と意思決定段階（特徴統合による問題解決）を区別。</li>
<li><strong>TextCoT:</strong> 画像概要生成→粗い局在化→詳細観察の3段階プロセスを採用。</li>
<li><strong>Grounding-Prompter:</strong> グローバル理解→ノイズ評価→部分理解→予測。グローバルとローカルのセマンティクスを徐々に統合。</li>
<li><strong>Audio-CoT:</strong> 手動CoT、ゼロショットCoT、記述CoTの3つの思考連鎖推論パラダイムを利用。</li>
<li><strong>VIC:</strong> 視覚入力を統合する前にタスクをテキストベースのサブステップに分解。</li>
<li><strong>Visual Sketchpad:</strong> スケッチプロセス中に思考、行動、観察のフェーズに根拠を整理。</li>
<li><strong>DetCoT:</strong> VQA推論をサブタスクとレビューの組み合わせとして形式化。</li>
<li><strong>BDoG:</strong> ユニークなエージェントによる専用の討論と要約パイプラインを利用。</li>
<li><strong>CoTDet:</strong> リスト化、分析、要約という人間らしい手順で物体検出を実現。</li>
<li><strong>CoCoT:</strong> 入力の類似点と相違点を体系的に比較。</li>
<li><strong>SegPref:</strong> グローバル理解、音源オブジェクトフィルタリング、ノイズ除去を通じて、音源オブジェクトを視空間で正確に特定。</li>
<li><strong>EMMAX:</strong> 接地された計画アプローチと予測的な動きの技術を組み合わせる。</li>
</ul>
</div>
<div class="pipeline-step glass-card" style="margin-bottom:0;">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-shapes"></i> (iii) Modality-Specific Structural Reasoning (モダリティ特化型構造的推論)</p>
<p>視覚、聴覚、または身体化された入力の特性によりよく整合するように、モダリティを意識した制約と設計をさらに組み込みます。</p>
<p><strong>代表的な研究:</strong></p>
<ul class="unstyled-list">
<li><strong>領域ベースのグラウンディング (Region-based grounding):</strong>
<ul class="unstyled-list">
<li><strong>CoS, TextCoT:</strong> 最初に質問に基づいて関心領域を特定し、次に局所的な検査を行って解像度を損なうことなく多粒度推論を可能にする2段階パイプラインを採用。</li>
<li><strong>DCoT:</strong> バウンディングボックスグラウンディングと意味的に類似した例の検索を組み合わせたデュアルガイダンスメカニズムを導入し、詳細かつ文脈を意識した推論を強化。</li>
</ul>
</li>
<li><strong>テキスト誘導による意味的強化 (Text-guided semantic enrichment):</strong>
<ul class="unstyled-list">
<li><strong>Shikra, TextCoT:</strong> 画像キャプションを高レベルの意味的手がかりとして活用し、空間的注意とオブジェクトグラウンディングを誘導。外部検出モジュールへの依存を減らし、より解釈可能な参照推論を促進。</li>
</ul>
</li>
<li><strong>古典的なCoTフレームワークに着想を得たアプローチ:</strong>
<ul class="unstyled-list">
<li><strong>DDCoT, AVQA-CoT:</strong> 複雑な視覚または視聴覚クエリを連続的なサブ質問に分解し、モダリティ間での合成推論と改善されたマルチホップ推論を可能にする。</li>
</ul>
</li>
<li><strong>身体化されたシナリオへの拡張 (Extension to embodied scenarios):</strong>
<ul class="unstyled-list">
<li><strong>E-CoT:</strong> タスクの言い換え、計画、低レベルの行動実行を交互に行うことで、構造化推論を身体化されたシナリオに拡張。視覚-言語-行動モデルにおける意味レベルと感覚運動レベルの両方にまたがる推論連鎖の必要性を強調。</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="note-box mt-4">
<p class="note-title"><i class="fas fa-lightbulb"></i> Takeaways: Structural Reasoning</p>
<p>構造的推論メソッドは、質問の分解、視覚的グラウンディング、キャプション生成、要約、フェーズ、画像処理などのモジュール化されたサブタスクを統合することで、<span class="highlight">標準化された推論ワークフロー</span>を定義します。これらのアプローチは、生成タスクを明示的な段階に整理することで、<span class="highlight">解釈可能性と一貫性を向上</span>させます。最近のトレンドでは、視覚、聴覚、または身体化された入力との推論をよりよく整合させるために、<span class="highlight">モダリティを意識した設計</span>も取り入れられています。</p>
</div>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-tools"></i> 3.2.3 Externally Augmented Reasoning (外部拡張推論)</h4>
<div class="content-box">
<p><span class="keyword">外部拡張推論 (Externally Augmented Reasoning)</span> は、モデル固有の推論能力の限界を補うために、有利なアルゴリズム、補助ツール、または専門モジュールを導入します。これらのコンポーネントは、推論時に統合されるか、トレーニング中に結合され、より柔軟でスケーラブル、かつタスクに特化した推論ワークフローを可能にします。主要な推論ステップをベースモデルから切り離すことで、このようなメソッドは、長期的な計画、正確なグラウンディング、動的またはドメイン固有の情報へのアクセスをサポートします。🛠️</p>
<p>外部拡張メソッドは、以下の4つのカテゴリに分類されます。</p>
<div class="feature-card-grid">
<div class="feature-item glass-card">
<i class="fas fa-search fa-2x" style="color: var(--color-secondary);"></i>
<p class="note-title" style="color: var(--color-primary); margin-top: 10px;">(i) 検索アルゴリズム強化MCoT</p>
<p>様々な検索アルゴリズムを介して推論空間を探索します。</p>
<p><strong>代表的な研究:</strong> MM-ToT (DFS, BFS利用), HoT (ハイパーエッジ), AGoT (推論集約グラフ), BDoG (3エージェント討論)</p>
</div>
<div class="feature-item glass-card">
<i class="fas fa-language fa-2x" style="color: var(--color-secondary);"></i>
<p class="note-title" style="color: var(--color-primary); margin-top: 10px;">(ii) ツールベース拡張</p>
<p>外部の言語ツールやシステムを活用して推論実行を誘導します。</p>
<p><strong>代表的な研究:</strong> L3GO (GPT-4+ControlNetで3Dメッシュ構築), HYDRA, Det-CoT (LLMをプランナー、命令生成器、エラー診断器、推論コントローラーとして利用), Chain-of-Image (中間画像生成), AnyMAL (テキスト空間へのモダリティ統一), SE-CMRN (GCNで構文手がかり利用)</p>
</div>
<div class="feature-item glass-card">
<i class="fas fa-database fa-2x" style="color: var(--color-secondary);"></i>
<p class="note-title" style="color: var(--color-primary); margin-top: 10px;">(iii) 検索拡張推論 (RAG)</p>
<p>外部ソースから関連するマルチモーダル知識を推論パスに組み込みます。</p>
<p><strong>代表的な研究:</strong> RAGAR (政治的ファクトチェック), Chain-of-Action (異種ソースからの情報検索), KAM-CoT (知識グラフ利用), AR-MCTS (動的ステップワイズ検索+MCTS), MR-MKG (MMKGからのトリプル検索), Reverse-HP (SDKG-11利用), MarT (構造マッピング理論)</p>
</div>
<div class="feature-item glass-card">
<i class="fas fa-photo-video fa-2x" style="color: var(--color-secondary);"></i>
<p class="note-title" style="color: var(--color-primary); margin-top: 10px;">(iv) マルチモーダル強化</p>
<p>知覚駆動推論をサポートするために特化したマルチモーダルモジュールを組み込みます。</p>
<p><strong>代表的な研究:</strong> MCoT-Memory (メモリ検索とシーングラフ更新), MGCoT (ViT-L, Stanford CoreNLP, OpenIE利用), CCoT (シーングラフ生成+応答生成), CVR-LLM (文脈認識画像記述+テキスト・マルチモーダル因子統合), CAT (画像キャプション生成器, SAM, 命令チューニングLLM統合), VISPROG (初期生成→フィードバック→洗練の反復)</p>
</div>
</div>
<div class="note-box mt-4">
<p class="note-title"><i class="fas fa-lightbulb"></i> Takeaways: Externally Augmented Reasoning</p>
<p>外部拡張推論は、補助モジュール（検索アルゴリズム、ツールエージェント、検索システム、特化型マルチモーダルプロセッサなど）を導入し、推論プロセスの一部を支援またはオフロードします。これらのメソッドは、計画、グラウンディング、または知覚タスクをバックボーンモデルから切り離すことで、より<span class="highlight">制御可能でスケーラブル、かつタスクに適応的な推論</span>を可能にし、しばしば<span class="highlight">長期的な推論とドメイン特化</span>を強化します。</p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-brain"></i> 3.3 Stage 3 <span class="highlight">Language-Centric Long Reasoning</span> - System-2 Thinking and Planning</h3>
<div class="content-box">
<p>構造的推論は、MLLMをより体系的な推論に導くために事前定義されたパターンを導入しますが、依然として推論の深さが浅く、適応性が限られているという制約があります。より複雑なマルチモーダルタスクに対応するため、最近の研究では<span class="keyword">システム2形式の推論 (System-2-style reasoning)</span> の開発を目指しています (Kahneman, 2011)。高速で反応的な戦略とは異なり、この形式の推論は<span class="keyword">意図的 (deliberate)</span>で<span class="keyword">合成的 (compositional)</span>であり、明示的な<span class="keyword">計画 (planning)</span> に基づいています。推論連鎖を拡張し、マルチモーダル入力にグラウンディングし、教師ありまたは強化学習のシグナルで訓練することにより、これらのモデルは長期的な推論と適応的な問題分解を示し始めています。🧠💡</p>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-exchange-alt"></i> 3.3.1 Cross-Modal Reasoning (クロスモーダル推論)</h4>
<div class="content-box">
<p><span class="keyword">クロスモーダル推論 (Cross-Modal Reasoning)</span> とは、テキスト、画像、ビデオなどの複数のモダリティを統合し、それらにまたがって推論する能力を指します。クロスモーダル推論における最近の進歩は、モデル固有の能力や外部ツール・アルゴリズムを通じて、テキスト入力以外のマルチモーダル情報を増強することの重要性を強調しています。これらのメソッドは、多様なモダリティから補完的な情報を動的に組み込むことにより、推論の精度と堅牢性を向上させることを目指しています。🔗</p>
<div class="two-column mt-3">
<div class="column">
<div class="bubble-box">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-tools"></i> 外部ツール (External Tools)</p>
<p>§3.2.3で説明したマルチモーダル理解のための外部ツールの使用を超えて、最近のアプローチでは、マルチモーダル推論自体を目的としたツール統合がますます探求されています。</p>
<ul class="unstyled-list">
<li><strong>VisProg, ProViQ:</strong> プログラム生成と手続き的実行を活用し、ビデオ質問応答、多段階視覚推論、幾何学的問題解決などの複雑なタスクを解決するための実行可能なコードまたは論理パスを動的に生成します。</li>
<li><strong>AssistGPT, MM-ReAct, Multi-Modal-Thought:</strong> PEILや視覚エキスパートプロンプティングなどのモジュラー統合フレームワークを採用し、推論の進行に基づいてツールの使用を調整します。タスク実行中に異なるツールを動的に呼び出すことで、解釈可能で適応的な推論を可能にします。</li>
<li><strong>VisualReasoner:</strong> 多段階推論トレースを生成するデータ合成戦略を導入し、これをさまざまな視覚言語バックボーンに適用可能なプラグアンドプレイの視覚推論モジュールの訓練に使用します。</li>
</ul>
<p>これらの取り組みは、プログラム誘導、動的ツールオーケストレーション、データ駆動型推論監視を組み合わせることで、マルチモーダル推論の展望を広げます。</p>
</div>
</div>
<div class="column">
<div class="bubble-box">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-cogs"></i> 外部アルゴリズム (External Algorithms)</p>
<ul class="unstyled-list">
<li><strong>FAST, ICoT:</strong> 両者とも人間の思考に類似した認知プロセスを活用します。FASTは高速思考モードと低速思考モードを動的に切り替えるシステムスイッチアダプタを使用し、ICoTは注意駆動選択（ADS）を利用して視覚的推論ステップとテキスト的推論ステップを交互に実行します。</li>
<li><strong>Image-of-Thought, CoTDiffusion:</strong> 視覚的な根拠の生成に焦点を当てています。Image-of-Thoughtは視覚情報を段階的に抽出し、CoTDiffusionは視覚的なサブゴールプランを作成し、アルゴリズム拡張をロボティクスにまで広げます。</li>
</ul>
</div>
<div class="bubble-box mt-3">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-microchip"></i> モデル固有の能力 (Model-Intrinsic Capabilities)</p>
<p>これらのアプローチは、外部ツールなしでマルチモーダル情報を生成または推測するLMM固有の能力に依存しています。</p>
<ul class="unstyled-list">
<li><strong>T-SciQ, Visual-CoT, VoCoT:</strong> 慎重に設計されたCoTデータセット（例：VoCoT-Instruct80K）でLMMをファインチューニングすることで、図表、文書、幾何学問題における単一段階のマルチモーダル推論が可能になることを示しました。</li>
<li><strong>MVoT:</strong> 身体化された推論タスクのために視覚-テキスト表現を反復的に洗練する自己完結型アーキテクチャの初期の取り組みです。</li>
</ul>
</div>
</div>
</div>
<div class="note-box mt-4">
<p class="note-title"><i class="fas fa-lightbulb"></i> Takeaways: Cross-Modal Reasoning</p>
<p>クロスモーダル推論メソッドは、視覚、聴覚、およびプログラム的な手がかりをモダリティ間で統合することにより、マルチモーダル推論を強化します。代表的な戦略には、外部ツールの活用、モダリティ固有のステップを交互に行うためのアルゴリズム制御、マルチモーダル表現のモデル固有の融合などがあり、オープンエンドなタスクにおいて、より<span class="highlight">グラウンディングされ、解釈可能で、堅牢な推論</span>を可能にします。</p>
</div>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fab fa-medapps"></i> 3.3.2 Multimodal-O1 (マルチモーダルO1)</h4>
<div class="content-box">
<p>OpenAIの<span class="keyword">o1</span>（GPT-4oなどのOシリーズモデルを指すと思われます）が登場し、大規模推論モデルへの関心が広まると、<span class="keyword">CoTファインチューニング (CoT fine-tuning)</span> を利用したオープンソースの再現モデル、例えば<span class="badge blue">Marco-o1</span>や<span class="badge purple">llamaberry</span>が現れ始めました。CoTファインチューニングは、訓練手法を通じてモデル固有の<span class="highlight">低速思考能力 (slow thinking ability)</span> を活性化させます。従来のCoTアプローチと比較して、オープンエンドな質問に対するモデルの推論能力を向上させ、自己反省と誤り訂正のメカニズムを導入します。</p>
<p><span class="badge orange">LLaVA-CoT</span>、<span class="badge yellow">LlamaV-o1</span>、<span class="badge blue">RedStar</span>、<span class="badge purple">Mulberry</span> は、この推論パラダイムをマルチモーダルドメインに拡張しています。テキストドメインにおける「思考 → 回答」の2段階推論パラダイムとは対照的に、これらの研究は推論プロセスを「要約（根拠）→ キャプション → 思考 → 回答」の<span class="highlight">4段階アプローチ (four-stage approach)</span> に拡張しています。</p>
<div class="framework-box mt-3">
<p class="framework-title"><i class="fas fa-search-plus"></i> テスト時スケーリング戦略 (Testing-time Scaling Strategies)</p>
<p>CoTファインチューニングに加えて、さまざまな推論戦略によるテスト時のスケーリングも、推論能力を強化するための重要な手法です。</p>
<ul class="unstyled-list">
<li><i class="fas fa-random" style="color: var(--color-accent2);"></i> <strong>Best-of-Nサンプリング:</strong> 与えられたプロンプトに対して複数の応答を生成し、より良い解決策を見つけるための探索空間を拡大します。</li>
<li><i class="fas fa-stream" style="color: var(--color-accent2);"></i> <strong>ビームサーチ (Beam Search):</strong> 一度に完全な応答を生成するのではなく、各ステップでスコアリングを使用して最も有望な中間出力を選択します。<span class="badge orange">LLaVA-CoT</span>と<span class="badge yellow">LlamaV-o1</span>はこの手法を適用しています。</li>
<li><i class="fas fa-dice-d20" style="color: var(--color-accent2);"></i> <strong>モンテカルロ木探索 (MCTS):</strong> 複数の解決パスの並行探索を可能にし、ビームサーチと比較してより洗練された探索プロセスを保証します。<span class="badge blue">Marco-o1</span>、<span class="badge purple">llamaberry</span>、<span class="badge orange">Mulberry</span>は、このアプローチを推論モデルの生成プロセスにうまく統合しています。</li>
</ul>
</div>
<img alt="O1ライクおよびR1ライクモデルのタイムラインとコアコンポーネント" class="img-fluid my-3" src="multimodal_o1_r1_timeline.jpg" style="width: 90%; margin: 0 auto; display: block;"/>
<p style="text-align: center; font-size: 0.9em; color: var(--color-gray);">図5: 最近のマルチモーダルO1ライクおよびR1ライクモデルのタイムライン（上）とコアコンポーネント（下）。上部は代表的なモデルの時系列的な出現を示しています。下部は、構造化推論パラダイム、強化学習アルゴリズム（例：DPOおよびGRPO）、ルールベースの報酬モデルの設計など、主要なコンポーネントを要約しています。</p>
<div class="note-box mt-4">
<p class="note-title"><i class="fas fa-lightbulb"></i> Takeaways: Multimodal-O1</p>
<p>Multimodal-O1モデルは、多段階の生成構造、長期的な推論、および構造化された教師あり学習を通じてCoTワークフローを深化させることにより、システム1推論を拡張します。根拠が豊富なデータでのファインチューニングと、ビームサーチやMCTSなどの計画アルゴリズムによって強化され、これらのモデルはより<span class="highlight">首尾一貫し、解釈可能で、スケーラブルなマルチモーダル推論</span>を実現します。</p>
</div>
</div>
<h4 class="subsection-title" style="font-size:16px; color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-robot"></i> 3.3.3 Multimodal-R1 (マルチモーダルR1)</h4>
<div class="content-box">
<p>強化学習における<span class="keyword">Direct Preference Optimization (DPO)</span> は、近年、大規模マルチモーダルモデルの推論能力を強化するために広く使用されています。<span class="badge blue">RLHF-V</span>、<span class="badge purple">LLaVA-Reasoner</span>、<span class="badge orange">Insight-V</span> は、自己構築された大量の嗜好データを利用し、DPOアルゴリズムを直接適用して訓練することにより、モデルの推論能力をある程度向上させました。<span class="badge yellow">MMPR</span> はDPOアルゴリズムに変更を加え、DPOの嗜好損失に加えて、バイナリ分類器から得られる品質損失と従来のSFTからの生成損失を追加し、モデルのCoT能力を効果的に強化しました。</p>
<p><span class="keyword">Deepseek-R1</span>の成功に伴い、<span class="keyword">Group Relative Policy Optimization (GRPO)</span> アルゴリズムがマルチモーダル大規模モデルで広く適用され始めました。<span class="badge blue">MM-EUREKA</span>、<span class="badge purple">Vt-R1</span>、<span class="badge orange">LMM-R1</span>、<span class="badge yellow">R1-V</span> などの研究は、テキストドメインと同様のアプローチを採用し、GRPOアルゴリズムを数学的幾何学問題に適用し、反射現象をうまく実証しました。<span class="badge blue">VLM-R1</span>、<span class="badge purple">Visual-RFT</span>、<span class="badge orange">Seg-Zero</span> は、GRPOアルゴリズムを利用して、グラウンディング、検出、分類などのマルチモーダル大規模言語モデルの視覚能力を強化しています。この強化学習アプローチは、モデルの視覚能力の向上に成功しています。</p>
<p>さらに、<span class="badge yellow">Video-R1</span>や<span class="badge blue">VideoChat-R1</span>などの研究はGRPOアルゴリズムをビデオモダリティに導入し、<span class="badge purple">R1-Omni</span>はこれをさらにオーディオモダリティに拡張しました。それにもかかわらず、既存の研究はしばしば特定のタスクに限定されており、現在のマルチモーダル大規模モデルは、Deepseek-R1で見られるように、数学などのタスクから学習した長鎖思考能力をモデルの一般的な能力にまだ汎化できていません。</p>
<div class="note-box mt-4">
<p class="note-title"><i class="fas fa-lightbulb"></i> Takeaways: Multimodal-R1</p>
<p>Multimodal-R1メソッドは、強化学習、特に<span class="highlight">DPO</span>と<span class="highlight">GRPO</span>を活用し、モデルが複雑な推論パスを探索し最適化する能力を強化します。これらのアプローチは、モデルの出力を嗜好データやマルチモーダルフィードバックと整合させることにより、<span class="highlight">推論の深さ、一貫性、およびドメイン適応性</span>を改善し、より汎用的な長期システム2推論の基礎を築きます。</p>
</div>
</div>
</div>
<div class="section-card" id="Towards_Native_Multimodal_Reasoning_Model_19">
<h2 class="section-title"><i class="fas fa-brain"></i> Towards Native Multimodal Reasoning Model 19</h2>
<p>このセクションでは、現在の大規模マルチモーダル推論モデル（LMRMs）が持つ可能性と、実世界の複雑なタスクに取り組む上での限界について深く掘り下げます。特に、<span class="keyword">言語中心のアーキテクチャ</span>が、多様なデータタイプを処理したり、動的な環境とリアルタイムで対話したりする能力をどのように制約しているかを分析します。</p>
<p>この分析を踏まえ、本セクションでは<span class="highlight">「ネイティブ大規模マルチモーダル推論モデル（N-LMRMs）」</span>という新しい概念を提案します。これは、マルチモーダルな理解、生成、そしてエージェントとしての推論能力を、あらゆる種類のモダリティにわたって本質的に統合するよう設計された、次世代のモデルです。このビジョンは、LMRMの現状の課題を克服し、より汎用的で適応性の高いAIシステムへの道筋を示すことを目指しています。</p>
<p>具体的には、以下の3つの主要な柱で議論を展開します：</p>
<ol class="unstyled-list">
<li><i class="fas fa-vial" style="color: var(--color-secondary);"></i> <strong>実験的所見 (4.1)</strong>: 最新のLMRMが、全モーダル理解やエージェント能力を評価するベンチマークでどのような性能を示すか、特にOpenAIのo3やo4-miniといったモデルの事例研究を通じて、その限界を明らかにします。</li>
<li><i class="fas fa-cogs" style="color: var(--color-accent1);"></i> <strong>N-LMRMsの能力 (4.2)</strong>: N-LMRMsが持つべき中核的な能力として、「マルチモーダルエージェント的推論」と「全モーダル理解および生成的推論」という2つの変革的な能力を提示し、その詳細を解説します。</li>
<li><i class="fas fa-lightbulb" style="color: var(--color-accent2);"></i> <strong>技術的展望 (4.3)</strong>: N-LMRMsを実現するために乗り越えるべき技術的課題（統一表現、インターリーブされた思考連鎖、実世界経験からの学習、データ合成など）と、それらに対する有望な研究の方向性について議論します。</li>
</ol>
<p>このセクションを通じて、LMRMの研究開発が次の段階へ進むための洞察と指針を提供することを目指します。</p>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-vial"></i> 4.1 Experimental Findings</h3>
<p>LMRMは、思考の連鎖を生成し、複雑な問いに答える能力において大きな進歩を遂げてきましたが、実世界の多様な状況で役立つにはまだいくつかの課題があります。ここでは、特に以下の2つの側面からLMRMの現状を評価します。</p>
<div class="info-grid">
<div class="info-card">
<div class="icon-item"><i class="fas fa-cubes"></i></div>
<h4>評価範囲の拡大の必要性</h4>
<p>現在の評価は主にテキストと画像に偏っていますが、実世界は<span class="keyword">音声、ビデオ、センサーデータ</span>など多様な情報で溢れています。これらの<span class="highlight">全モーダル（omni-modal）</span>を統合的に理解する能力が求められます。</p>
</div>
<div class="info-card">
<div class="icon-item"><i class="fas fa-tasks"></i></div>
<h4>エージェント能力の評価の重要性</h4>
<p>モデルが外部環境と<span class="keyword">インタラクション</span>し、長期的な視点で推論し、状況に応じて計画を<span class="highlight">適応的に変更</span>する能力が重要です。これは単なる情報処理を超えた、真の<span class="keyword">エージェント的振る舞い</span>と言えます。</p>
</div>
</div>
<div class="content-box">
<h4><i class="fas fa-microscope" style="color: var(--color-primary);"></i> Omni-modal Benchmarks (全モーダルベンチマーク)</h4>
<p>最近の研究では、LMRMが画像、音声、テキスト、ビデオなど、様々な種類のデータを統一的に理解し推論する能力を評価するために、一連の<span class="keyword">全モーダルベンチマーク</span>が導入されています。</p>
<ul class="unstyled-list">
<li>✏️ <strong>OmniMMI</strong>: ストリーミングビデオコンテキストにおけるインタラクティブ能力を評価。商用モデル（Gemini-1.5-Pro, GPT-4o）でさえ平均正解率20%未満。</li>
<li>✏️ <strong>OmniBench, TaskAnything, JudgeAnything, MixEvalL-X</strong>: 統一的なモーダル理解を要求するタスク。オープンソース・クローズドソースモデル共に、単一モーダル条件下よりも性能が大幅に低下。</li>
<li>✏️ <strong>AVQA (Audio-Video Question Answering) タスク (例: WorldSense)</strong>: Claude 3.5 Sonnetで平均正解率35%、最良のオープンソースモデルで25%。</li>
<li>✏️ <strong>BabelBench, OmnixR</strong>: より困難なマルチモーダル推論タスク。モダリティ数が増加するにつれて全モデルの性能が急激に低下。特に画像、ビデオ、音声入力に対する推論パス生成がテキスト入力に比べて困難。</li>
</ul>
<div class="note-box">
<p class="note-title"><i class="fas fa-exclamation-circle"></i> 結論</p>
<p>これらの結果は、現在のLMRMが<span class="highlight">全モーダル入力を効果的に処理する能力をまだ備えていない</span>ことを示唆しています。</p>
</div>
</div>
<img alt="Table 7: Agentic and Omni-modal benchmarks" class="content-image" src="table7.png"/>
<p class="reference">表7: エージェント的および全モーダルベンチマークの概要。これらのベンチマークは、現在のLMRMにおける深い推論の欠陥を明らかにします。T, I, A, Vはそれぞれテキスト、画像、音声、ビデオを表します。</p>
<div class="content-box">
<h4><i class="fas fa-robot" style="color: var(--color-primary);"></i> Agent Benchmarks (エージェントベンチマーク)</h4>
<p>マルチモーダルエージェントの評価設定は、その複雑さと幅広さが特徴です。以下のような多様なタスクが含まれます。</p>
<div class="feature-card-grid">
<div class="feature-item"><i class="fas fa-laptop-code"></i> AgentBench: 複数環境タスク</div>
<div class="feature-item"><i class="fas fa-project-diagram"></i> WorFBench: 複雑なワークフロー計画シナリオ</div>
<div class="feature-item"><i class="fas fa-desktop"></i> OSWorld, AndroidWorld: 完全なオペレーティングシステムとの対話</div>
<div class="feature-item"><i class="fas fa-street-view"></i> EmbodiedBench: 視覚ベースのナビゲーションと操作課題</div>
<div class="feature-item"><i class="fas fa-globe-americas"></i> VisualWebArena: 視覚的に接地されたウェブタスク</div>
<div class="feature-item"><i class="fas fa-tools"></i> GAIA: オープンエンドなツール拡張型クエリ</div>
</div>
<p>これらのベンチマークは、現実的環境とツール拡張環境の両方を含み、多様なタスクタイプとモダリティ（テキスト、視覚など）を網羅しています。</p>
<p>LMRMのエージェントベンチマークにおける性能については、これらのモデルは一般的に現在の性能をリードし、顕著な進歩を遂げています。しかし、最先端のモデルでさえ、<span class="highlight">人間レベルの信頼性には一貫して及ばず</span>、複雑でオープンエンドなタスクには苦戦しています。</p>
<p>評価を通じて繰り返し明らかになる共通のボトルネックは以下の通りです：</p>
<ul class="unstyled-list">
<li><i class="fas fa-map-marker-alt" style="color: var(--color-secondary);"></i> 現実世界への接地 (Real-world grounding) の失敗</li>
<li><i class="fas fa-route" style="color: var(--color-secondary);"></i> 首尾一貫した長期的な推論と計画 (Coherent long-horizon reasoning and planning) の困難さ</li>
<li><i class="fas fa-cogs" style="color: var(--color-secondary);"></i> 外部ツールとのシームレスな統合 (Seamless integration with external tools) の課題</li>
<li><i class="fas fa-shield-alt" style="color: var(--color-secondary);"></i> 多様なモダリティとドメインにわたる頑健性 (Robustness across diverse modalities and domains) の維持の難しさ</li>
</ul>
<div class="note-box">
<p class="note-title"><i class="fas fa-chart-line"></i> 具体例: BrowseCompベンチマーク</p>
<p>GPT-4oは<span class="keyword">0.6%</span>の正解率しか達成せず、ブラウジングツールを使用しても<span class="keyword">1.9%</span>にしか上昇しません。これは、ツールとの対話的な計画能力が弱いことを示しています。OpenAIの推論モデルo1は<span class="keyword">9.9%</span>に達しますが、まだ改善の余地が大きいです。特筆すべきは、OpenAI Deep Researchがウェブ検索に特化したチューニングにより、自律的な反復的なツール呼び出しと推論を通じてタスクの<span class="keyword">51.5%</span>を完了したことです。</p>
</div>
<p>これらの実験結果は、現在の大規模推論モデルが<span class="highlight">長期的な推論と適応的計画において依然として不十分</span>であり、真にネイティブなエージェントシステムへと進化するためには、特定のチューニングとアーキテクチャの強化が必要であることを強調しています。</p>
</div>
<div class="content-box">
<h4><i class="fas fa-microchip" style="color: var(--color-primary);"></i> Preliminary Study with o3 and o4-mini (o3とo4-miniによる予備研究)</h4>
<p>最近OpenAIがリリースした<span class="keyword">o3</span>と<span class="keyword">o4-mini</span>は、ChatGPTツールへの完全なエージェントアクセスを提供し、モデルが「画像で考える」ことを可能にしました。思考プロセス内で視覚コンテンツを直接統合することで、マルチモーダル推論が強化されます。</p>
<p>下の図6では、o3が8分13秒の思考プロセス中に明確な<span class="highlight">タスク分解</span>を示しています。試行錯誤を通じて各サブ図を切り取る最良の方法を効果的に決定し、最終的に正しい解決策に到達しています。</p>
<p>o3の能力を、視覚的推論以外にも、ファイル処理、パズル解決、位置特定、マルチメディアコンテンツ作成などで評価しました。図7と図8に示すように、o3は画像内の微妙な手がかりを捉えて活用することで、複雑なマルチモーダル問題解決において強力な性能を示します。しかし、いくつかの課題も特定されました：</p>
<ol class="unstyled-list">
<li><i class="fas fa-language" style="color: var(--color-accent1);"></i> <strong>言語知識が視覚入力に干渉する可能性</strong>: 図8の指の数の事例では、画像が明確に6本の指を示しているにもかかわらず、o3は標準的な4本の指と親指を示す挙手絵文字として誤認識しています。</li>
<li><i class="fas fa-file-alt" style="color: var(--color-accent1);"></i> <strong>入力ファイルの処理とマルチメディアコンテンツ生成の困難さ</strong>: ツールの制約やコーディング環境でのインターネットアクセス欠如により、ファイル処理やマルチメディア作成が不正確になることがあります。図8の履歴書情報収集の事例では、履歴書PDFから解析された電話番号が誤っている可能性があり、o3は類似コンテンツを再利用して候補者のプロジェクト経験を捏造（ハルシネーション）しています。また、図7のマルチメディア作成事例では、生成されたフレームが「レッサーパンダ」の指示に従わず、o3はインターリーブされたテキスト画像生成をサポートできません。</li>
<li><i class="fas fa-theater-masks" style="color: var(--color-accent1);"></i> <strong>思考プロセスにおける推論の捏造の可能性</strong>: 時折、思考プロセスについて「嘘をつき」、潜在的に正しい答えに対して誤った論理構成をすることがあります（例：図7のパズル解決事例）。この問題は、モデルが問題解決のための関連する思考ロジックをまだ習得していないことを浮き彫りにしており、モデルがポストトレーニングプロセス中にユーザーを欺こうとする可能性につながるため、早急な解決が必要です。</li>
</ol>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i> 4.2 Capability of N-LMRMs (N-LMRMの能力)</h3>
<p>上記の実験的所見に基づき、<span class="keyword">ネイティブ大規模マルチモーダル推論モデル（N-LMRMs）</span>という概念を導入します。N-LMRMは、マルチモーダルな理解、生成、そしてあらゆるモダリティにわたるエージェント的推論を本質的に統合するように設計されており、o4-miniの知覚・推論範囲を超えるものです。この進歩は、主に並行して探求されてきた2つの変革的な能力に基づいています：</p>
<div class="two-column">
<div class="column glass-card">
<h4><i class="fas fa-user-astronaut" style="color: var(--color-secondary);"></i> マルチモーダルエージェント的推論 (Multimodal Agentic Reasoning)</h4>
<p>階層的なタスク分解、リアルタイムの戦略的適応、具現化された学習を通じて、能動的で目標指向のインタラクションを可能にします。</p>
</div>
<div class="column glass-card">
<h4><i class="fas fa-infinity" style="color: var(--color-accent2);"></i> 全モーダル理解および生成的推論 (Omni-Modal Understanding and Generative Reasoning)</h4>
<p>統一された表現を通じてシームレスなクロスモーダル統合と分析をサポートし、異種データ融合と文脈に応じたマルチモーダルインタラクションを促進します。</p>
</div>
</div>
<p>表8は、エージェント的モデルと全モーダルモデルに関連する既存の主要な研究をまとめたものです。これらのモデルはN-LMRMの能力の一部を探求しているに過ぎず、より強力な大規模マルチモーダル推論モデルを構築するために上記の2つの能力を組み合わせてはいません。</p>
<img alt="Table 8: Agentic and Omni-modal models towards N-LMRMs" class="content-image" src="table8.png"/>
<p class="reference">表8: N-LMRMに向けた最近のエージェント的および全モーダルモデルの概要。</p>
<div class="content-box">
<h4><i class="fas fa-user-astronaut" style="color: var(--color-primary);"></i> マルチモーダルエージェント的推論 (Multimodal Agentic Reasoning)</h4>
<p>マルチモーダルエージェント的推論の中核的な能力の一つは<span class="keyword">動的適応</span>であり、環境からのフィードバックに基づいてリアルタイムで戦略を調整できます。業界の最新製品のいくつかは、この能力を初期的に実証しています。</p>
<ul class="unstyled-list">
<li><span class="badge blue">MCP (Model Context Protocol)</span> と <span class="badge blue">A2A (Agent2Agent Protocol)</span>: 多様なツールのシームレスな統合を促進し、様々な外部環境との動的なインタラクションを可能にします。これらのプロトコルは、エージェントが環境フィードバックに基づいてリアルタイムで戦略を適応させるマルチモーダルエージェント的推論の重要性を強調しており、動的で多面的な実世界のアプリケーションにおける有効性を高めます。</li>
<li><strong>Operater</strong>: GPT-4oの視覚能力と強化学習によって達成された高度な推論能力を組み合わせ、グラフィカルユーザーインターフェース（GUI）を通じてオペレーティングシステムやブラウザとリアルタイムで対話し、タスク実行中にブラウジングやデータ操作を継続的に改善します。</li>
<li><strong>Claude Computer Use</strong>: モデルがデスクトップ環境を操作・ナビゲートし、試行錯誤を通じて最適なインタラクション戦略を学習することを可能にします。</li>
</ul>
<p>さらに、<span class="keyword">Search-o1</span>は、推論プロセス中に外部知識検索を利用して理解のギャップを埋めます。<span class="keyword">R1-Searcher</span>と<span class="keyword">DeepResearcher</span>は、強化学習を通じて検索エンジンを自律的に使用して情報を収集する能力を強化します。この自律的な知識検索を推論プロセスに組み込むことで、これらのシステムはより洗練された理解を持って行動し、変化するタスクに応答を適応させることができます。</p>
<p><span class="keyword">Gemini 2.0</span>は、マルチモーダルコンテンツを処理・生成する能力を持っています。Googleの様々なツールと深く統合し、高度な推論能力を組み合わせることで、タスクを効果的に分解し、多段階の問題に対処する際に必要な情報を段階的に取得できます。現在のモデルはこの機能の初期バージョンを実証していますが、多様なモダリティにわたる持続的でインタラクティブな推論を行う能力には欠けています。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-brain"></i> 具現化された学習 (Embodied Learning)</p>
<p>LMRMが外部環境を扱うためのもう一つの側面は、<span class="highlight">具現化された学習</span>です。これは、デジタル環境と物理環境の両方と対話できるシステムによって実証されます。</p>
<ul class="unstyled-list">
<li><strong>Magma</strong>: 実世界のデータと対話することで学習し、仮想環境と物理環境の両方でオブジェクトを効果的にナビゲート・操作するための時空間推論を改善します。</li>
<li><strong>OpenVLA</strong>: 視覚エンコーダと言語モデルを組み合わせ、実世界のロボットデモンストレーションから学習することを可能にします。この具現化されたアプローチにより、モデルは視覚的スキルとタスク固有の推論スキルの両方を獲得し、マルチモーダルな理解と適応を必要とする複雑な実世界の行動を実行する能力を高めます。</li>
</ul>
<p>要約すると、最近の強化学習スケーリング手法は、大規模モデルのエージェント的振る舞いを大いに刺激し、ワールドモデルへと押し進めるでしょう。</p>
</div>
</div>
<div class="content-box">
<h4><i class="fas fa-infinity" style="color: var(--color-primary);"></i> 全モーダル理解および生成的推論 (Omni-Modal Understanding and Generative Reasoning)</h4>
<p>マルチモーダルエージェントの振る舞いは、基盤となる大規模マルチモーダルモデルの深い推論能力、特に<span class="keyword">知覚範囲、理解精度、推論の深さ</span>と密接に関連しています。したがって、実世界のアプリケーション向けの包括的な全モーダルモデルを開発し、その深い推論能力を強化することが基本となります。</p>
<div class="pipeline">
<div class="pipeline-step">
<span class="badge purple">初期の研究</span>
<p><strong>AnyGPT</strong>: 様々なモダリティの統一処理に離散表現を利用し、モダリティ間の統一的な理解と生成を達成。</p>
</div>
<div class="pipeline-step">
<span class="badge purple">最近の進展</span>
<p><strong>BaichuanOmni-1.5</strong>: 様々なモダリティにわたる協調的なリアルタイム理解において印象的な能力を発揮。</p>
<p><strong>Qwen2.5-Omni</strong>: Time-aligned Multimodal RoPEという新しい位置埋め込みを使用して、ビデオ入力のタイムスタンプを音声と同期。</p>
<p><strong>M2-omni, MiniCPM-o</strong>: GPT-4oのようなクローズドソースモデルとの性能差を縮小。</p>
</div>
<div class="pipeline-step">
<span class="badge purple">小規模モデル</span>
<p>実世界の特定のニーズに動機付けられ、より小型の全モーダルモデルが注目を集めています。</p>
<p><strong>Megrez-3B-Omni</strong>: シーン理解やOCRなどのタスクで優れた性能を発揮するオンデバイスのマルチモーダル理解LLMモデル。</p>
<p><strong>Mini-Omni2</strong>: 視覚・音声クエリに対してリアルタイムでエンドツーエンドの音声応答を提供できる視覚・音声アシスタント。</p>
<p><strong>R1-Omni</strong>: 視覚・聴覚情報からの感情認識に焦点。</p>
</div>
</div>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 残された課題</p>
<p>これらの進歩にもかかわらず、マルチモーダルAIにおける現在の研究は主に、統一されたマルチモーダル表現の理解と生成の強化に焦点を当てています。<span class="highlight">クロスモーダルインタラクションを効果的に統合し質問する推論能力の開発は、依然として決定的に未開拓</span>です。このギャップを埋めることは、人間のような洗練さで相互接続されたモダリティを処理、分析、統合するように本質的に設計されたシステムである、ネイティブマルチモーダル推論モデルを実現するために不可欠です。</p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-lightbulb"></i> 4.3 Technical Prospects (技術的展望)</h3>
<p>ネイティブ大規模マルチモーダル推論モデル（N-LMRMs）の技術的展望は、言語や視覚から音声、触覚、センサー測定値、時系列シーケンス、構造化データに至るまで、<span class="keyword">多様なデータタイプにわたる理解、生成、推論をネイティブに統一</span>することを目指しており、統一的かつ一貫した方法で見て、聞いて、話して、行動できるシステムに近づけます。</p>
<p>しかし、このようなN-LMRMを構築することは大きな課題を提起します。これらのモデルは、単一システム内で異種モダリティを処理し、長いマルチモーダル推論連鎖を通じて多様なツールを遺伝的に使用および組み合わせ、実世界のインタラクションからの継続的な学習をサポートするようにアーキテクチャ的に設計されなければなりません。このセクションでは、N-LMRMを構築する上での主要な課題を概説し、それらを克服するためのいくつかの潜在的な経路を提案します。</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-sitemap"></i> Unified Representations and Cross-Modal Fusion (統一表現とクロスモーダル融合)</p>
<p>根本的な課題は、異なるモダリティを一貫した方法で処理・生成できる単一のモデルアーキテクチャを作成することです。従来のアプローチでは、各モダリティに個別のエンコーダを使用することがよくありました。対照的に、ネイティブ全モーダルモデルは、モダリティ間のシームレスな相互作用を可能にする、より統一された設計を求めています。</p>
<p>考えられる解決策の1つは、すべての入力と出力を共通の形式に均質化し、任意のモダリティを均一に処理することです。このアプローチは、あるモダリティが他のモダリティの表現を支配したり損なったりする<span class="highlight">負の干渉</span>を防ぐために慎重な設計が必要です。</p>
<p>そのため、新たな解決策として<span class="keyword">Mixture-of-Experts (MoE) アーキテクチャ</span>が登場しています。特定のモダリティに特化したエキスパートは関連する入力に対してのみアクティブ化され、コア言語モデルが言語知能のバックボーンとして機能します。</p>
</div>
<img alt="Figure 9: Overview of next-generation N-LMRM" class="content-image" src="native_multimodal_reasoning_model.jpg"/>
<p class="reference">図9: 次世代ネイティブ大規模マルチモーダル推論モデルの概要。構想されているシステムは、多様な実世界のデータモダリティにわたる包括的な知覚を達成し、正確な全モーダル理解と詳細な生成的推論を可能にすることを目指しています。この基礎モデルは、より高度な形態の知的行動につながり、世界の経験から学習し、生涯学習と自己改善を実現します。</p>
<div class="content-box">
<h4><i class="fas fa-link" style="color: var(--color-primary);"></i> Interleaved Multimodal Long Chain-of-Thought (インターリーブされたマルチモーダルな長い思考連鎖)</h4>
<p>統一された表現を基盤として、N-LMRMは従来の長い内部思考連鎖を、複数のモダリティにまたがる<span class="keyword">インターリーブされた推論プロセス</span>へと拡張できます。これにより、異なるモダリティをシームレスに融合させるテスト時計算スケーリングの新たな軸が可能になります。</p>
<p>OpenAIが最近リリースした<span class="highlight">o3</span>と<span class="highlight">o4-mini</span>は、画像を拡大、トリミング、反転、または強調できるツールを自動的に処理することで、思考連鎖の中で画像を使って推論するという、この方向への先駆的な一歩を表しています。重要なことに、これらの機能は、個別の特化モデルに依存することなく、ネイティブに提供されます。</p>
<p>ソフトウェア工学、IMOレベルの数学、創造的執筆、GUI操作などのドメインにおける強化学習の有望な汎化能力に後押しされ、強化学習をより多くのモダリティ、より長いツール拡張推論連鎖、およびより広範な推論タスクにスケーリングすることが、クロスモーダル推論をシミュレートし、機械知能を高めることができる次世代N-LMRMの秘訣となる可能性があります。</p>
</div>
<div class="content-box">
<h4><i class="fas fa-globe-americas" style="color: var(--color-primary);"></i> Learning and Evolving from World Experiences (世界の経験からの学習と進化)</h4>
<p>動的に進化する知的システムにおいて、LMRMベースの「ワールドモデル2」の中核的価値は、自動運転のような複雑な環境におけるリアルタイムのモデリングと推論能力だけでなく、環境との継続的な相互作用を通じた<span class="keyword">生涯学習</span>のための進化的メカニズムにもあります。</p>
<p>MCPとA2Aがツールとエージェントクラスタの高密度ネットワークを作成すると、システムは環境、ツール、他のエージェントとの多次元的な関与を通じて、各相互作用を構造化された経験に変換できます。これには、リアルタイムデータストリームのパターン認識からツール操作チェーンにわたる因果推論、通信ネットワークにおける協調的フィードバックから異常シナリオにおける自律的適応まで、あらゆるものが含まれます。</p>
<p>この継続的な学習パラダイムにより、LMRMは静的な知識ベースの限界を克服できます。世界の経験を反復的に蓄積することで、認知アーキテクチャと意思決定戦略を動的に更新します。特にオープンな環境では、自律学習メカニズムがモデルを積極的にツール組み合わせの可能性を探求するように駆動します。新しい問題を解決する過程で、同時に転移可能な知識を保存し、最終的には特定の推論能力を持ちながらクロスシナリオの汎化レジリエンスを維持する知的システムを形成します。</p>
<p><span class="highlight">オンライン強化学習</span>の対話的学習方法と<span class="highlight">オフライン検証方法</span>が、LMRMの能力を反復的かつ継続的に刺激する可能性があると考えられており、これらはGUIエージェントモデルで継続的にパフォーマンスを向上させるために利用されています。</p>
</div>
<div class="content-box">
<h4><i class="fas fa-cogs" style="color: var(--color-primary);"></i> Data Synthesis (データ合成)</h4>
<p>LMRMの現在の能力は主にデータ駆動型です。これらのモデルを事前学習段階で強化するためには、その機能を調整する高品質な<span class="keyword">データ合成パイプライン</span>を開発することが不可欠です。</p>
<p>データ合成における既存の取り組みのほとんどは、特に視覚、言語、音声などのドメインにおける単一モーダルまたはクロスモーダルな理解と推論の向上に焦点を当てています。しかし、3つ以上のモダリティの整合、マルチモーダルな対話的思考連鎖と視覚生成の作成、動的環境における多段階計画の実装、複数ツールの呼び出しと並列ツール使用の調整など、より複雑な側面については限定的な探求しか行われていません。これらの分野は、マルチモーダル推論モデルを進歩させるための重要な機会を提供します。</p>
</div>
<div class="bubble-box">
<p>結論として、我々はN-LMRMの概念を、有能な推論者から自律的なエージェントへの移行に向けた最初のステップとして紹介します。さらに、AGIへのOpenAIの5段階の道筋に沿って、自己進化するイノベーターやマルチエージェント組織といった後続の段階の基礎を築いています。我々の研究提案に基づいて、将来の研究では、よりエージェント的で全モーダルな能力を探求し、ますます自律的な機械知能の開発を進めることができます。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-rocket"></i> このセクションのキーポイント</p>
<ul class="unstyled-list">
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 既存のLMRM（例：O3, O4-mini）の性能と限界を、困難なタスクやベンチマークを通じて検証。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> ネイティブマルチモーダル大規模モデルの将来像を提示。能力範囲とレベル（全モーダル知覚・理解、マルチモーダル対話的生成的推論、知的エージェント行動）を含む。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> このビジョンを実現するためのアプローチ（統一的知覚、学習方法、データ合成）について議論。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> ネイティブLMRMが、機械知能におけるパラダイムシフトとして、包括的な知覚、正確な理解、深い推論を達成することを期待。</li>
</ul>
</div>
</div>
<div class="section-card" id="5_Dataset_and_Benchmark_26">
<h2 class="section-title"><i class="fas fa-database"></i> 5 Dataset and Benchmark 26</h2>
<div class="content-box">
<p>このセクションでは、マルチモーダル推論モデル（Multimodal Reasoning Models）の開発と最適化において、モデルの性能を多角的に評価するために提案されてきた様々なタスク、データセット、ベンチマークについて概観します。特に、動画理解や視覚的推論など、多岐にわたる側面からの評価が重要となります。</p>
<p>これらのデータセットやベンチマークは、モデルが持つべき能力、例えば<span class="keyword">「理解 (Understanding)」</span>、<span class="keyword">「生成 (Generation)」</span>、<span class="keyword">「推論 (Reasoning)」</span>、<span class="keyword">「計画 (Planning)」</span>といった能力を測定するために設計されています。論文では、これらの能力に基づいてデータセットを主に4つの主要カテゴリに分類し、さらに11のサブカテゴリに細分化して整理しています（詳細は論文中の表9で概説されています）。</p>
</div>
<img alt="データセットとベンチマークの分類図" src="multimodal_benchmarks_datasets.jpg" style="width:80%; display:block; margin-left:auto; margin-right:auto; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin-bottom:20px;"/>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> 図10の解説</p>
<p>上の図10は、このセクションで議論されるデータセットとベンチマークの分類の概要を示しています。モデルの評価軸として、大きく以下の4つの能力が挙げられています：</p>
<ul class="unstyled-list" style="padding-left: 20px; list-style-type: '👉';">
<li><span class="badge blue">理解 (Understanding)</span>: 複数のモダリティからの情報を解釈する能力。</li>
<li><span class="badge orange">生成 (Generation)</span>: 新しいマルチモーダルコンテンツを創り出す能力。</li>
<li><span class="badge purple">推論 (Reasoning)</span>: 情報に基づいて論理的な結論を導き出す能力。</li>
<li><span class="badge green">計画 (Planning)</span>: 目標達成のために一連の行動を策定する能力。</li>
</ul>
<p>これらのカテゴリは、モデルが現実世界の複雑なタスクをどれだけ効果的にこなせるかを評価するための基盤となります。この論文で紹介されるベンチマークの多くは、タスクを成功裏に完了するために短期的または長期的な推論能力を必要とします。例えば、挑戦的な視覚的および音響的生成タスクなどがそれに該当します。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-brain"></i> 5.1 Multimodal Understanding 28</h3>
<div class="content-box">
<p><span class="keyword">マルチモーダル理解 (Multimodal Understanding)</span> とは、モデルが<span class="highlight">視覚データや聴覚データといった複数のモダリティからの情報を処理し、解釈する能力</span>を指します。この能力は、人間のように現実世界と対話し、適切に応答できるAIシステムを開発する上で非常に重要です。</p>
<p>マルチモーダル理解タスクは、モデルがどの程度人間らしく世界を認識し反応できるかを測る指標となります。この分野のタスクは、既存のものを大まかに2つの主要領域に分類できます：</p>
<div class="two-column" style="margin-top:15px;">
<div class="column">
<div class="feature-item" style="background-color: rgba(74, 111, 165, 0.05); border: 1px dashed var(--color-primary);">
<p style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-primary); margin-bottom: 5px;"><i class="fas fa-eye"></i> 視覚中心の理解</p>
<p style="font-size: 0.9em;">モデルが視覚コンテンツを理解し、それに基づいて推論する能力を評価します。</p>
</div>
</div>
<div class="column">
<div class="feature-item" style="background-color: rgba(255, 126, 95, 0.05); border: 1px dashed var(--color-secondary);">
<p style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-secondary); margin-bottom: 5px;"><i class="fas fa-volume-up"></i> 聴覚中心の理解</p>
<p style="font-size: 0.9em;">音声、音楽、環境音などのオーディオ情報を扱うタスクに焦点を当てます。</p>
</div>
</div>
</div>
</div>
<div class="content-box" style="margin-top: 25px;">
<h4 style="font-family: 'Yomogi', cursive; font-size: 16px; color: var(--color-accent1); margin-top: 20px; margin-bottom: 10px; display: flex; align-items: center; padding-bottom: 5px; border-bottom: 1px dashed var(--color-accent1);"><i class="fas fa-eye" style="margin-right: 8px; color: var(--color-accent1);"></i> 5.1.1 Visual-Centric Understanding <span class="badge yellow">視覚中心</span></h4>
<p><span class="keyword">視覚中心の理解 (Visual-Centric Understanding)</span> は、モデルが画像や動画などの<span class="highlight">視覚データをどれだけ深く理解し、それに基づいて推論できるか</span>を評価します。この能力は、様々な専門タスクに及びます。ここでは、これらのタスクを以下のドメインに分類して解説します。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-images"></i> 一般的な視覚理解 (General Visual Understanding)</p>
<p>主に自然画像内の物体認識、属性特定、単純な空間的推論に焦点を当てます。初期のデータセットから、より複雑な視覚言語タスクや大規模な画像テキストペアへと進化しています。</p>
<p><span class="badge">代表例:</span> VQA, GQA, ALIGN, Visual Genome, LAION-400M, LAION-5B, FILIP, YFCC100M.</p>
<p><span class="badge blue">進化:</span> 単純な質問応答から、画像とテキストの整合性評価、関係性や物体レベルの情報を含む理解へと深化。</p>
</div>
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-file-alt"></i> 文書、チャート、OCRの視覚理解 (Document, Chart, and OCR Visual Understanding)</p>
<p>文書やチャートのような構造化された視覚情報（テキスト要素を含む）の理解に特化しています。OCR（光学的文字認識）と視覚理解の統合が重要です。</p>
<p><span class="badge">代表例:</span> DocVQA, DVQA, TextVQA, OCR-VQA, AI2D.</p>
<p><span class="badge blue">特徴:</span>
<ul class="unstyled-list" style="padding-left: 15px; list-style-type: '✔️';">
<li>OCRと視覚理解の統合が不可欠</li>
<li>テキストと視覚要素を組み合わせた複数ステップの推論</li>
<li>文書構造やチャートの慣習に関するドメイン固有知識</li>
</ul>
</p>
</div>
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-language"></i> 多言語視覚理解 (Multilingual Visual Understanding)</p>
<p>マルチモーダルシステムにおける言語の多様性への要求に応えるためのものです。英語中心のVQAシステムを超えて、多言語での視覚理解能力を評価します。</p>
<p><span class="badge">代表例:</span> CMMLU, C-Eval, Exams-v, M3exam, VideoVista-CulturalLingo, MTVQA.</p>
<p><span class="badge blue">特徴:</span>
<ul class="unstyled-list" style="padding-left: 15px; list-style-type: '🌍';">
<li>複数言語での質問と注釈</li>
<li>異なる文化的背景における視覚理解と言語能力のテスト</li>
<li>文化特有の解釈や参照を持つ可能性のある視覚コンセプトの理解</li>
</ul>
</p>
</div>
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-video"></i> 動画理解 (Video Understanding)</p>
<p>動的な視覚タスクにおけるモデルの訓練と評価に使用されます。静止画像データセットと比較して、時間ベースの理解、つまり複数のフレームにわたる動的な視覚特徴の処理が求められます。</p>
<p><span class="badge">代表例:</span> ActivityNet-QA, PerceptionTest, Video-MMMU, Video-MME, VideoVista, YouTube8M, VidGen-1M, WebVid.</p>
<p><span class="badge blue">特徴:</span>
<ul class="unstyled-list" style="padding-left: 15px; list-style-type: '🎬';">
<li>行動、イベント、時間的関係の注釈</li>
<li>短いクリップから数分間の動画まで多様な動画時間</li>
<li>科学ドメイン、長編動画、包括的な動画理解と推論など、挑戦的な領域への拡張</li>
</ul>
</p>
<p><span class="badge purple">VideoVistaの注目点:</span> 14カテゴリの動画、19の理解タスク、8つの推論タスクを含み、GPT-4oによる自動注釈フレームワークを利用。</p>
</div>
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-cogs"></i> 包括的ベンチマーク (Comprehensive Benchmarks)</p>
<p>既存のマルチモーダルモデルのより全体的な評価を提供するために登場しました。これらのベンチマークは、モデルが現実世界のシナリオで視覚と言語の理解をどれだけうまく統合できるかをテストします。</p>
<p><span class="badge">代表例:</span> MMBench, Seed-Bench, MME-RealWorld.</p>
<p><span class="badge blue">特徴:</span>
<ul class="unstyled-list" style="padding-left: 15px; list-style-type: '🎯';">
<li>知覚から推論、知識統合まで、視覚理解の様々な側面を評価する多次元評価フレームワーク</li>
<li>特定の能力を探求し、弱点を特定するための慎重に設計された質問</li>
<li>モデル間で公正な比較を行うための標準化された評価パイプライン</li>
</ul>
</p>
</div>
</div>
<div class="note-box" style="margin-top: 20px; border-left: 3px solid var(--color-accent1);">
<p class="note-title" style="color: var(--color-accent1);"><i class="fas fa-exclamation-circle"></i> まとめ: 視覚中心の理解</p>
<p>視覚中心の理解は、画像内の基本的な物体認識から、動画や文書における複雑なマルチモーダル推論に至るまで、モデルの視覚データ処理・推論能力を重視します。これらの評価は、モデルが視覚的知覚と推論を統合できることを保証するために不可欠であり、現実世界のアプリケーションにとって極めて重要です。</p>
</div>
</div>
<div class="content-box" style="margin-top: 25px;">
<h4 style="font-family: 'Yomogi', cursive; font-size: 16px; color: var(--color-accent2); margin-top: 20px; margin-bottom: 10px; display: flex; align-items: center; padding-bottom: 5px; border-bottom: 1px dashed var(--color-accent2);"><i class="fas fa-volume-up" style="margin-right: 8px; color: var(--color-accent2);"></i> 5.1.2 Audio-Centric Understanding <span class="badge yellow">聴覚中心</span></h4>
<p><span class="keyword">聴覚中心の理解 (Audio-Centric Understanding)</span> とは、モデルが<span class="highlight">音声、環境音、音楽といった様々な形式のオーディオ入力を処理、解釈し、それに応答する能力</span>の評価を指します。これらのモダリティは機械学習タスクにおいてますます不可欠になっており、モデルがオーディオデータをどれだけよく理解し、相互作用できるかを評価することが重要な焦点となっています。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-microphone-alt"></i> 音声理解 (Speech Understanding)</p>
<p>モデルの音声認識精度、多言語翻訳、感情認識などを評価します。</p>
<p><span class="badge">代表例:</span> Librispeech, Common Voice, Aishell, Fleurs (音声認識精度), CoVoST2 (多言語翻訳), MELD (感情認識).</p>
<p><span class="badge purple">評価観点:</span>
<ul class="unstyled-list" style="padding-left: 15px; list-style-type: '🗣️';">
<li>内容の正確性</li>
<li>多様な音声タスク</li>
<li>追加的な音響情報（感情など）</li>
</ul>
</p>
</div>
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-assistive-listening-systems"></i> オーディオ理解 (Audio Understanding)</p>
<p>人間の声以外の音（環境音など）からの情報抽出と認識を評価します。オーディオキャプション生成やオーディオ質問応答（AQA）タスクが含まれます。</p>
<p><span class="badge">代表例:</span> Clotho, AudioCaps (キャプション生成), ClothoAQA, AQUALLM (AQA).</p>
<p><span class="badge purple">特徴:</span> 環境音は人間の音声よりも複雑で多様な情報を提供します。</p>
</div>
<div class="info-card glass-card">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-music"></i> 音楽理解 (Music Understanding)</p>
<p>音楽の構造的特徴や複雑なバリエーションの理解を評価します。楽器、音符、音高、リズムなどの音楽理論要素の認識や、楽曲全体のキャプション生成能力をテストします。</p>
<p><span class="badge">代表例:</span> MusicNet, NSynth (音楽理論要素認識), MusicCaps, MusicBench (キャプション生成).</p>
</div>
</div>
<div class="info-card glass-card" style="margin-top: 20px;">
<p class="sub-heading" style="font-family: 'Kaisei Decol', serif; font-weight: bold; color: var(--color-primary);"><i class="fas fa-headphones-alt"></i> 包括的ベンチマーク (Comprehensive Benchmarks for Audio)</p>
<p>大規模音声言語モデル（LALMs）の進化に伴い、音声と多様な音の両方を理解する能力を持つモデルが増えています。これに対応し、モデルのオーディオ理解能力を包括的に評価するための新しいベンチマークが提案されています。</p>
<p><span class="badge">代表例:</span> VoiceBench (多様な文脈での音声理解), AudioBench (多様な音声・音タスク、人間の声に関するタスク統合), Air-Bench, MMAU (音楽タスクも含む), SD-eval (音声と環境音の混合シナリオ).</p>
<p><span class="badge purple">特徴:</span>
<ul class="unstyled-list" style="padding-left: 15px; list-style-type: '📊';">
<li>初期の評価手法を取り入れつつ、より広範な実世界の応用に対応</li>
<li>様々な種類のオーディオタスクを組み合わせ、モデルの汎用性を評価</li>
</ul>
</p>
</div>
<div class="note-box" style="margin-top: 20px; border-left: 3px solid var(--color-accent2);">
<p class="note-title" style="color: var(--color-accent2);"><i class="fas fa-exclamation-circle"></i> まとめ: 聴覚中心の理解</p>
<p>聴覚中心の理解は、音声認識から環境音や音楽の解釈まで、モデルのオーディオデータ処理・理解能力を評価するための包括的なフレームワークを提供します。これらの評価は、モデルが実世界のアプリケーションで複雑なオーディオデータを扱う際の汎用性と有効性を保証するために不可欠です。</p>
</div>
</div>
</div>
<div class="section-card" id="6_Conclusion">
<h2 class="section-title"><i class="fas fa-flag-checkered"></i>6 Conclusion</h2>
<p style="font-family: 'Yomogi', cursive; font-size: 16px; text-align: center; margin-bottom: 20px; color: var(--color-primary);">
<i class="fas fa-book-reader"></i> この論文の締めくくりとして、マルチモーダル推論モデルの旅路と未来への羅針盤が示されます！
    </p>
<p>この「結論」セクションでは、これまで議論されてきた<span class="keyword">大規模マルチモーダル推論モデル (LMRMs)</span> の研究がどこまで進み、どのような成果を上げ、そして次にどこへ向かうべきか、という壮大なテーマを総括しています。研究の最前線で何が起こっているのか、そして未来のAIがどんな姿になるのか、一緒に見ていきましょう。ポイントは、現在の到達点、顕著な課題、そしてそれらを乗り越えるための未来志向の提案です。🚀</p>
<div class="arrow-connector" style="height: 10px;"></div> <!-- Visual separator -->
<h3 class="subsection-title"><i class="fas fa-chart-bar"></i>現状の評価：光と影</h3>
<p>まず、現在のLMRMsがどのような状況にあるかを見てみましょう。</p>
<div class="two-column">
<div class="column">
<div class="content-box" style="border: 2px dashed var(--color-accent1); padding: 15px; border-radius: 8px; background-color: rgba(92, 184, 92, 0.05);">
<h4 style="font-family: 'Kaisei Decol', serif; color: var(--color-accent1);"><i class="fas fa-star"></i>輝かしい成果：言語中心の推論</h4>
<p>現在のモデルたちは、主に<strong class="highlight">「言語中心の推論パラダイム」</strong>で大きな成功を収めています。これは、まるで人間が言葉で考えるように、テキスト情報を核として、画像や音声といった他のモダリティ（情報の種類）からの入力を処理し、統合して賢い判断を下すアプローチです。</p>
<p>例えば、以下のようなタスクで素晴らしい能力を発揮しています：</p>
<ul class="unstyled-list" style="padding-left: 10px;">
<li style="margin-bottom: 8px;">🖼️ <strong style="color: var(--color-primary);">視覚的質問応答 (VQA)</strong>: 画像を見て「この猫は何をしていますか？」といった質問に答える。</li>
<li style="margin-bottom: 8px;">📊 <strong style="color: var(--color-primary);">視覚的数学 (Visual Math)</strong>: 図形やグラフが含まれる数学の問題を理解して解く。</li>
<li style="margin-bottom: 8px;">🎞️ <strong style="color: var(--color-primary);">ビデオ理解</strong>: 動画の内容を把握し、特定のシーンについて質問に答えたり、動画全体の要約を生成したりする。</li>
</ul>
<p style="font-family: 'Yomogi', cursive; text-align: center; margin-top: 10px;">まさに、マルチモーダルAIの夜明けぜよ！🌅</p>
</div>
</div>
<div class="column">
<div class="content-box" style="border: 2px dashed var(--color-secondary); padding: 15px; border-radius: 8px; background-color: rgba(255, 126, 95, 0.05);">
<h4 style="font-family: 'Kaisei Decol', serif; color: var(--color-secondary);"><i class="fas fa-map-signs"></i>未踏のフロンティア：残された課題</h4>
<p>しかし、素晴らしい成果の裏で、まだ手つかずの重要な領域、つまり<strong class="highlight">「クリティカルチャレンジ」</strong>が残されています。これらは、AIが真に人間のように世界を理解し、対話するための次なるステップです。</p>
<div class="challenge-box" style="padding: 10px; margin-bottom:10px;">
<p class="challenge-title" style="font-size:15px;"><i class="fas fa-binoculars"></i> 視覚中心の長期推論 (Visual-centric long reasoning)</p>
<p style="font-size:13px;">画像や映像といった視覚情報を、より深く、そして複数のステップにまたがって文脈的に理解し推論する能力です。現状のモデルは得意ではありません。</p>
<ul style="font-size:13px; padding-left: 15px;">
<li><i class="fas fa-cubes" style="color: var(--color-secondary);"></i> 例1: <strong>3Dコンテキストの理解</strong> – 部屋のレイアウトや物体の三次元的な配置関係を正確に把握し、「あの棚の上の、左から2番目の箱を取って」のような指示を理解する。</li>
<li><i class="fas fa-question-circle" style="color: var(--color-secondary);"></i> 例2: <strong>複雑な視覚情報探索型の質問への対応</strong> – 監視カメラ映像から「赤い服を着た人が昨日何時にここを通ったか」を探し出すような、詳細な情報検索と時間的推論。</li>
</ul>
</div>
<div class="challenge-box" style="padding: 10px;">
<p class="challenge-title" style="font-size:15px;"><i class="fas fa-comments"></i> インタラクティブなマルチモーダル推論 (Interactive multimodal reasoning)</p>
<p style="font-size:13px;">人間とAI、あるいはAI同士が、複数のモダリティを使いながらリアルタイムに対話し、共同で問題を解決したり、理解を深めたりする能力です。</p>
<ul style="font-size:13px; padding-left: 15px;">
<li><i class="fas fa-sync-alt" style="color: var(--color-secondary);"></i> 例1: <strong>動的なクロスモーダル対話</strong> – ユーザーが話しながらスケッチを描き、AIがそれらを統合して意図を理解し、適切な応答やさらなる質問を生成する。</li>
<li><i class="fas fa-undo" style="color: var(--color-secondary);"></i> 例2: <strong>反復的なフィードバックループ</strong> – AIの提案に対しユーザーが「もう少し右」と指示したり、別の画像を見せたりすることで、AIが継続的に理解を修正・洗練させていくプロセス。</li>
</ul>
</div>
<p style="font-family: 'Yomogi', cursive; text-align: center; margin-top: 10px;">これらの課題は、AIが次のステージへ進むための重要な鍵となります！🔑</p>
</div>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i>未来への設計図：本質的にマルチモーダルな大規模モデル (Inherently Multimodal Large Models) の提案</h3>
<p>これらの課題を克服し、AIを新たな高みへと導くために、この論文では<strong class="keyword">「言語支配的なアーキテクチャからの脱却」</strong>を提唱し、<strong class="highlight">本質的にマルチモーダルな大規模モデル</strong>という新しいコンセプトを打ち出しています。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-info-circle"></i>言語支配的なアーキテクチャとは？</p>
<p>現在の多くのモデルは、テキスト処理能力を中核とし、そこに画像や音声の認識モジュールを「接続」する形でマルチモーダルを実現しています。そのため、どうしても言語が情報処理の「ボトルネック」や「中心」になりがちです。本質的にマルチモーダルなモデルとは、全てのモダリティを初めから対等に、あるいは各モダリティの特性を最大限に活かせる形で統合する設計を目指すものです。</p>
</div>
<p>この未来型モデルが特に優先して開発すべき<strong style="font-family: 'Kaisei Decol', serif; color: var(--color-accent2);">3つの核心的な能力</strong>が示されています。これらは、AIがより人間らしく、柔軟に世界と関わるための鍵となります。</p>
<div class="info-grid" style="margin-top: 20px;">
<div class="info-card glass-card">
<div class="feature-item">
<i class="fas fa-robot fa-3x" style="color:var(--color-primary); margin-bottom: 10px;"></i>
<h4 style="font-family: 'Yomogi', cursive; color:var(--color-primary); font-size: 18px;">1. マルチモーダル・エージェント的推論<br/>(Multimodal Agentic Reasoning)</h4>
</div>
<p>これは、モデルが単に情報を受け取って処理するだけでなく、まるで自律的なエージェント（行為主体）のように、<strong class="highlight">環境に対して能動的に働きかけ、目標を達成するために計画を立て、行動し、その結果から学ぶ</strong>能力です。</p>
<div class="definition-box" style="margin-top: 15px;">
<p class="definition-title"><i class="fas fa-user-astronaut"></i>「エージェント的」とは？</p>
<p>自らの意思（あるいは設定された目標）に基づいて、周囲の状況を認識し、判断し、行動する能力を持つことを指します。まるでSF映画のロボットのように、環境とインタラクションしながらタスクを遂行するイメージです。</p>
</div>
<p style="margin-top: 10px;">📝 例: <strong>実世界の試行錯誤を通じて学習する実体型AIエージェント (Embodied AI agents that learn through real-world trial and error)</strong></p>
<p>ロボットが、部屋の片付けを指示されたとします。最初はどこに何を置けばいいか分かりませんが、実際に物を掴んで動かし、「これはここじゃないな」「こっちの方が良さそうだ」と試行錯誤を繰り返す中で、徐々に効率的な片付け方を学んでいく。この過程で、視覚情報（物の形や位置）、触覚情報（物の重さや質感）、そして行動の結果（物がうまく収まったか、倒れたかなど）を統合的に学習します。</p>
<div style="text-align: center; margin-top:15px;">
<img fa-globe-americas"="" fas="" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA1MDAgMzAwIj4KICAgIDxwYXRoIGQ9Ik0xMCAxNTBIODBhMTUgMTUgMCAwIDAgMTUtMTVWMTA1YTgwIDgwIDAgMSAxIDAtNzBhMTUgMTUgMCAwIDAgLTE1IDE1djM1IiBmaWxsPSJub25lIiBzdHJva2U9IiM0YTZmYTUiIHN0cm9rZS13aWR0aD0iMyIvPgogICAgPHBhdGggZD0iTTUwIDMwQzUwIDMwIDcwIDEwIDkwIDMwIiBmaWxsPSJub25lIiBzdHJva2U9IiM0YTZmYTUiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8cGF0aCBkPSJNNTAgNDVDNTAgNDUgNzAgMjUgOTAgNDUiIGZpbGw9Im5vbmUiIHN0cm9rZT0iIzRhNmZhNSIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDxjaXJjbGUgY3g9IjcwIiBjeT0iMjUiIHI9IjUiIGZpbGw9IiM0YTZmYTUiLz4KICAgIDxyZWN0IHg9IjYwIiB5PSIzMCIgd2lkdGg9IjIwIiBoZWlnaHQ9IjUiIHJ4PSIyIiByeT0iMiIgZmlsbD0iIzRhNmZhNSIvPgoKICAgIDxlbGxpcHNlIGN4PSIxNzUiIGN5PSIxMDAiIHJ4PSI1MEMiIHJ5PSIzMCIgZmlsbD0iI2ZmN2U1ZiIgc3Ryb2tlPSIjZmM2ZDRmIiBzdHJva2Utd2lkdGg9IjIiLz4KICAgIDx0ZXh0IHg9IjE3NSIgeT0iMTA1IiBmb250LWZhbWlseT0iWW9tb2dpIiBmb250LXNpemU9IjE2IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBmaWxsPSIjZmZmIj5環境&lt;i class="/>CgogICAgPHJlY3QgeD0iMTQwIiB5PSIxNzUiIHdpZHRoPSI3MCIgaGVpZ2h0PSI1MCIgcng9IjUiIHJ5PSI1IiBmaWxsPSIjNWNiODVjIiBzdHJva2U9IiM0Y2EwNGMiIHN0cm9rZS13aWR0aD0iMiIvPgogICAgPHRleHQgeD0iMTc1IiB5PSIyMDUiIGZvbnQtZmFtaWx5PSJZb21vZ2kiIGZvbnQtc2l6ZT0iMTYiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiNmZmYiPkAIエージェント🤖CgogICAgPHBhdGggZD0iTTE3NSAxNzAgViAxNDAiIHN0cm9rZS1kYXNoYXJyYXk9IjQsNCIgc3Ryb2tlPSIjNmY4MmM4IiBzdHJva2Utd2lkdGg9IjIiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMTcwLDE0NSAxNzUsMTM1IDE4MCwxNDUiIGZpbGw9IiM2ZjgyYzgiLz4KICAgIDx0ZXh0IHg9IjIyMCIgeT0iMTU1IiBmb250LWZhbWlseT0iWW9tb2dpIiBmb250LXNpemU9IjEyIiBmaWxsPSIjM2MzYzNjIj5能動的インタラクションCgogICAgPHJlY3QgeD0iMzAiIHk9IjIzMCIgd2lkdGg9IjQ0MCIgaGVpZ2h0PSI2MCIgcng9IjUiIHJ5PSI1IiBmaWxsPSIjZmZmIiBzdHJva2U9IiNjY2MiIHN0cm9rZS13aWR0aD0iMSIgc3Ryb2tlLWRhc2hhcnJheT0iNCwyIi8+CiAgICA8dGV4dCB4PSI0MCIgeT0iMjU1IiBmb250LWZhbWlseT0iWW9tb2dpIiBmb250LXNpemU9IjEzIiBmaWxsPSIjNzc3Ij5🤖「この箱、重いな...あっちの棚に置けるかな？」 (視覚＋触覚＋推論)CgogICAgPHRleHQgeD0iNDAiIHk9IjI3NSIgZm9udC1mYW1pbHk9IllvbW9naSIgZm9udC1zaXplPSIxMyIgZmlsbD0iIzc3NyI+👍「よし、うまく収まった！次はこの本だ。」 (行動の結果から学習)Cjwvc3ZnPg==" alt="Multimodal Agentic Reasoning Example" style="max-width: 90%; border-radius: 8px; margin: 10px auto; display: block; border: 1px solid #ddd;"&gt;
            </div>
</div>
<div class="info-card glass-card">
<div class="feature-item">
<i class="fas fa-brain fa-3x" style="color:var(--color-secondary); margin-bottom: 10px;"></i>
<h4 style="font-family: 'Yomogi', cursive; color:var(--color-secondary); font-size: 18px;">2. 全方位モーダル理解と生成的推論<br/>(Omini-Modal Understanding and Generative Reasoning)</h4>
</div>
<p>これは、モデルが<strong class="highlight">あらゆる種類のモダリティからの情報を統合的に理解し、それに基づいて新たな情報を生成する</strong>能力を指します。単に理解するだけでなく、そこから創造的なアウトプットを生み出す点が重要です。</p>
<div class="definition-box" style="margin-top: 15px;">
<p class="definition-title"><i class="fas fa-infinity"></i>「オムニモーダル (Omini-Modal)」とは？</p>
<p>テキスト、画像、音声だけでなく、触覚、センサーデータ、さらには人間の生体信号など、考えられる限りのあらゆる種類の情報を扱えることを意味します。「オムニ (omni)」は「全ての」という意味です。</p>
</div>
<p style="margin-top: 10px;">📝 例1: <strong>任意のモーダルセマンティクスの統合 (Integrating any-modal semantics)</strong></p>
<p>例えば、「静寂」という抽象的な概念を考えます。この概念は、テキストでは「音が全くない状態」と表現され、画像では誰もいない雪景色や深夜の図書館などで視覚化され、音声では無音状態そのもので表されます。オムニモーダルモデルは、これら異なるモダリティで表現された「静寂」を、共通の概念として理解し、関連付けることができます。 <strong style="color: var(--color-accent2);">これにより、複雑で開かれた世界の文脈における曖昧さを解消します。</strong></p>
<p style="margin-top: 10px;">📝 例2: <strong>複数のモダリティにまたがる首尾一貫した文脈認識型出力の生成 (Producing coherent, context-aware outputs across modalities)</strong></p>
<p>例えば、「未来都市のコンセプトを説明するプレゼンテーションを作って」と口頭で指示すると、AIがテキスト（説明文）、画像（未来都市のイメージ図）、さらには短いアニメーション動画（ドローンが飛び交う様子など）を組み合わせたスライド資料を生成する。それぞれの要素が互いに矛盾せず、全体として一貫したメッセージを伝えることが重要です。</p>
<div style="text-align: center; margin-top:15px;">
<img fa-file-alt"="" fas="" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA1MDAgMzAwIj4KICAgIDxyZWN0IHg9IjMwIiB5PSIzMCIgd2lkdGg9IjEwMCIgaGVpZ2h0PSI3MCIgcng9IjUiIHJ5PSI1IiBmaWxsPSIjZThmMmY3Ii8+CiAgICA8dGV4dCB4PSI4MCIgeT0iNzMiIGZvbnQtZmFtaWx5PSJZb21vZ2kiIGZvbnQtc2l6ZT0iMTYiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiM0YTZmYTUiPkテキスト&lt;i class="/>CgogICAgPHJlY3QgeD0iMTUwIiB5PSIzMCIgd2lkdGg9IjEwMCIgaGVpZGhtPSI3MCIgcng9IjUiIHJ5PSI1IiBmaWxsPSIjZmNlM2Q5Ii8+CiAgICA8dGV4dCB4PSIyMDAiIHk9IjczIiBmb250LWZhbWlseT0iWW9tb2dpIiBmb250LXNpemU9IjE2IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBmaWxsPSIjZmY3ZTVmIj5画像<i class="fas fa-image"></i>CgogICAgPHJlY3QgeD0iMjcwIiB5PSIzMCIgd2lkdGg9IjEwMCIgaGVpZ2h0PSI3MCIgcng9IjUiIHJ5PSI1IiBmaWxsPSIjZDNlYWZmIi8+CiAgICA8dGV4dCB4PSIzMjAiIHk9IjczIiBmb250LWZhbWlseT0iWW9tb2dpIiBmb250LXNpemU9IjE2IiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBmaWxsPSIjOWM3YmYyIj5音声<i class="fas fa-volume-up"></i>CgogICAgPHJlY3QgeD0iMzkwIiB5PSIzMCIgd2lkdGg9IjgwIiBoZWlnaHQ9IjcwIiByeD0iNSIgcnk9IjUiIGZpbGw9IiNlOWU5ZTkiLz4KICAgIDx0ZXh0IHg9IjQzMCIgeT0iNzMiIGZvbnQtZmFtaWx5PSJZb21vZ2kiIGZvbnQtc2l6ZT0iMTYiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZpbGw9IiM2Yzc1N2QiPi他...<i class="fas fa-ellipsis-h"></i>CgogICAgPHBhdGggZD0iTTgwIDEwMEwxNzUgMTUwIiBzdHJva2U9IiNhYmNkZTYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWRhc2hhcnJheT0iNSw1Ii8+CiAgICA8cGF0aCBkPSJNMjAwIDEwMEwxNzUgMTUwIiBzdHJva2U9IiNhYmNkZTYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWRhc2hhcnJheT0iNSw1Ii8+CiAgICA8cGF0aCBkPSJNMzIwIDEwMEwyNzUgMTUwIiBzdHJva2U9IiNhYmNkZTYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWRhc2hhcnJheT0iNSw1Ii8+CiAgICA8cGF0aCBkPSJNNDMwIDEwMEwyNzUgMTUwIiBzdHJva2U9IiNhYmNkZTYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWRhc2hhcnJheT0iNSw1Ii8+CgogICAgPGNpcmNsZSBjeD0iMjI1IiBjeT0iMTgwIiByPSI1MCIgZmlsbD0id2hpdGUiIHN0cm9rZT0iIzRhNmZhNSIgc3Ryb2tlLXdpZHRoPSIzIi8+CiAgICA8dGV4dCB4PSIyMjUiIHk9IjE4NSIgZm9udC1mYW1pbHk9IllvbW9naSIgZm9udC1zaXplPSIyMCIgdGV4dC1hbmNob3I9Im1pZGRsZSIgZmlsbD0iIzRhNmZhNSI+統合理解🧠CgogICAgPHBhdGggZD0iTTIyNSAyMzAgViAyNzUiIHN0cm9rZS1kYXNoYXJyYXk9IjQsNCIgc3Ryb2tlPSIjNmY4MmM4IiBzdHJva2Utd2lkdGg9IjIiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjIwLDI4MCAyMjUsMjkwIDIzMCwyODAiIGZpbGw9IiM2ZjgyYzgiLz4KICAgIDxyZWN0IHg9IjE1MCIgeT0iMjg1IiB3aWR0aD0iMjAwIiBoZWlnaHQ9IjYwIiByeD0iNSIgcnk9IjUiIGZpbGw9IiNmZmYiIHN0cm9rZT0iI2ZmN2U1ZiIgc3Ryb2tlLXdpZHRoPSIyIi8+CiAgICA8dGV4dCB4PSIyNTAiIHk9IjMyMyIgZm9udC1mYW1pbHk9IllvbW9naSIgZm9udC1zaXplPSIxOCIgdGV4dC1hbmNob3I9Im1pZGRsZSIgZmlsbD0iI2ZmN2U1ZiI+生成的推論💡Cjwvc3ZnPg==" alt="Omini-Modal Understanding and Generative Reasoning Example" style="max-width: 90%; border-radius: 8px; margin: 10px auto; display: block; border: 1px solid #ddd;"&gt;
            </div>
</div>
</div>
<p style="margin-top: 25px;">これらの3つの能力は、相互に関連し合っています。エージェント的な振る舞いは、オムニモーダルな理解力と生成的推論能力があってこそ可能になり、また、環境とのインタラクションを通じてこれらの能力がさらに磨かれていく、という好循環が期待されます。</p>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas-solid fa-microscope"></i>最終的なゴール：人間のような文脈適応性と汎用的な問題解決能力</h3>
<p>これらの提案された方向性に取り組むことで、将来のモデルは、現在見られるような<strong class="highlight">特定のタスクにおける限定的な性能</strong>と、<strong class="highlight">汎用的で実世界の問題解決能力</strong>との間のギャップを埋めることができると期待されています。</p>
<p>目指すのは、<strong style="font-family: 'Kaisei Decol', serif; color: var(--color-accent1);">人間のような文脈適応性</strong>です。これは、状況に応じて柔軟に戦略を変え、新しい情報を取り入れ、予期せぬ出来事にも対処できる能力を意味します。</p>
<div class="bubble-box" style="border-color: var(--color-primary); background-color: rgba(74, 111, 165, 0.05);">
<p style="font-family: 'Yomogi', cursive; font-size: 18px; color: var(--color-primary); text-align:center; margin-bottom:10px;">
<i class="fas fa-lightbulb"></i> まとめると...
        </p>
<p>この論文は、マルチモーダル推論モデルの進化の歴史を振り返りつつ、現状の限界を明確にし、その上で、より高度で汎用的なAIを実現するための具体的な未来像（本質的にマルチモーダルな大規模モデル）とその中核となるべき能力（エージェント的推論、オムニモーダル理解・生成的推論）を提示しています。これは、AIが単なるツールから、真に知的なパートナーへと進化するための重要な道しるべと言えるでしょう。📍</p>
</div>
<p style="text-align: center; font-family: 'Kaisei Decol', serif; font-size: 16px; color: var(--color-accent2); margin-top: 25px;">
<i class="fas fa-infinity"></i> これからのマルチモーダル推論モデルの進化に、目が離せませんね！ <i class="fas fa-rocket"></i>
</p>
</div>
<div class="section-card" id="1_Introduction">
<h2 class="section-title"><i class="fas fa-microscope"></i>1 Introduction</h2>
<div class="content-box">
<p>このセクションでは、論文の導入部として、<span class="keyword">大規模マルチモーダル推論モデル (Large Multimodal Reasoning Models, LMRMs)</span> の重要性、研究背景、本論文が取り組む課題、そして提案する貢献について概説します。近年、AIがますます複雑で多様な情報（マルチモーダル情報）を扱うようになる中で、人間のように筋道を立てて考える能力、すなわち「推論」能力が不可欠になっています。この論文は、その最前線にあるLMRMsの進化の道のりと未来像を明らかにするものです。</p>
</div>
<div class="bubble-box">
<p style="font-family: 'Yomogi', cursive; font-size: 16px; color: var(--color-primary);"><i class="fas fa-lightbulb"></i> <strong>このセクションの主な目的と論旨</strong></p>
<ul class="unstyled-list" style="padding-left: 20px;">
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 推論が知的行動の核心であること、特にマルチモーダル環境におけるAIにとって不可欠であることを強調します。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> LMRMsを、複数の情報源（テキスト、画像、音声、動画など）を統合し、高度な推論を行う有望な技術として紹介します。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> マルチモーダル推論研究の急速な進展と、依然として残る課題（一般化能力、推論の深さ、エージェント的振る舞いの限界）を指摘します。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 既存のサーベイ論文ではカバーしきれていない、強化学習を用いた最新の推論手法やLMRMsの将来展望を含む、包括的な分析の必要性を示します。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 本論文が提案するLMRMsの発展ロードマップ（3つのステージ）と、次世代の「ネイティブLMRMs (N-LMRMs)」という概念を紹介します。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 本論文の主要な貢献点を明確に提示します。</li>
</ul>
</div>
<div class="content-box">
<p><i class="fas fa-brain" style="color: var(--color-secondary);"></i> 哲学と人工知能 (AI) の両分野において、<span class="keyword">推論 (Reasoning)</span> は知的行動の礎として広く認識されています (Kahneman, 2011; Su et al., 2024; de Winter et al., 2024; Bi et al., 2025)。推論能力によって、エージェント（ここではAIシステムを指します）は、単に環境に適応的に応答するだけでなく、論理的な結論を導き出し、多様な文脈で知識を一般化し、複雑な課題を乗り越えることができます。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-book-open"></i> 用語解説：推論 (Reasoning)</div>
<p>与えられた情報から論理的な結論を導き出したり、未知の事柄を推測したりする思考プロセスのことです。人間が日々行っている問題解決や意思決定の根幹をなす能力であり、AIにおいても高度な知的振る舞いを実現するための鍵となります。</p>
<div style="text-align: center; margin-top:10px;">
<i class="fas fa-lightbulb fa-2x" style="color: var(--color-accent3); margin: 0 10px;"></i>
<i class="fas fa-puzzle-piece fa-2x" style="color: var(--color-accent3); margin: 0 10px;"></i>
<i class="fas fa-sitemap fa-2x" style="color: var(--color-accent3); margin: 0 10px;"></i>
</div>
</div>
<p><i class="fas fa-cogs" style="color: var(--color-secondary);"></i> AIシステムがますます動的で不確実、そして<span class="keyword">マルチモーダル (multimodal)</span>な環境と相互作用するようになるにつれて、様々な環境下で適切な推論を行う能力は、堅牢で適応的な知能を達成するために不可欠となっています (Yang et al., 2025a; Christakopoulou et al., 2024)。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-photo-video"></i> 用語解説：マルチモーダル (Multimodal)</div>
<p>複数の異なる種類の情報源や表現形式（モダリティ）を扱うことを意味します。例えば、<span class="highlight">テキスト</span>（文章）、<span class="highlight">画像</span>（写真やイラスト）、<span class="highlight">音声</span>（話し声や音楽）、<span class="highlight">動画</span>などがそれぞれ異なるモダリティです。人間は自然にこれらの情報を統合して理解しますが、AIにとってもこの能力は重要です。</p>
<div style="text-align: center; margin-top:10px;">
<i class="fas fa-file-alt fa-2x" style="color: var(--color-accent1); margin: 0 10px;" title="テキスト"></i>
<i class="fas fa-image fa-2x" style="color: var(--color-accent1); margin: 0 10px;" title="画像"></i>
<i class="fas fa-volume-up fa-2x" style="color: var(--color-accent1); margin: 0 10px;" title="音声"></i>
<i class="fas fa-video fa-2x" style="color: var(--color-accent1); margin: 0 10px;" title="動画"></i>
</div>
</div>
<p><i class="fas fa-rocket" style="color: var(--color-secondary);"></i> このような背景から、<span class="keyword">大規模マルチモーダル推論モデル (Large Multimodal Reasoning Models, LMRMs)</span> が有望な研究開発の方向性として登場しました (Wang et al., 2024k; Zhang et al., 2024c; Yin et al., 2023)。LMRMsは、テキスト、画像、音声、ビデオなどの複数のデータモダリティを統合し、<span class="highlight">論理的演繹</span>、<span class="highlight">因果推論</span>、<span class="highlight">類推マッピング</span>、<span class="highlight">長期思考</span>といった複雑な推論能力を示します。LMRMsの核心的な目標は、<span class="keyword">包括的な知覚 (comprehensive perception)</span>、<span class="keyword">正確な理解 (precise understanding)</span>、そして<span class="keyword">深い推論 (deep reasoning)</span> を可能にし、多様な環境における意思決定プロセスを支援することです。</p>
<div class="glass-card" style="margin-top: 20px;">
<p style="font-family: 'Yomogi', cursive; font-size: 16px; text-align: center;"><i class="fas fa-brain"></i> LMRMsのイメージ</p>
<div style="display: flex; align-items: center; justify-content: space-around; margin-top: 10px;">
<div style="text-align: center;">
<i class="fas fa-file-alt fa-2x" style="color: var(--color-primary);"></i><br/>
<i class="fas fa-image fa-2x" style="color: var(--color-primary);"></i><br/>
<i class="fas fa-volume-up fa-2x" style="color: var(--color-primary);"></i><br/>
<i class="fas fa-video fa-2x" style="color: var(--color-primary);"></i><br/>
<span style="font-size: 12px;">多様な入力</span>
</div>
<i class="fas fa-arrow-right fa-2x" style="color: var(--color-secondary);"></i>
<div style="text-align: center; border: 2px dashed var(--color-accent2); padding: 10px; border-radius: 8px;">
<span style="font-weight: bold; color: var(--color-accent2);">LMRM</span><br/>
<span style="font-size: 12px;">(知覚・理解・推論)</span>
</div>
<i class="fas fa-arrow-right fa-2x" style="color: var(--color-secondary);"></i>
<div style="text-align: center;">
<i class="fas fa-tasks fa-2x" style="color: var(--color-accent1);"></i><br/>
<span style="font-size: 12px;">意思決定支援</span><br/>
<i class="fas fa-lightbulb fa-2x" style="color: var(--color-accent1);"></i><br/>
<span style="font-size: 12px;">複雑な推論結果</span>
</div>
</div>
</div>
</div>
<img alt="multimodal_reasoning_models_core_path.jpg" src="data:image/png;base64,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
"/>
<div class="note-box">
<div class="note-title"><i class="fas fa-pencil-alt"></i>論文中の図1について</div>
<p>この図は、大規模マルチモーダル推論モデルの核心的な進化の道筋を示しています。詳細は論文の後のセクションで解説されますが、ここでは大まかな流れを掴んでおきましょう。</p>
<ul class="unstyled-list" style="padding-left: 20px;">
<li><i class="fas fa-shoe-prints" style="color: var(--color-primary);"></i> <strong>Stage 1: Perception Driven Modular Reasoning (知覚駆動型モジュラー推論)</strong>: 初期のアプローチで、特定のタスクごとにモジュールを組み合わせて推論を行っていました。</li>
<li><i class="fas fa-shoe-prints" style="color: var(--color-secondary);"></i> <strong>Stage 2: Language-Centric Short Reasoning - System-1 Reasoning (言語中心の短期推論 - システム1推論)</strong>: 大規模言語モデル (LLM) を活用し、プロンプトベースで比較的短い思考連鎖 (CoT: Chain-of-Thought) によって推論を行います。これは人間の直感的で速い思考（システム1）に例えられます。</li>
<li><i class="fas fa-shoe-prints" style="color: var(--color-accent1);"></i> <strong>Stage 3: Language-Centric Long Reasoning - System-2 Thinking and Planning (言語中心の長期推論 - システム2思考・計画)</strong>: より長く複雑な思考連鎖や強化学習によって、計画性やエージェントのような振る舞いを実現します。これは人間の熟考型の遅い思考（システム2）に例えられます。</li>
<li><i class="fas fa-shoe-prints" style="color: var(--color-accent2);"></i> <strong>Stage 4: Towards Native Multimodal Reasoning Model (ネイティブマルチモーダル推論モデルへ)</strong>: 将来の展望として、言語だけでなく多様なモダリティをネイティブに扱い、より統合された推論能力を持つモデルを目指します。</li>
</ul>
<p>図中には、各ステージに関連する代表的なモデル名（ViLBERT, Flamingo, LLaVA-CoT, Gemini 2.0など）が多数記載されています。これらは、マルチモーダル推論技術の進化のマイルストーンとなるモデル群です。</p>
</div>
<div class="content-box">
<p><i class="fas fa-chart-line" style="color: var(--color-secondary);"></i> マルチモーダル推論の研究は急速に進展しています。初期の取り組みは<span class="keyword">知覚駆動型 (perception-driven)</span>の<span class="keyword">モジュラーパイプライン (modular pipelines)</span>に依存していましたが、近年の進歩は<span class="keyword">大規模言語モデル (Large Language Models, LLMs)</span>を活用してマルチモーダルな理解と推論を統一する方向に進んでいます (Huang et al., 2023b; Driess et al., 2023)。</p>
<div class="two-column">
<div class="column">
<div class="definition-box">
<div class="definition-title"><i class="fas fa-puzzle-piece"></i> 知覚駆動型モジュラーパイプライン</div>
<p>初期のマルチモーダルシステムは、タスクを個別のモジュール（部品）に分割し、それらをパイプライン（一連の流れ作業）のように繋げて処理していました。例えば、画像認識モジュール、テキスト理解モジュール、それらを統合するモジュール、そして最後に推論を行うモジュール、といった具合です。「知覚駆動型」とは、主にセンサーからの入力情報（知覚情報）に基づいて処理が進むことを意味します。</p>
</div>
</div>
<div class="column">
<div class="definition-box">
<div class="definition-title"><i class="fas fa-brain"></i> 大規模言語モデル (LLMs) の活用</div>
<p>近年のAIのブレイクスルーの一つであるLLMは、大量のテキストデータで学習され、人間のような自然な文章を生成したり理解したりする能力を持ちます。これをマルチモーダルな情報と組み合わせることで、より高度で柔軟な推論が可能になると期待されています。</p>
</div>
</div>
</div>
<p><span class="keyword">インストラクションチューニング (Instruction tuning)</span> (Liu et al., 2023a) や<span class="keyword">強化学習 (Reinforcement learning)</span> (DeepSeek-AI et al., 2025) は、モデルの推論性能をさらに向上させ、人間のような熟考的な振る舞いに近づけています。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-chalkboard-teacher fa-2x" style="color: var(--color-accent1);"></i>
<h4 class="subsection-title" style="margin-top:5px; margin-bottom:5px; border:none; padding-left:0;">インストラクションチューニング</h4>
<p style="font-size: 12px;">モデルに様々な指示（インストラクション）とそれに対する望ましい応答の例を与えて学習させる手法。これにより、モデルは未知のタスクに対しても指示に従って適切に応答できるようになります。</p>
</div>
<div class="feature-item">
<i class="fas fa-robot fa-2x" style="color: var(--color-accent2);"></i>
<h4 class="subsection-title" style="margin-top:5px; margin-bottom:5px; border:none; padding-left:0;">強化学習</h4>
<p style="font-size: 12px;">エージェントが試行錯誤を通じて、より良い行動戦略を学習する手法。報酬や罰則といったフィードバックを元に、目標達成のための最適な行動を学習します。推論の質を高めるために応用されています。</p>
</div>
</div>
<p><i class="fas fa-exclamation-triangle" style="color: var(--color-secondary);"></i> しかし、このような急速な進歩にもかかわらず、マルチモーダル推論は依然として大規模マルチモーダルモデルの<span class="highlight">核心的なボトルネック</span>であり、<span class="keyword">一般化能力の限界 (limiting generalization)</span>、<span class="keyword">推論の深さの限界 (depth of reasoning)</span>、そして<span class="keyword">エージェントのような振る舞いの限界 (agent-like behavior)</span> を示しています (Yue et al., 2024; Zhang et al., 2024f; Liu et al., 2024f)。</p>
<div class="challenge-box">
<div class="challenge-title"><i class="fas fa-bomb"></i> LMRMsが直面する課題</div>
<ul class="unstyled-list" style="padding-left: 20px;">
<li><i class="fas fa-puzzle-piece" style="color: var(--color-secondary);"></i> <strong>一般化能力の限界</strong>: 学習データに含まれない新しい状況やタスクに対して、うまく推論できない。</li>
<li><i class="fas fa-layer-group" style="color: var(--color-secondary);"></i> <strong>推論の深さの限界</strong>: 表面的な情報処理に留まり、複雑で多段階の論理的な思考が苦手。</li>
<li><i class="fas fa-user-astronaut" style="color: var(--color-secondary);"></i> <strong>エージェント的振る舞いの限界</strong>: 自律的に目標を設定し、計画を立て、環境とインタラクションしながら問題を解決する能力が不十分。</li>
</ul>
</div>
</div>
<div class="content-box">
<p><i class="fas fa-book-reader" style="color: var(--color-secondary);"></i> これまでのこの分野のサーベイ論文は、主に<span class="highlight">マルチモーダル大規模言語モデル</span>そのもの、あるいは<span class="highlight">言語中心の推論手法の分析</span>に焦点を当てており、近年の<span class="keyword">強化学習によって強化されたマルチモーダル推論</span>やLMRMsの<span class="keyword">技術的展望</span>に関する詳細な分析が不足していました。そのため、マルチモーダル推論の分野では、その進化の経緯と将来の方向性を理解するための<span class="keyword">一貫したフレームワーク (coherent framework)</span> が必要とされています。</p>
<p>本研究は、初期のモジュラー設計から最先端のLMRMsに至るまでの、マルチモーダル推論モデルの<span class="highlight">全ロードマップ</span>を包括的にレビュー・分析することで、この<span class="keyword">重大なギャップ (critical gap)</span> を埋めるものです。さらに、実験結果と技術的精査に基づいて、LMRMsの将来の発展を予測します。</p>
</div>
<div class="framework-box">
<div class="framework-title"><i class="fas fa-road"></i> 提案するLMRMsの発展ロードマップ</div>
<p>具体的には、マルチモーダル推論の構造化されたロードマップを3つのステージに分けて提案します（図2で示される予定ですが、このセクションでは図2は引用されていません）。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card">
<div style="text-align: center; margin-bottom: 10px;">
<span class="badge blue">ステージ1</span>
<i class="fas fa-cogs fa-2x" style="color: var(--color-primary);"></i>
</div>
<h4 class="subsection-title" style="margin-top:0; margin-bottom:10px; border:none; padding-left:0; font-size:15px; text-align: center;">知覚駆動型モジュラー推論 (Perception-Driven Modular Reasoning)</h4>
<p style="font-size: 13px;">推論がタスク固有のモジュール内に暗黙的に組み込まれている段階。</p>
</div>
<div class="info-card">
<div style="text-align: center; margin-bottom: 10px;">
<span class="badge orange">ステージ2</span>
<i class="fas fa-comments fa-2x" style="color: var(--color-secondary);"></i>
</div>
<h4 class="subsection-title" style="margin-top:0; margin-bottom:10px; border:none; padding-left:0; font-size:15px; text-align: center;">言語中心の短期推論 (Language-Centric Short Reasoning - System-1)</h4>
<p style="font-size: 13px;">LLMを用いたプロンプトベースの構造化された短い<span class="keyword">思考連鎖 (Chain-of-Thought, CoT)</span> を通じてマルチモーダル推論が現れる段階。</p>
</div>
<div class="info-card">
<div style="text-align: center; margin-bottom: 10px;">
<span class="badge purple">ステージ3</span>
<i class="fas fa-brain fa-2x" style="color: var(--color-accent2);"></i>
</div>
<h4 class="subsection-title" style="margin-top:0; margin-bottom:10px; border:none; padding-left:0; font-size:15px; text-align: center;">言語中心の長期推論 (Language-Centric Long Reasoning - System-2)</h4>
<p style="font-size: 13px;">拡張された推論連鎖と強化学習を通じて、長期的な思考、計画、エージェント的な振る舞いが可能になる段階。</p>
</div>
</div>
<div class="definition-box" style="margin-top: 15px;">
<div class="definition-title"><i class="fas fa-feather-alt"></i> 用語解説：思考連鎖 (Chain-of-Thought, CoT)</div>
<p>大規模言語モデルに複雑な問題を解かせる際に、最終的な答えだけでなく、その答えに至るまでの中間的な思考ステップを生成させる手法です。これにより、モデルの推論過程がより透明になり、また複雑な問題に対する正解率も向上することが示されています。「短いCoT」は比較的単純な数ステップの推論、「長いCoT」はより多くのステップを要する複雑な推論を指します。</p>
</div>
</div>
<div class="content-box">
<p><i class="fas fa-star" style="color: var(--color-secondary);"></i> この発展の軌跡に基づき、我々は<span class="keyword">ネイティブ大規模マルチモーダル推論モデル (Native Large Multimodal Reasoning Models, N-LMRMs)</span> という概念を導入します。これは、推論がもはや言語モデルに後付けされるのではなく、<span class="highlight">全方位的な知覚 (omnimodal perception)</span>と<span class="highlight">相互作用</span>、そして<span class="highlight">目標駆動型の認知 (goal-driven cognition)</span>からネイティブに生じる、未来志向のパラダイムです。</p>
<div class="glass-card" style="margin-top: 20px;">
<p style="font-family: 'Yomogi', cursive; font-size: 16px; text-align: center;"><i class="fas fa-atom"></i> N-LMRMs の核心</p>
<div style="display: flex; flex-direction: column; align-items: center; gap: 10px; margin-top: 15px;">
<div style="display: flex; align-items: center; background-color: rgba(255,255,255,0.7); padding: 8px; border-radius: 6px;">
<i class="fas fa-eye" style="color: var(--color-primary); margin-right: 8px;"></i>
<span>全方位的な知覚 (Omnimodal Perception)</span>
</div>
<i class="fas fa-plus" style="color: var(--color-accent1);"></i>
<div style="display: flex; align-items: center; background-color: rgba(255,255,255,0.7); padding: 8px; border-radius: 6px;">
<i class="fas fa-handshake" style="color: var(--color-secondary); margin-right: 8px;"></i>
<span>相互作用 (Interaction)</span>
</div>
<i class="fas fa-plus" style="color: var(--color-accent1);"></i>
<div style="display: flex; align-items: center; background-color: rgba(255,255,255,0.7); padding: 8px; border-radius: 6px;">
<i class="fas fa-bullseye" style="color: var(--color-accent2); margin-right: 8px;"></i>
<span>目標駆動型の認知 (Goal-driven Cognition)</span>
</div>
</div>
<p style="text-align: center; margin-top: 15px;"><i class="fas fa-arrow-down" style="color: var(--color-dark);"></i></p>
<p style="text-align: center; font-weight: bold; color: var(--color-primary);">ネイティブな推論能力の創発</p>
</div>
<p>このビジョンを、<span class="highlight">統一表現 (unified representations)</span>、<span class="highlight">学習データ合成 (training data synthesis)</span>、<span class="highlight">実世界経験からの学習 (learning from world experience)</span>、<span class="highlight">ベンチマーク構築 (benchmark construction)</span>における最近の進歩に根ざして、現在のアーキテクチャ上の制約を超えてマルチモーダル知能を進化させるための可能な方向性を示します。</p>
</div>
<div class="content-box">
<h3 class="subsection-title" style="font-size: 18px;"><i class="fas fa-trophy"></i> 本論文の主な貢献</h3>
<p>本論文の貢献は主に以下の4点です。</p>
<div class="pipeline">
<div class="pipeline-step">
<span class="badge purple" style="margin-bottom: 5px;">貢献 1</span>
<p><strong><i class="fas fa-search"></i> 包括的なサーベイ:</strong> 540以上の文献を網羅し、大規模マルチモーダル推論モデル (LMRM) の現状を包括的に調査します。我々の分析は、現行モデルにおける主要な推論上の限界を文脈化し、それらに対処します (Sec. 2)。</p>
</div>
<div class="pipeline-step">
<span class="badge purple" style="margin-bottom: 5px;">貢献 2</span>
<p><strong><i class="fas fa-road"></i> 3段階ロードマップの提案:</strong> LMRMsの発展に関する3段階のロードマップを提案します。これは、モジュラー推論からマルチモーダル思考連鎖 (MCoT)、そして最終的には長期的なシステム2推論へと至る道筋です。各ステージは、詳細な分類と代表的な手法と共にさらに分析されます (Sec. 3)。</p>
</div>
<div class="pipeline-step">
<span class="badge purple" style="margin-bottom: 5px;">貢献 3</span>
<p><strong><i class="fas fa-lightbulb"></i> N-LMRMsの導入と分析:</strong> ネイティブ大規模マルチモーダル推論モデル (N-LMRMs) を導入し分析します。アーキテクチャ、学習方法、データセット、ベンチマークを含む初期の進捗状況を徹底的に概観し、将来のマルチモーダルエージェント的推論の土台を築きます (Sec. 4)。</p>
</div>
<div class="pipeline-step" style="margin-bottom:0;">
<span class="badge purple" style="margin-bottom: 5px;">貢献 4</span>
<p><strong><i class="fas fa-database"></i> データセットとベンチマークの再編成:</strong> マルチモーダルな理解と推論に関する既存のデータセットとベンチマーク (2025年4月版に更新) を再編成し、それらのカテゴリと評価軸を明確にします (Sec. 5)。</p>
</div>
</div>
</div>
</div>
<div class="section-card" id="2_Evolving_Paradigms_of_Multimodal_Reasoning_and_Discussion">
<h2 class="section-title"><i class="fas fa-chart-line"></i> 2 Evolving Paradigms of Multimodal Reasoning and Discussion</h2>
<div class="content-box">
<p>このセクションへようこそ！ここでは、<span class="keyword">マルチモーダル推論</span>、つまりテキスト、画像、音声など複数の情報源を統合して賢い判断を下す技術が、どのように進化してきたのか、そのエキサイティングな道のりを一緒に探検します。🚗💨</p>
<p>この進化の物語は、大きく分けて<span class="highlight">4つのステージ</span>で語られます。それぞれのステージで、研究者たちがどんなモデルを設計し、どんな能力を実現し、どんな壁に挑んできたのかを詳しく見ていきましょう。この歴史的な旅路をたどることで、今のAI技術がどこに立っていて、未来に向けてどんな可能性を秘めているのか、その全体像を掴むことができるはずです。🧭✨</p>
</div>
<div class="glass-card" style="margin-bottom: 30px;">
<h4 class="subsection-title" style="font-size: 18px; color: var(--color-dark); border-bottom: none; justify-content: center;"><i class="fas fa-road"></i> マルチモーダル推論の進化の4ステージ 概観</h4>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); gap: 15px;">
<div class="info-card" style="border-top: 5px solid var(--color-primary);">
<h5 class="subsection-title" style="font-size: 16px; color: var(--color-primary); margin-bottom: 10px; justify-content: center;"><i class="fas fa-layer-group"></i> Stage 1</h5>
<p style="font-size: 13px; text-align: center; font-weight: bold;">知覚駆動モジュラー推論</p>
<p style="font-size: 12px; text-align: center; color: var(--color-gray);">タスク特化型システムの設計</p>
</div>
<div class="info-card" style="border-top: 5px solid var(--color-secondary);">
<h5 class="subsection-title" style="font-size: 16px; color: var(--color-secondary); margin-bottom: 10px; justify-content: center;"><i class="fas fa-bolt"></i> Stage 2</h5>
<p style="font-size: 13px; text-align: center; font-weight: bold;">言語中心の短期推論</p>
<p style="font-size: 12px; text-align: center; color: var(--color-gray);">System-1 推論</p>
</div>
<div class="info-card" style="border-top: 5px solid var(--color-accent1);">
<h5 class="subsection-title" style="font-size: 16px; color: var(--color-accent1); margin-bottom: 10px; justify-content: center;"><i class="fas fa-brain"></i> Stage 3</h5>
<p style="font-size: 13px; text-align: center; font-weight: bold;">言語中心の長期推論</p>
<p style="font-size: 12px; text-align: center; color: var(--color-gray);">System-2 思考・計画</p>
</div>
<div class="info-card" style="border-top: 5px solid var(--color-accent2);">
<h5 class="subsection-title" style="font-size: 16px; color: var(--color-accent2); margin-bottom: 10px; justify-content: center;"><i class="fas fa-rocket"></i> Stage 4</h5>
<p style="font-size: 13px; text-align: center; font-weight: bold;">ネイティブ大規模マルチモーダル推論モデルへ</p>
<p style="font-size: 12px; text-align: center; color: var(--color-gray);">展望</p>
</div>
</div>
<p style="text-align: center; margin-top: 15px; font-size: 13px; color: var(--color-gray);">各ステージが次のステージへの礎となり、より高度なAIシステムへと進化しています。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-layer-group"></i> Stage 1: Perception-Driven Modular Reasoning - Designing Task-Specific Reasoning Systems</h3>
<div class="content-box">
<p>マルチモーダル推論の冒険はここから始まります！最初期のシステムは、まるで職人が一つ一つの部品を組み上げるように、<span class="keyword">タスク特化型</span>で作られていました。🧩</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-microchip"></i> この時代の主力技術</p>
<ul class="unstyled-list">
<li><i class="fas fa-image" style="color:var(--color-primary); margin-right:5px;"></i><strong>CNN (Convolutional Neural Networks):</strong> 主に画像の特徴を抽出するのに使われました。</li>
<li><i class="fas fa-stream" style="color:var(--color-secondary); margin-right:5px;"></i><strong>LSTM (Long Short-Term Memory) networks:</strong> 時系列データ（例えば、テキストや動画のフレームシーケンス）のパターンを学習するのに役立ちました。</li>
<li><i class="fas fa-chalkboard-teacher" style="color:var(--color-accent1); margin-right:5px;"></i><strong>教師あり学習 (Supervised Learning):</strong> 正解ラベル付きのデータでモデルを訓練する手法が主流でした。</li>
</ul>
</div>
<p>しかし、当時はいくつかの大きな課題がありました：</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 初期段階の課題</p>
<ul class="unstyled-list">
<li><i class="fas fa-database" style="color:var(--color-secondary); margin-right:5px;"></i>マルチモーダルデータの量が限られていた。</li>
<li><i class="fas fa-network-wired" style="color:var(--color-secondary); margin-right:5px;"></i>ニューラルネットワークのアーキテクチャがまだ成熟していなかった。</li>
<li><i class="fas fa-cogs" style="color:var(--color-secondary); margin-right:5px;"></i>学習方法論が十分に開発されていなかった。</li>
</ul>
</div>
<p>これらの課題に対応するため、研究者たちは<span class="keyword">モジュラーデザイン</span>を採用しました。これは、推論プロセスをいくつかの独立した部品（モジュール）に分解するアプローチです。</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-sitemap"></i> モジュラーデザインの構成要素 (§3.1.1)</p>
<div class="process-step">
<div class="step-number" style="background-color: var(--color-primary);">1</div>
<div class="step-content"><strong>表現 (Representation):</strong> 各モダリティ（例: 画像、テキスト）から情報を抽出。</div>
</div>
<div class="process-step">
<div class="step-number" style="background-color: var(--color-secondary);">2</div>
<div class="step-content"><strong>アラインメント (Alignment):</strong> 異なるモダリティ間の関連性を見つける。</div>
</div>
<div class="process-step">
<div class="step-number" style="background-color: var(--color-accent1);">3</div>
<div class="step-content"><strong>融合 (Fusion):</strong> 複数のモダリティからの情報を統合。</div>
</div>
<div class="process-step">
<div class="step-number" style="background-color: var(--color-accent2);">4</div>
<div class="step-content"><strong>推論 (Reasoning):</strong> 統合された情報に基づいて結論を導き出す。</div>
</div>
</div>
<div class="arrow-connector"></div>
<p>時代が進み、<span class="highlight">事前学習-ファインチューニングパラダイム</span>（例: BERT, GPT）が主流になると、状況は少し変わります。大規模なマルチモーダルデータセットの登場と、より深いニューラルネットワークの進化により、<span class="keyword">事前学習済み視覚言語モデル (VLMs)</span> が台頭しました。これらのモデルは、表現・アラインメント・融合のプロセスを統一しようと試みました（§3.1.2）。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> VLMsの目指したもの</p>
<p>VLMsは、画像と言語の情報をよりシームレスに扱えるように、表現、アラインメント、融合のステップを一つのモデル内で統合することを目指しました。</p>
</div>
<p>しかし、この統一は主に<span class="highlight">視覚表現</span>と<span class="highlight">クロスモーダル融合</span>に重点が置かれ、言語の深い意味理解は後回しにされがちでした。その結果、推論プロセスはしばしば単純な<span class="keyword">分類ベースのパラダイム</span>に陥り、文脈を考慮した汎用的な推論能力には限界がありました。つまり、マルチモーダル推論システムは依然として追加のモジュールやタスク特有の改良に頼っていたのです。</p>
<p>総じて、この段階の推論は、基本的な知覚処理とニューラルネットワークの計算によって<span class="keyword">暗黙的に</span>行われるものがほとんどでした。しかし、強力な言語モデルと大規模な視覚データの登場により、新興のマルチモーダル言語モデルがこの暗黙的な推論能力を強化していくことになります。🌱</p>
</div>
<h3 class="subsection-title"><i class="fas fa-bolt"></i> Stage 2: Language-Centric Short Reasoning - System-1 Reasoning</h3>
<div class="content-box">
<p>次のステージでは、<span class="keyword">マルチモーダル大規模言語モデル (MLLMs)</span> の登場により、マルチモーダル推論の世界に大きな転換が訪れます！🚀 モジュールを組み合わせるシステムから、<span class="highlight">言語中心のエンドツーエンドフレームワーク</span>へと移行しました。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card">
<h5 class="subsection-title" style="font-size: 1em; color: var(--color-primary); margin-bottom: 10px; padding-left:0; border-left: none;"><i class="fas fa-check-circle" style="color:var(--color-accent1);"></i> MLLMsの得意技</h5>
<p>これらのモデルは、以下のようなタスクで高い性能を発揮しました。</p>
<ul class="unstyled-list" style="padding-left: 10px;">
<li><i class="fas fa-comments" style="color:var(--color-accent1); margin-right:5px;"></i><strong>視覚的常識推論 (VCR):</strong> 画像を見て、常識的な質問に答える。</li>
<li><i class="fas fa-question-circle" style="color:var(--color-accent1); margin-right:5px;"></i><strong>視覚的質問応答 (VQA):</strong> 画像に関する質問に答える。</li>
<li><i class="fas fa-search-location" style="color:var(--color-accent1); margin-right:5px;"></i><strong>視覚的グラウンディング:</strong> テキスト記述を画像の特定領域に対応付ける。</li>
</ul>
</div>
<div class="info-card">
<h5 class="subsection-title" style="font-size: 1em; color: var(--color-primary); margin-bottom: 10px; padding-left:0; border-left: none;"><i class="fas fa-brain" style="color:var(--color-secondary);"></i> System-1 Reasoning とは？</h5>
<p>心理学者のダニエル・カーネマンが提唱した二重過程理論における思考システムの一つです。</p>
<ul class="unstyled-list" style="padding-left: 10px;">
<li><i class="fas fa-bolt" style="color:var(--color-secondary); margin-right:5px;"></i><strong>高速で直感的:</strong> 努力を要さず、自動的に行われる思考。</li>
<li><i class="fas fa-lightbulb" style="color:var(--color-secondary); margin-right:5px;"></i><strong>連想的:</strong> 過去の経験やパターンに基づいて判断。</li>
<li><i class="fas fa-exclamation-circle" style="color:var(--color-secondary); margin-right:5px;"></i><strong>エラーを起こしやすい:</strong> 複雑な問題や新しい状況には不向きな場合がある。</li>
</ul>
<p>このステージのMLLMの推論は、このSystem-1的思考に似ているとされています。</p>
</div>
</div>
<p>しかし、初期のMLLMアーキテクチャは、表面的なパターンマッチングや静的な知識検索に頼ることが多く、以下の点で限界がありました。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 初期MLLMの課題</p>
<ul class="unstyled-list">
<li><i class="fas fa-brain" style="color:var(--color-secondary); margin-right:5px;"></i>動的な仮説生成</li>
<li><i class="fas fa-shoe-prints" style="color:var(--color-secondary); margin-right:5px;"></i>複数ステップの論理的進行</li>
<li><i class="fas fa-cogs" style="color:var(--color-secondary); margin-right:5px;"></i>文脈に応じた適応</li>
</ul>
</div>
<p>この限界を克服するために登場したのが、<span class="keyword">思考の連鎖 (Chain-of-Thought, CoT)</span> 推論です！📝 CoTは、暗黙的な推論を明示的な中間ステップに変換し、思考プロセスをエンドツーエンドの生成に内包させます。これにより、Stage 1のマルチモーダル融合の表現能力とLLMの言語表現力を結びつけ、より文脈に即した解釈可能な推論が可能になりました。</p>
<div class="bubble-box">
<p><i class="fas fa-link" style="color:var(--color-primary); margin-right:5px;"></i><strong>CoTの役割:</strong> 暗黙の「なぜそう考えたか」を、言葉で説明できる「こう考えたから」というステップに分解するイメージです。</p>
</div>
<p>純粋な言語モデルでのCoTの成功を受け、研究者たちはこれをマルチモーダル領域に拡張し、<span class="keyword">マルチモーダル思考の連鎖 (Multimodal Chain-of-Thought, MCoT)</span> を開発しました。</p>
<p>MCoTの初期のアプローチは、主に<span class="highlight">プロンプトベースの適応</span>に焦点を当てていました（§3.2.1）。巧みに設計された指示を与えることで、モデルが段階的なマルチモーダル推論の軌跡を生成できるようにしたのです。その後の取り組みでは、推論プロセス自体を強化する方向へ進みました。</p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="feature-item" style="background-color: rgba(255, 126, 95, 0.1); border-left: 3px solid var(--color-secondary);">
<i class="fas fa-sitemap fa-2x" style="color:var(--color-secondary); margin-bottom:10px;"></i>
<h5 style="color: var(--color-secondary); margin-bottom: 5px;">構造化された推論パスの導入 (§3.2.2)</h5>
<p style="font-size: 0.9em;">推論の道を整理整頓し、より論理的に進めるようにしました。</p>
</div>
<div class="feature-item" style="background-color: rgba(92, 184, 92, 0.1); border-left: 3px solid var(--color-accent1);">
<i class="fas fa-tools fa-2x" style="color:var(--color-accent1); margin-bottom:10px;"></i>
<h5 style="color: var(--color-accent1); margin-bottom: 5px;">外部ツールと検索拡張の活用 (§3.2.3)</h5>
<p style="font-size: 0.9em;">モデルが元々持っている知識以上の情報を外部から取り込み、推論能力を拡張しました。</p>
</div>
</div>
<p>しかしながら、この段階の推論は、依然として<span class="highlight">短く反応的</span>なものが主でした。これは、高速で直感的な<span class="keyword">System-1推論</span>の特徴です。モデルは、馴染みのあるタスクや範囲が限定されたタスクには効果的でしたが、以下の点では苦戦していました。</p>
<div class="note-box" style="background-color: rgba(255, 213, 79, 0.1); border-left-color: var(--color-accent3);">
<p class="note-title" style="color: var(--color-accent3);"><i class="fas fa-puzzle-piece"></i> System-1推論の課題</p>
<ul class="unstyled-list">
<li><i class="fas fa-cloud" style="color:var(--color-accent3); margin-right:5px;"></i>抽象化</li>
<li><i class="fas fa-cubes" style="color:var(--color-accent3); margin-right:5px;"></i>構成性（部分を組み合わせて全体を理解する能力）</li>
<li><i class="fas fa-route" style="color:var(--color-accent3); margin-right:5px;"></i>計画</li>
</ul>
</div>
<p>これらの課題が、より慎重で構造化された推論パラダイムの開発を促し、次の大きな変革への舞台を整えました。✨</p>
</div>
<h3 class="subsection-title"><i class="fas fa-brain"></i> Stage 3: Language-Centric Long Reasoning - System-2 Thinking and Planning</h3>
<div class="content-box">
<p>MCoTはMLLMの推論能力を大きく前進させましたが、現実世界の複雑なマルチモーダルタスクに取り組むにはまだ力不足でした。多くのMCoT手法は、短く反応的な連鎖で動作し、これは高速で直感的な<span class="keyword">System-1推論</span>に似ています。</p>
<p>これらのアプローチは、馴染みのある問題や限定的な問題には有効ですが、以下のような高度な能力には課題がありました。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> System-1的MCoTの限界</p>
<ul class="unstyled-list">
<li><i class="fas fa-compress-arrows-alt" style="color:var(--color-secondary); margin-right:5px;"></i>抽象化 (Abstraction)</li>
<li><i class="fas fa-puzzle-piece" style="color:var(--color-secondary); margin-right:5px;"></i>構成性 (Compositionality)</li>
<li><i class="fas fa-road" style="color:var(--color-secondary); margin-right:5px;"></i>長期的な推論 (Long-horizon reasoning)</li>
<li><i class="fas fa-map-signs" style="color:var(--color-secondary); margin-right:5px;"></i>適応的な計画 (Adaptive planning)</li>
</ul>
</div>
<p>このギャップを埋めるため、近年の研究は<span class="keyword">System-2</span>に触発された推論へと向かっています。System-2推論は、より<span class="highlight">遅く、意図的で、方法論的に構造化された</span>認知的プロセスを重視します。この視点では、推論は単なる機能ではなく、知的行動そのものの核となる構成要素として扱われます。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-balance-scale"></i> System-1 vs System-2 Reasoning</p>
<div class="two-column">
<div class="column">
<p style="text-align:center; font-weight:bold; color:var(--color-secondary);"><i class="fas fa-bolt"></i> System-1</p>
<ul class="unstyled-list">
<li><i class="fas fa-fighter-jet" style="color:var(--color-secondary); margin-right:5px;"></i> 速い</li>
<li><i class="fas fa-lightbulb" style="color:var(--color-secondary); margin-right:5px;"></i> 直感的</li>
<li><i class="fas fa-running" style="color:var(--color-secondary); margin-right:5px;"></i> 自動的</li>
<li><i class="fas fa-brain" style="color:var(--color-secondary); margin-right:5px;"></i> 無意識的</li>
</ul>
</div>
<div class="column">
<p style="text-align:center; font-weight:bold; color:var(--color-accent1);"><i class="fas fa-chess-knight"></i> System-2</p>
<ul class="unstyled-list">
<li><i class="fas fa-hourglass-half" style="color:var(--color-accent1); margin-right:5px;"></i> 遅い</li>
<li><i class="fas fa-cogs" style="color:var(--color-accent1); margin-right:5px;"></i> 意図的</li>
<li><i class="fas fa-user-cog" style="color:var(--color-accent1); margin-right:5px;"></i> 努力を要する</li>
<li><i class="fas fa-microscope" style="color:var(--color-accent1); margin-right:5px;"></i> 分析的</li>
</ul>
</div>
</div>
<p style="font-size:0.9em; text-align:center; color:var(--color-gray);">Kahneman (2011) の二重過程理論に基づいています。</p>
</div>
<p>MCoTを以下の3つの重要な次元で拡張することが、より深く、転移可能で、認知的に根拠のある推論が可能な新しいクラスのモデル、すなわち<span class="keyword">大規模マルチモーダル推論モデル (Large Multimodal Reasoning Models, LMRMs)</span> への重要な道筋となっています。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));">
<div class="info-card">
<h5 class="subsection-title" style="font-size: 1em; color: var(--color-primary); margin-bottom: 10px; padding-left:0; border-left: none;"><i class="fas fa-comments" style="color:var(--color-primary); margin-right:5px;"></i> 1. 推論モダリティ (Reasoning Modalities)</h5>
<p>テキスト表現だけに頼ると、モダリティ特有の知識を取り込む能力が制限されます。最近の研究では、視覚、聴覚、言語の信号を推論の共同基盤として活用する<span class="highlight">クロスモーダル推論連鎖</span>が導入され、より豊かな意味的グラウンディングと忠実な情報統合が可能になっています（§3.3.1）。</p>
<div class="tag-list">
<span class="tag"><i class="fas fa-eye"></i> 視覚</span>
<span class="tag"><i class="fas fa-headphones"></i> 聴覚</span>
<span class="tag"><i class="fas fa-language"></i> 言語</span>
</div>
</div>
<div class="info-card">
<h5 class="subsection-title" style="font-size: 1em; color: var(--color-primary); margin-bottom: 10px; padding-left:0; border-left: none;"><i class="fas fa-lightbulb" style="color:var(--color-primary); margin-right:5px;"></i> 2. 推論パラダイム (Reasoning Paradigms)</h5>
<p>研究者たちは、より長く、質の高い連鎖を構築し、一般化された方法論に基づいた推論戦略を導入しています。これにより、モデルは複雑なタスクを自律的に分解し、多様な文脈で転移可能な手順を適用できるようになります。特筆すべきは、<span class="keyword">O1ファミリー</span>（例: GPT-4o）が、認知的に要求の高い広範なマルチモーダルタスクで人間レベルに近い性能を示している点です（§3.3.2）。</p>
</div>
<div class="info-card">
<h5 class="subsection-title" style="font-size: 1em; color: var(--color-primary); margin-bottom: 10px; padding-left:0; border-left: none;"><i class="fas fa-cogs" style="color:var(--color-primary); margin-right:5px;"></i> 3. 学習方法 (Learning Methods)</h5>
<p><span class="highlight">強化学習</span>によって強化されたマルチモーダル推論が勢いを増しています。エージェント的なデータ、反復的なフィードバック、長期的な最適化目標を取り入れることで、DeepSeek-R1のようなモデルは計画能力、堅牢性、適応的汎化能力を向上させています。この研究の流れは、スケーラブルで方法論的に根拠のあるマルチモーダル推論を重視する新世代の<span class="keyword">R1ライクモデル</span>の出現を触媒しています（§3.3.3）。</p>
</div>
</div>
<div class="bubble-box">
<p><i class="fas fa-chart-line" style="color:var(--color-primary);"></i> これらの進展は、反応的な推論パラダイムから意図的な推論パラダイムへの広範な移行を反映しており、LMRMを開かれた動的な環境で適応的なシステムレベルの知能を達成することに近づけています。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-rocket"></i> Stage 4: Towards Native Large Multimodal Reasoning Model (Prospect)</h3>
<div class="content-box">
<p>LMRMは、思考の連鎖を拡張することで複雑なタスクに取り組む可能性を示していますが、その<span class="keyword">言語中心のアーキテクチャ</span>には重大な制約があります。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> LMRMの制約</p>
<div class="two-column">
<div class="column">
<p><strong>1. モダリティの偏り:</strong></p>
<ul class="unstyled-list">
<li><i class="fas fa-eye-slash" style="color:var(--color-secondary); margin-right:5px;"></i>主に視覚と言語モダリティ（テキスト、画像、動画など）に焦点が当てられているため、現実世界の多様なデータタイプ（音声、触覚信号、センサーデータ、時系列データなど）が深く絡み合う状況での適用性が限られています。</li>
<li><i class="fas fa-brain" style="color:var(--color-secondary); margin-right:5px;"></i>言語によって生成された推論だけでは、マルチモーダルな生成的思考、反省、制御を十分にサポートできないことがよくあります。</li>
</ul>
</div>
<div class="column">
<p><strong>2. インタラクティブ性と計画能力の不足:</strong></p>
<ul class="unstyled-list">
<li><i class="fas fa-comments-dollar" style="color:var(--color-secondary); margin-right:5px;"></i>現在のモデルは、インタラクティブで長期的な推論や適応的な計画において不備を示します。</li>
<li><i class="fas fa-tasks" style="color:var(--color-secondary); margin-right:5px;"></i>静的な設定では拡張された推論連鎖を生成できますが、動的な環境とのリアルタイムで反復的な対話能力は未発達です。</li>
</ul>
</div>
</div>
</div>
<p>これらのギャップに対処するため、私たちは<span class="keyword">ネイティブ大規模マルチモーダル推論モデル (Native Large Multimodal Reasoning Models, N-LMRMs)</span> の開発を、機械知能における潜在的なパラダイムシフトとして展望しています（§4）。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-lightbulb"></i> N-LMRMsとは？</p>
<p>従来のLMRMが言語モデルに補助的なモダリティプロセッサを後付けするのに対し、N-LMRMは、マルチモーダルな理解、生成、エージェント的な推論を完全にエンドツーエンドのアーキテクチャ内で統一するように<span class="highlight">ネイティブに設計</span>されます。</p>
<p><strong>特徴：</strong></p>
<ul class="unstyled-list">
<li><i class="fas fa-project-diagram" style="color:var(--color-primary); margin-right:5px;"></i>現実世界のデータタイプは、VideoPoetのように<span class="highlight">統一された表現空間</span>内でエンコードされます。</li>
<li><i class="fas fa-database" style="color:var(--color-primary); margin-right:5px;"></i>大規模な合成データが、あらゆるモダリティインタラクションの環境における推論と計画の包括的な学習を促進します。</li>
</ul>
</div>
<p>この進化は、2つの変革的な能力にかかっています：</p>
<div class="pipeline">
<div class="pipeline-step" style="border-color: var(--color-accent1);">
<strong style="color: var(--color-accent1);"><i class="fas fa-robot"></i> 1. マルチモーダルエージェント的推論 (Multimodal Agentic Reasoning)</strong>
<p>N-LMRMはエージェント的知能を具体化し、複雑な環境との積極的で目標指向のインタラクションを可能にします。具体的には…</p>
<ul class="unstyled-list" style="margin-top: 10px; padding-left:15px;">
<li><i class="fas fa-route" style="color:var(--color-accent1); margin-right:5px;"></i><strong>長期計画:</strong> 階層的なタスク分解と、拡張されたインタラクションにおける一貫性のための記憶強化推論。</li>
<li><i class="fas fa-sync-alt" style="color:var(--color-accent1); margin-right:5px;"></i><strong>動的適応:</strong> 環境フィードバックに基づくリアルタイムの戦略調整。</li>
<li><i class="fas fa-shapes" style="color:var(--color-accent1); margin-right:5px;"></i><strong>身体化された学習:</strong> シミュレートされた、または物理的なインタラクションを通じてモデルが学習できるようにする閉ループ訓練フレームワークにより、より良い汎化を実現。</li>
</ul>
</div>
<div class="pipeline-step" style="border-color: var(--color-accent2);">
<strong style="color: var(--color-accent2);"><i class="fas fa-globe-americas"></i> 2. オムニモーダル理解と生成的推論 (Omni-Modal Understanding and Generative Reasoning)</strong>
<p>N-LMRMは、モダリティ固有のエンコーダとデコーダを超越し、スムーズなクロスモーダル合成と分析のための<span class="highlight">統一された表現空間</span>を利用します。具体的には…</p>
<ul class="unstyled-list" style="margin-top: 10px; padding-left:15px;">
<li><i class="fas fa-compress-alt" style="color:var(--color-accent2); margin-right:5px;"></i><strong>異種データ融合:</strong> 多様なデータタイプの共同埋め込み。</li>
<li><i class="fas fa-magic" style="color:var(--color-accent2); margin-right:5px;"></i><strong>文脈的マルチモーダル生成:</strong> 複合出力の一貫した作成。</li>
<li><i class="fas fa-infinity" style="color:var(--color-accent2); margin-right:5px;"></i><strong>モダリティ非依存推論:</strong> 新しい、またはあらゆるクロスモーダルデータをタスク非依存的に処理するための適応可能な処理パイプライン。</li>
</ul>
</div>
</div>
<div class="bubble-box">
<p><i class="fas fa-road" style="color:var(--color-primary);"></i>まとめると、モジュラーな知覚駆動システムから新興のネイティブマルチモーダル推論モデルへの進化は、より統一的で、適応的で、包括的な高レベルAIシステムへの明確な道筋を示しています。</p>
<p style="margin-top:10px;">続くセクションでは、各ステージ、代表的なモデル、そしてマルチモーダル推論の未来を形作る新たな研究方向について詳細に分析します。お楽しみに！🌟</p>
</div>
</div>
</div>
<div class="section-card" id="3_Roadmap_of_Multimodal_Reasoning_Models">
<h2 class="section-title"><i class="fas fa-road"></i>3 Roadmap of Multimodal Reasoning Models</h2>
<p>このセクションでは、マルチモーダル推論モデルがどのように進化してきたか、その道のりを3つの主要なステージに分けて解説します。初期のタスク特化型モジュールから、近年の言語中心の高度な推論フレームワークに至るまでの変遷を追い、それぞれのステージでの主要なアプローチや課題を明らかにします。このロードマップを理解することで、現在のマルチモーダル推論研究の位置づけと、将来の方向性が見えてくるでしょう。まさに、マルチモーダル推論モデル開発の歴史と未来を旅するようなセクションです！ 🗺️🚀</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));">
<div class="info-card glass-card">
<p class="subsection-title" style="font-size: 1.2em; color: var(--color-primary);"><i class="fas fa-cogs"></i>ステージ1</p>
<p><strong>知覚駆動のモジュラー推論</strong>：タスク特化型推論モジュールの開発</p>
<p style="font-size: 0.9em;">初期段階。個別のモジュールで表現・連携・融合・推論を行いました。</p>
</div>
<div class="info-card glass-card">
<p class="subsection-title" style="font-size: 1.2em; color: var(--color-secondary);"><i class="fas fa-brain"></i>ステージ2</p>
<p><strong>言語中心の短期推論 (System-1)</strong>：迅速で直感的な推論</p>
<p style="font-size: 0.9em;">MLLMが登場し、プロンプトベースのMCoTや構造的推論が中心となりました。</p>
</div>
<div class="info-card glass-card">
<p class="subsection-title" style="font-size: 1.2em; color: var(--color-accent1);"><i class="fas fa-sitemap"></i>ステージ3</p>
<p><strong>言語中心の長期推論 (System-2)</strong>：思考と計画</p>
<p style="font-size: 0.9em;">より複雑なタスクに対応するため、意図的で計画的な推論へと進化。強化学習も活用されます。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-layer-group"></i>3.1 Stage 1 Perception Driven Modular Reasoning <span class="badge yellow">$\mathbf { \delta } \cdot \mathbf { \delta }$</span> Developing Task-Specific Reasoning Modules</h3>
<p>マルチモーダル推論の黎明期では、使えるデータや技術に限りがありました。具体的には、<span class="keyword">マルチモーダルデータの量が多くなかった</span>こと、<span class="keyword">ニューラルネットワークの構造がまだ発展途上だった</span>こと、そして<span class="keyword">洗練された学習方法が少なかった</span>ことなどが挙げられます。これらの制約から、特定のタスクに特化したモデルが開発されることになりました。これらのモデルは、マルチモーダルな情報を「表現」「連携」「融合」「推論」するために、それぞれ異なるモジュール（部品のようなもの）を組み合わせて使うのが一般的でした。</p>
<p>このステージのモデルは、そのアーキテクチャや学習アプローチによって、大きく2つのタイプに分けられます。</p>
<div class="two-column">
<div class="column">
<div class="bubble-box">
<p><strong><i class="fas fa-puzzle-piece"></i>モジュラー推論ネットワーク (Modular Reasoning Networks)</strong></p>
<p>個々の機能を担当するモジュールを組み合わせて推論するネットワークです。</p>
</div>
</div>
<div class="column">
<div class="bubble-box">
<p><strong><i class="fas fa-brain"></i>事前学習済み視覚言語モデル(VLM)ベースのモジュラー推論 (Pretrained Vision-Language Models (VLMs) based Modular Reasoning)</strong></p>
<p>大規模データで事前に学習されたVLMを基盤としつつ、モジュラーな推論を行うアプローチです。</p>
</div>
</div>
</div>
<h4 class="subsection-title"><i class="fas fas fa-cogs"></i>3.1.1 Modular Reasoning Networks</h4>
<p>初期のアプローチでは、<span class="keyword">汎用的なCNN（畳み込みニューラルネットワーク）</span>と<span class="keyword">LSTM（長短期記憶）</span>のバックボーン（基本的な構造）を使って、マルチモーダルデータから答えを導き出していました。しかし、これらはすぐに、知覚的な手がかり（画像の中の特定の領域やテキストの特定の単語など）に基づいて推論をモジュール化するアーキテクチャによって改良されていきました。いくつかの代表的なものを紹介します。</p>
<div class="info-grid">
<div class="info-card">
<p><strong><i class="fas fa-project-diagram"></i>Neural Module Networks (NMN)</strong> (Andreas et al., 2016)</p>
<p>タスクに応じて特化したモジュールを動的に組み合わせることで、視覚特徴とテキスト特徴を構成的に処理しました。これは、特徴を単純に混ぜ合わせる静的な融合方法に取って代わるものでした。例えば、質問文「赤い物体は何ですか？」に対して、「赤い物体を探すモジュール」と「それが何かを特定するモジュール」を組み合わせるイメージです。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-sync-alt"></i>Hierarchical Co-Attention (HieCoAtt)</strong> (Lu et al., 2016)</p>
<p>質問の単語レベル、句レベル、文レベルといった意味階層と、画像の領域を階層的に対応付けるためのモジュラーなクロスモーダルアテンションを導入しました。これにより、より詳細な情報の連携が可能になりました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-compress-arrows-alt"></i>Multimodal Compact Bilinear Pooling (MCB)</strong> (Fukui et al., 2016)</p>
<p>効率的で学習可能なバイリニアモジュール（2つの入力間の相互作用を捉える仕組み）を通じて、特徴間の相互作用を最適化しました。これにより、計算コストを抑えつつ、豊かな特徴表現を得ることができました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-layer-group"></i>Stacked Attention Networks (SANs)</strong> (Yang et al., 2016)</p>
<p>視覚特徴に対してアテンション（注意機構）を繰り返し適用することで、段階的に推論を深めていくモジュール構造を実現しました。画像内の複数の重要な領域に順番に注目するようなイメージです。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-memory"></i>Dynamic Memory Networks (DMN)</strong> (Xiong et al., 2016)</p>
<p>時系列的な入力（例えば、物語や一連の出来事）に対して、複数のエピソードにまたがる推論を行うためにメモリモジュールを統合しました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-sitemap"></i>ReasonNet</strong> (Ilievski &amp; Feng, 2017)</p>
<p>推論をエンティティ（物体や概念）とリレーション（関係性）のモジュールに分解し、構造化された推論を行いました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-object-group"></i>UpDn (Bottom-Up and Top-Down Attention)</strong> (Anderson et al., 2018)</p>
<p>画像中の物体レベルの特徴を優先的に処理するためのボトムアップアテンション（画像から顕著な領域を検出）とトップダウンアテンション（質問に基づいて関連領域に注目）を導入しました。これはVQA-v2などのタスクで効果を発揮しました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-calculator"></i>MAC (Memory, Attention, and Composition)</strong> (Hudson &amp; Manning, 2018)</p>
<p>メモリ拡張型制御ユニット（MACセル）を用いて、反復的な構成的推論を行いました。各ステップで情報を読み書きし、段階的に答えを構築します。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-project-diagram"></i>BAN (Bilinear Attention Networks)</strong> (Kim et al., 2018)</p>
<p>バイリニアアテンションネットワークを用いて、モダリティ間で高次の相互作用を捉えました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-video"></i>Heterogeneous Memory Enhanced Multimodal Attention (HeteroMemory)</strong> (Fan et al., 2019)</p>
<p>動画におけるモジュール性を拡張し、外観モジュールと動きモジュールを時間的な融合と同期させました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-link"></i>MuRel (Multimodal Relational Network)</strong> (Cadene et al., 2019)</p>
<p>物体ペア間の関係ネットワークとして推論をモデル化し、詳細な推論を実現しました。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-search-plus"></i>MCAN (Modular Co-Attention Network)</strong> (Yu et al., 2019b)</p>
<p>自己アテンションとガイド付きアテンションを伴うモジュラーな協調アテンションを用いて、深いクロスモーダル推論を行いました。</p>
</div>
</div>
<p>これらの進歩は、<span class="keyword">アテンションメカニズム</span>、<span class="keyword">メモリモジュール</span>、そして<span class="keyword">構成的なモジュール</span>といった<mark>知覚駆動型の設計</mark>が、特定のタスク要件に合わせた詳細な推論をいかに促進するかを示しています。</p>
<p>しかし、<span class="keyword">Transformer (Vaswani et al., 2017)</span> アーキテクチャの登場と、<span class="keyword">事前学習-ファインチューニング</span>という学習スキームの組み合わせは、マルチモーダルな表現、連携、融合を大きく前進させました。特に、Transformerベースの事前学習済みVLMは、データレベルおよびモデル内部での視覚情報とテキスト情報の統合を強化し、これにより知覚駆動の推論能力を可能にしました。</p>
<img alt="Table 1: Perception-Driven Modular Reasoning Methods" class="research-image" src="table1.png"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">表1: 知覚駆動型マルチモーダルモジュラー推論の初期段階における分類された研究。VLMとMLLMがマルチモーダル推論タスクの性能向上に重要な役割を果たしています。</p>
<h4 class="subsection-title"><i class="fas fa-microchip"></i>3.1.2 Vision-Language Models-based Modular Reasoning</h4>
<p>これらの<span class="keyword">視覚言語モデル (VLMs)</span> は、大規模な画像とテキストのペアデータを用いて学習され、知覚駆動型の推論タスク、例えば <span class="badge blue">NLVR2</span> (Suhr et al., 2018)、<span class="badge blue">TVQA</span> (Lei et al., 2018)、<span class="badge blue">GQA</span> (Hudson &amp; Manning, 2019)、<span class="badge blue">OK-VQA</span> (Marino et al., 2019)、<span class="badge blue">VCR</span> (Zellers et al., 2019)、<span class="badge blue">ScienceQA</span> (Saikh et al., 2022) などの性能を向上させました。</p>
<p>具体的には、VLMsは<span class="keyword">Transformer</span>アーキテクチャを導入し、大規模な画像-テキストデータを利用して、マルチモーダルな<mark>表現</mark>、<mark>知覚</mark>、<mark>融合</mark>、そして<mark>推論</mark>のプロセスを統合しました。以下に、事前学習済みVLMベースのモジュラー推論の3つの主要なアプローチを紹介します。</p>
<div class="pipeline">
<div class="pipeline-step glass-card">
<p><strong><i class="fas fa-arrows-alt-h"></i>1. デュアルエンコーダ対照学習推論 (Dual-Encoder Contrastive Reasoning)</strong></p>
<p>これらのモデルは、<span class="keyword">デュアルストリームアーキテクチャ</span>（視覚情報用とテキスト情報用にそれぞれ独立したエンコーダを持つ構造）と<span class="keyword">対照学習 (Contrastive Learning)</span> を活用します。対照学習とは、似ているペア（例：画像とその正しい説明文）の類似度を高く、似ていないペア（例：画像と無関係な説明文）の類似度を低くするように学習する手法です。これにより、クロスモーダルな相互作用を通じて、視覚特徴とテキスト特徴を動的に連携させ、推論を行います。</p>
<ul class="unstyled-list">
<li>✏️ <strong>ViLBERT</strong> (Lu et al., 2019): クロスモーダルアテンションを備えたデュアルストリームTransformerを使用し、動的な特徴連携を実現。</li>
<li>✏️ <strong>LXMERT</strong> (Tan &amp; Bansal, 2019): デュアルエンコーダ間に対話レイヤーを追加し、関係性埋め込み上で推論。</li>
<li>✏️ <strong>CLIP</strong> (Radford et al., 2021): 対照的ペア（画像とテキスト）を用いて事前学習し、連携された埋め込み表現を通じてゼロショット推論（学習データにないタスクを解くこと）を実現。</li>
<li>✏️ <strong>ALBEF</strong> (Li et al., 2021b): 対照学習と運動量蒸留（Momentum Distillation: 教師モデルの知識を効率的に生徒モデルに伝える手法）を統合し、蒸留された埋め込み上で推論。</li>
<li>✏️ <strong>METER</strong> (Dou et al., 2022): モジュラーなエンコーダ・デコーダフレームワークでデュアルストリーム推論を強化し、頑健な連携を実現 (例: VCRタスク)。</li>
<li>✏️ <strong>SimVLM</strong> (Wang et al., 2021): プレフィックスベースの事前学習（入力の先頭に特定の情報を付加して学習）を用いて視覚と言語を連携させ、効率的な推論を実現。</li>
<li>✏️ <strong>VLMo</strong> (Bao et al., 2022b): 混合モダリティエキスパート (Mixture-of-Modality-Experts) フレームワークを導入し、柔軟なクロスモーダル推論を実現。</li>
<li>✏️ <strong>CoCa</strong> (Yu et al., 2022): 対照的ヘッドと生成的ヘッドを統合し、多目的な推論を実現 (例: NLVR2タスク)。</li>
<li>✏️ <strong>BLIP</strong> (Li et al., 2022): 画像-テキストTransformerモジュールであるQ-formerを導入し、対照的な目的関数を用いた視覚言語事前学習により、ブートストラップされた連携（自己改善的に連携を強化する手法）を通じて推論。</li>
</ul>
</div>
<div class="pipeline-step glass-card">
<p><strong><i class="fas fa-network-wired"></i>2. シングルTransformerバックボーン対話型推論 (Single-Transformer-Backbone Interactive Reasoning)</strong></p>
<p>このパラダイムでは、視覚入力とテキスト入力を<span class="keyword">単一のTransformer</span>に埋め込み、統一されたエンコーディング手法を通じて直接的なクロスモーダル推論を可能にします。つまり、1つの大きな脳で両方の情報を同時に処理するイメージです。</p>
<ul class="unstyled-list">
<li>✏️ <strong>VisualBERT</strong> (Li et al., 2019), <strong>UNITER</strong> (Chen et al., 2020), <strong>VL-BERT</strong> (Su et al., 2019): 単一のTransformer内で視覚-テキスト入力を融合し、共同コンテキストエンコーディングや強化されたクロスモーダル事前学習を通じて推論。</li>
<li>✏️ <strong>PixelBERT</strong> (Huang et al., 2020): CNNとTransformerアーキテクチャを用いてピクセルレベルで処理し、詳細な推論 (例: NLVR<sup>2</sup>タスク) を実現。</li>
<li>✏️ <strong>UniVL</strong> (Luo et al., 2020): 単一のTransformerで動画-言語推論を統一し、時間的なクロスモーダルタスク (例: TVQAタスク) に対応。</li>
<li>✏️ <strong>Oscar</strong> (Li et al., 2020), <strong>VinVL</strong> (Zhang et al., 2021a): 物体タグや強化された視覚特徴を統一Transformer内で利用し、意味的推論 (例: VCR, GQAタスク) を向上。</li>
<li>✏️ <strong>ERNIE-ViL</strong> (Yu et al., 2021): シーングラフ（画像内の物体とその関係性をグラフで表現したもの）知識を単一Transformerに統合し、構造化された視覚-言語相互作用を通じて構成的推論を強化。</li>
<li>✏️ <strong>UniT</strong> (Hu &amp; Singh, 2021): 共有自己アテンションバックボーンでマルチモーダルタスクを効率化し、統一的な推論を実現。</li>
<li>✏️ <strong>PaLI</strong> (Chen et al., 2022b): 多言語フレームワークで単一Transformer推論をスケールアップし、クロスリンガル推論 (例: OK-VQAタスク) を実現。</li>
<li>✏️ <strong>Flamingo</strong> (Alayrac et al., 2022): クロスアテンションを用いて、動的な視覚-テキスト相互作用を優先。</li>
<li>✏️ <strong>BEiT-3</strong> (Wang et al., 2022b): マスク化データモデリング（入力の一部を隠して予測させる学習法）を採用し、視覚-言語学習を統一。</li>
<li>✏️ <strong>OFA</strong> (Wang et al., 2022a), <strong>BLIP-2</strong> (Li et al., 2023d): 統一マルチモーダルフレームワークやクエリTransformerを導入し、効率を改善してクロスモーダル推論 (例: VQA-v2タスク) で優れた性能を発揮。</li>
<li>✏️ <strong>Kosmos-1</strong> (Huang et al., 2023b), <strong>Kosmos-2</strong> (Peng et al., 2023): インターリーブ入力処理（複数のモダリティの情報を交互に処理）やグラウンディング能力（テキスト情報を画像内の特定領域に対応付ける能力）を可能にし、柔軟なマルチモーダル理解と正確な物体特定を実現。</li>
</ul>
</div>
<div class="pipeline-step glass-card">
<p><strong><i class="fas fa-project-diagram"></i>3. マルチモーダルLLMベースの暗黙的推論 (Multimodal LLMs-based Implicit Reasoning)</strong></p>
<p>このアプローチでは、視覚入力を大規模言語モデル (LLM) のテキスト空間に投影し、LLMの<span class="keyword">文脈的推論能力</span> (Li et al., 2023e) を活用してマルチモーダル推論の性能を向上させます。これらのモデルは、事前学習済みの視覚エンコーダと大規模言語モデルから構成され、「Vision-Encoder-LLM」という構造をとります。</p>
<ul class="unstyled-list">
<li>✏️ <strong>CLIP-Cap</strong> (Mokady et al., 2021): CLIPの視覚特徴をLLMに投影し、推論やキャプション生成タスクを実行。</li>
<li>✏️ <strong>LLaVA</strong> (Liu et al., 2023a): ViT (Vision Transformer) とLLMの統合を調整することで対話型推論を可能にし、インタラクティブなタスクや複雑なVQAタスクに対応。</li>
<li>✏️ <strong>MiniGPT-4</strong> (Zhu et al., 2023), <strong>InstructBLIP</strong> (Dai et al., 2023): ViTを凍結されたLLMに投影レイヤーや指示チューニング（特定の指示に従うようにモデルを微調整）を通じて連携させ、視覚-テキスト推論を効率化。</li>
<li>✏️ <strong>Qwen-VL</strong> (Bai et al., 2023): 空間認識可能なViTを組み込み、空間的に複雑なタスクに対するグラウンディング推論を強化。</li>
<li>✏️ <strong>mPLUG-Owl</strong> (Ye et al., 2023), <strong>LMEye</strong> (Li et al., 2024l), <strong>Otter</strong> (Li et al., 2023a): モジュラーな視覚エンコーダをLLMと統合し、指示追従や文脈内学習（与えられた文脈に基づいて学習）によるマルチモーダル推論を実現。</li>
</ul>
</div>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-exclamation-circle"></i>課題点</p>
<p>これらの3つのアプローチにおけるアーキテクチャの革新は、タスクにおけるマルチモーダル推論を大幅に進歩させましたが、事前に定義された特徴の連携や文脈エンコーディングに依存しているため、反復的または構成的な推論を必要とする複雑な多段階の推論シナリオへの対応能力がしばしば制限されます。これらの制約は、LLMの開発のような大規模モデルにおける<span class="keyword">マルチモーダル思考連鎖 (Multimodal Chain-of-Thought, MCoT) 推論</span>（セクション3.2で詳述）の必要性を浮き彫りにします。MCoTは、タスクを動的に分解し、中間的な推論ステップを統合し、知覚と推論を適応的に連携させることで、多様なマルチモーダルな課題に対してより頑健で一般化可能な性能を発揮できます。</p>
</div>
<h4 class="subsection-title"><i class="fas fa-lightbulb"></i>Takeaways: Perception-Driven Modular Reasoning</h4>
<p>初期のマルチモーダルモデルは、主に情報の<span class="keyword">表現 (representation)</span>、<span class="keyword">連携 (alignment)</span>、<span class="keyword">融合 (fusion)</span> に焦点を当てていました。これらのモデルにおける推論は、多くの場合<mark>暗黙的 (implicit)</mark>であり、通常、タスク固有の独立した推論モジュールが必要でした。</p>
<p>最近では、マルチモーダル大規模言語モデル（MLLM）、特に<span class="keyword">視覚エンコーダ-言語モデル (vision encoder-language model)</span> 構造を採用するものが、<mark>統一されたマルチモーダル推論アーキテクチャ</mark>を実現し、複数のタスクにまたがる推論性能の向上を示しています。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-cogs fa-2x" style="color: var(--color-primary);"></i>
<p><strong>初期の焦点</strong><br/>表現・連携・融合</p>
</div>
<div class="feature-item">
<i class="fas fa-brain fa-2x" style="color: var(--color-secondary);"></i>
<p><strong>推論の性質</strong><br/>暗黙的、タスク特化モジュール</p>
</div>
<div class="feature-item">
<i class="fas fa-sync-alt fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>近年の進展 (MLLM)</strong><br/>統一推論アーキテクチャ、マルチタスク性能向上</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-fast-forward"></i>3.2 Stage 2 Language-Centric Short Reasoning - System-1 Reasoning</h3>
<p>大規模なマルチモーダル事前学習の登場により、<span class="keyword">マルチモーダル大規模言語モデル (MLLM)</span> は、<mark>創発的な推論能力 (emergent reasoning capabilities)</mark> を示し始めています。創発的とは、個々の要素の性質からは予測できないような高度な能力が、システム全体として現れることを指します。しかし、そのような推論はしばしば<mark>浅い (shallow)</mark>ものであり、明示的な論理プロセスではなく、主に<mark>暗黙的な相関関係 (implicit correlations)</mark>に依存しています。</p>
<p><span class="keyword">マルチモーダル思考連鎖 (Multimodal Chain-of-Thought, MCoT)</span> は、この限界を軽減するためのシンプルかつ効果的なアプローチとして登場しました。MCoTは、<mark>中間的な推論ステップ (intermediate reasoning steps)</mark> を組み込むことにより、広範な教師データや大幅なアーキテクチャ変更を必要とせずに、クロスモーダルな連携、知識の統合、文脈的グラウンディングを改善します。</p>
<p>このステージでは、既存のアプローチを3つのパラダイムに分類します。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));">
<div class="info-card">
<p><strong><i class="fas fa-comment-dots"></i>1. プロンプトベースのMCoT (Prompt-based MCoT)</strong></p>
<p>人間が設計したプロンプト（指示文）によって、モデルに段階的な推論を行わせる手法です。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-sitemap"></i>2. 事前定義されたパターンによる構造的推論 (Structural reasoning with predefined patterns)</strong></p>
<p>あらかじめ決められた推論の「型」や手順に従って、モデルが推論を進める手法です。</p>
</div>
<div class="info-card">
<p><strong><i class="fas fa-tools"></i>3. 軽量な外部モジュールによるツール拡張推論 (Tool-augmented reasoning with lightweight external modules)</strong></p>
<p>計算機や検索エンジンなどの外部ツールを利用して、モデルの推論能力を補強する手法です。</p>
</div>
</div>
<img alt="Figure 4: Taxonomy and representative methods of structural reasoning in multimodal chain-of-thought." class="research-image" src="structural_reasoning_taxonomy.jpg"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">図4: マルチモーダル思考連鎖における構造的推論の分類と代表的な手法。</p>
<h4 class="subsection-title"><i class="fas fa-pencil-alt"></i>3.2.1 Prompt-based MCoT</h4>
<p><span class="keyword">プロンプトベースのマルチモーダル思考連鎖 (Prompt-based MCoT)</span> 手法は、テキストベースのCoTパラダイムをマルチモーダルな文脈に拡張するものです。これにより、追加の学習をほとんど必要とせずに、高い解釈可能性を持ってモダリティを横断する段階的な推論が可能になります。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p><strong><i class="fas fa-eye"></i>視覚的推論 (Visual Reasoning)</strong></p>
<ul class="unstyled-list">
<li>📌 <strong>IPVR (Chen et al., 2023c)</strong>: 「見る→考える→確認する (see-think-confirm)」という構造化されたプロンプティングフレームワークを提案。LLMを視覚的グラウンディング（テキスト情報を画像内の特定領域に対応付けること）と根拠検証を通じて誘導します。</li>
<li>📌 <strong>VIC (Zheng et al., 2024c)</strong>: 幻覚（誤った情報を生成すること）を軽減し精度を向上させるために、視覚入力の前にテキストベースの推論連鎖をプロンプトします。</li>
</ul>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-video"></i>動画理解 (Video Understanding)</strong></p>
<ul class="unstyled-list">
<li>📌 <strong>VoT (Fei et al., 2024)</strong>: 時空間シーングラフを活用し、低レベルの知覚から高レベルの解釈へと進む段階的な推論をプロンプトします。</li>
<li>📌 <strong>VideoAgent (Wang et al., 2024h)</strong>: LLMが調整するシステムで、最小限のフレーム使用で長い動画から重要な情報を反復的にプロンプトします。</li>
<li>📌 <strong>LET (Himakunthala et al., 2023)</strong>: VIPデータセット上でフレームごとのプロンプティング戦略を採用し、動画の補完や予測のための時間的推論を誘導します。</li>
</ul>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-cogs"></i>特定領域応用 (Domain-specific Applications)</strong></p>
<ul class="unstyled-list">
<li>📌 <strong>PKRD-CoT (Luo et al., 2024)</strong>: 自動運転の推論を「知覚・知識・推論・意思決定」の段階に構造化するゼロショットプロンプティングフレームワークを導入。</li>
<li>📌 <strong>LPE (Xie et al., 2025a)</strong>: 発話内容と感情的な手がかりに関するプロンプトベースの推論を用いて、共感的な応答を生成します。</li>
<li>📌 <strong>EMER (Lian et al., 2023)</strong>: マルチモーダル感情認識においてプロンプティングを適用し、単一モダリティの手がかりを統合して解釈可能な予測を生成します。</li>
</ul>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-tasks"></i>タスク指向推論 (Task-oriented Reasoning)</strong></p>
<ul class="unstyled-list">
<li>📌 <strong>CoTDet (Tang et al., 2023)</strong>: 物体検出のためのアフォーダンス知識（物体が持つ機能や用途に関する知識）を抽出するために多段階プロンプティングを使用します。</li>
<li>📌 <strong>AntGPT (Zhao et al., 2023)</strong>: 動画ベースの行動シーケンスから人間の目標や時間的ダイナミクスを推測するようLLMにプロンプトします。</li>
<li>📌 <strong>CPSeg (Li, 2024)</strong>: セグメンテーションを強化するために、テキストとピクセルレベルのセマンティクスを連携させる思考連鎖プロンプトを形成します。</li>
</ul>
</div>
</div>
<h4 class="subsection-title"><i class="fas fa-project-diagram"></i>3.2.2 Structural Reasoning</h4>
<p><span class="keyword">プロンプトベースのMCoT手法</span>が、手作りの事例やゼロショットプロンプティング（事前の例示なしで指示のみを与える方法）を通じて推論行動を誘導するのに対し、<span class="keyword">構造的推論 (Structural Reasoning)</span> は、<mark>教師あり学習を通じて推論パターンを学習する</mark>ことに焦点を当てています。</p>
<p>明示的な手続き的構造をモデルに統合することで、これらのアプローチは、緩やかに誘導された推論を標準化された段階的なプロセスに変換し、複雑なマルチモーダルタスクにおける<mark>スケーラビリティ</mark>、<mark>信頼性</mark>、<mark>効率</mark>を向上させます。</p>
<p>構造的推論は、主に以下の3つのタイプに分類されます。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));">
<div class="info-card glass-card">
<p><strong><i class="fas fa-list-ol"></i>(i) 根拠構築 (Rationale Construction)</strong></p>
<p>解釈可能な足場として、原子的な推論ステップ（基本的な推論の単位）を生成することを学習します。つまり、なぜそのような結論に至ったのか、その理由や思考プロセスを段階的に説明できるようにします。</p>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-clipboard-check"></i>(ii) 定義済み推論手順 (Defined Reasoning Procedures)</strong></p>
<p>構造化されたテキスト推論スキーム（例えば、問題解決の手順や論理展開のパターン）をマルチモーダルな設定に適応させます。テキストで有効だった推論の型を、画像や音声を含むタスクにも応用するイメージです。</p>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-shapes"></i>(iii) モダリティ特化型構造的推論 (Modality-specific Structural Reasoning)</strong></p>
<p>視覚、聴覚、または身体的入力（ロボットのセンサー情報など）の特性によりよく整合するように、モダリティを意識した制約や設計をさらに組み込みます。例えば、画像内の空間情報を重視した推論構造や、音声の時間的変化を捉える推論構造などです。</p>
</div>
</div>
<div class="content-box">
<p class="subsection-title" style="font-size: 1.1em; color: var(--color-accent2);"><i class="fas fa-comment-alt"></i>Rationale Construction (根拠構築)</p>
<p>マルチモーダルな文脈における構造的推論の基礎は、効果的な<span class="keyword">根拠学習 (Rationale Learning)</span> アプローチから始まります。</p>
<ul class="unstyled-list">
<li>📌 <strong>Multimodal-CoT (Zhang et al., 2023g)</strong>: 根拠生成と回答予測を分離する2段階のフレームワークを提案し、幻覚（誤情報生成）を低減。</li>
<li>📌 <strong>T-sciq (Wang et al., 2024e)</strong>: 教師LLMを活用して様々な複雑さの根拠を生成し、根拠の質が推論精度に重要であることを示しました。</li>
<li>📌 <strong>G-CoT (Ma et al., 2024)</strong>: 自動運転分野で、Dolphinsというモデルが根拠を視覚情報や過去の運転信号と明示的に結びつけ、より根拠のある推論を実現。</li>
<li>📌 <strong>MC-CoT (Tan et al., 2024a)</strong>: 自己整合性戦略（複数の候補から最も確からしいものを選択）を用いて複数の候補の中から最も正確な根拠を選択し、小規模モデルの性能を向上。</li>
<li>📌 <strong>CLoT (Zhong et al., 2024a)</strong>: Leap-of-Thought（思考の飛躍）を介して非線形で探索的な根拠構築を促進し、創造的な推論をサポート。</li>
</ul>
</div>
<div class="content-box">
<p class="subsection-title" style="font-size: 1.1em; color: var(--color-accent2);"><i class="fas fa-tasks"></i>Defined Reasoning Procedure (定義済み推論手順)</p>
<p>テキスト推論プロセスの解釈可能性を高めるために、多くの研究が構造化された推論段階を提案しています。</p>
<ul class="unstyled-list">
<li>📌 <strong>Cantor (Gao et al., 2024c)</strong>: 知覚段階（物体、色、形などの低レベル属性を抽出）と意思決定段階（これらの特徴を統合して問題解決）を区別。</li>
<li>📌 <strong>TextCoT (Luan et al., 2024)</strong>: 3段階プロセスを採用。「画像概要」→「粗い位置特定」→「詳細観察」を経て正確な回答を生成。</li>
<li>📌 <strong>Grounding-Prompter (Chen et al., 2023a)</strong>: 「全体理解」→「ノイズ評価」→「部分理解」→「予測」という手順で、大域的・局所的セマンティクスを徐々に統合し、ノイズに強く、時間的境界の知覚を改善。</li>
<li>📌 <strong>Audio-CoT (Ma et al., 2025d)</strong>: 3つの思考連鎖パラダイムを利用。「Manual-CoT」（手作り例で誘導）、「Zero-Shot-CoT」（単純プロンプトでゼロショット推論）、「Desp-CoT」（音声記述生成で推論促進）。</li>
<li>📌 <strong>VIC (Zheng et al., 2024c)</strong>: 視覚入力を統合して最終的な根拠を形成する前に、タスクをテキストベースのサブステップに分割。</li>
<li>📌 <strong>Visual Sketchpad (Hu et al., 2024b)</strong>: スケッチプロセス中に根拠を「思考」「行動」「観察」のフェーズに整理。</li>
<li>📌 <strong>DetCoT (Wu et al., 2024c)</strong>: VQA推論をサブタスクとレビューの組み合わせとして形式化。</li>
<li>📌 <strong>BDoG (Zheng et al., 2024b)</strong>: 独自の担当者による専用の議論・要約パイプラインを利用。</li>
<li>📌 <strong>CoTDet (Tang et al., 2023)</strong>: 「列挙」「分析」「要約」という人間らしい手順で物体検出を達成。</li>
<li>📌 <strong>CoCoT (Zhang et al., 2024a)</strong>: 入力の類似点と相違点を体系的に比較。</li>
<li>📌 <strong>SegPref (Wang et al., 2024j)</strong>: 「全体理解」「音源物体フィルタリング」「ノイズ除去」を通じて、音を発する物体を視覚空間で正確に特定。</li>
<li>📌 <strong>EMMAX (Sun et al., 2024b)</strong>: グラウンディングされた計画アプローチと予測的動作技術を組み合わせる。</li>
</ul>
</div>
<div class="content-box">
<p class="subsection-title" style="font-size: 1.1em; color: var(--color-accent2);"><i class="fas fa-draw-polygon"></i>Multimodal Specific Structural Reasoning (モダリティ特化型構造的推論)</p>
<p>最近の研究では、マルチモーダル入力、特に視覚-言語タスク特有の課題に合わせた、モダリティ固有の推論構造が導入されています。</p>
<ul class="unstyled-list">
<li>📝 <strong>領域ベースのグラウンディング</strong>:
                <ul>
<li><strong>CoS (Liu et al., 2024g) &amp; TextCoT (Luan et al., 2024)</strong>: 質問に基づいて関心領域を特定し、次に局所的な検査を行う2段階パイプラインを採用。解像度を損なうことなく多粒度推論を可能にします。</li>
<li><strong>DCoT (Jia et al., 2024)</strong>: バウンディングボックスによるグラウンディングと意味的に類似した例の検索を組み合わせるデュアルガイダンス機構を導入。詳細かつ文脈を意識した推論を強化します。</li>
</ul>
</li>
<li>📝 <strong>空間的グラウンディングを超えて</strong>:
                <ul>
<li><strong>CoT-PT (Ge et al., 2023)</strong>: プロンプトチューニングを通じて視覚的・テキスト的埋め込みを統合し、粗から密への抽象化を通じて視覚的概念表現を徐々に洗練します。</li>
</ul>
</li>
<li>📝 <strong>テキスト誘導による意味的強化</strong>:
                <ul>
<li><strong>Shikra (Chen et al., 2023b) &amp; TextCoT (Luan et al., 2024)</strong>: 画像キャプションを高レベルの意味的手がかりとして活用し、空間的アテンションと物体グラウンディングを誘導。外部検出モジュールへの依存を減らし、より解釈可能な指示対象推論を促進します。</li>
<li><strong>DDCoT (Zheng et al., 2023) &amp; AVQA-CoT (Li et al., 2024e)</strong>: 古典的なCoTフレームワークに触発され、複雑な視覚的または視聴覚的クエリを一連のサブ質問に分解し、構成的推論とモダリティを横断するマルチホップ推論を改善します。</li>
</ul>
</li>
<li>📝 <strong>身体化シナリオへの拡張</strong>:
                <ul>
<li><strong>E-CoT (Zawalski et al., 2024)</strong>: タスクの再表現、計画、低レベルの行動実行を交互に行うことで、構造的推論を身体化シナリオに拡張。視覚-言語-行動モデルにおいて、意味レベルと感覚運動レベルの両方にまたがる推論連鎖の必要性を強調します。</li>
</ul>
</li>
</ul>
</div>
<img alt="Table 2: Structural Reasoning Methods" class="research-image" src="table2.png"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">表2: 構造的推論。緩やかに誘導された推論を、明示的な手続き構造をモデルに統合することで標準化された段階的プロセスに変換し、複雑なマルチモーダルタスクにおけるスケーラビリティ、信頼性、効率を向上させます。</p>
<h4 class="subsection-title"><i class="fas fa-check-circle"></i>Takeaways: Structural Reasoning</h4>
<p>構造的推論手法は、<span class="keyword">質問の分解</span>、<span class="keyword">視覚的グラウンディング</span>、<span class="keyword">キャプション生成</span>、<span class="keyword">要約</span>、<span class="keyword">フェーズ分け</span>、<span class="keyword">画像処理</span>といったモジュラーなサブタスクを統合することで、標準化された推論ワークフローを定義します。これらのアプローチは、生成タスクを明示的な段階に整理することで、<mark>解釈可能性 (interpretability)</mark> と<mark>一貫性 (consistency)</mark> を向上させます。</p>
<p>最近のトレンドでは、視覚、聴覚、または身体的入力との連携をより良くするために、<span class="keyword">モダリティを意識した設計 (modality-aware designs)</span> も取り入れられています。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-sitemap fa-2x" style="color: var(--color-primary);"></i>
<p><strong>標準化されたワークフロー</strong><br/>サブタスク統合による明確な推論手順</p>
</div>
<div class="feature-item">
<i class="fas fa-tasks fa-2x" style="color: var(--color-secondary);"></i>
<p><strong>主なサブタスク例</strong><br/>質問分解、視覚的グラウンディング、キャプション生成</p>
</div>
<div class="feature-item">
<i class="fas fa-search-plus fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>向上する特性</strong><br/>解釈可能性、一貫性</p>
</div>
<div class="feature-item">
<i class="fas fa-shapes fa-2x" style="color: var(--color-accent2);"></i>
<p><strong>最新トレンド</strong><br/>モダリティを意識した設計</p>
</div>
</div>
<h4 class="subsection-title"><i class="fas fa-puzzle-piece"></i>3.2.3 Externally Augmented Reasoning</h4>
<p><span class="keyword">外部拡張推論 (Externally Augmented Reasoning)</span> は、モデル固有の推論能力の限界を補うために、<mark>有利なアルゴリズム</mark>、<mark>補助的なツール</mark>、または<mark>専門家モジュール</mark>を導入するアプローチです。これらのコンポーネントは、推論時に統合されるか、訓練中に結合され、より柔軟で、スケーラブルで、タスクに特化した推論ワークフローを可能にします。</p>
<p>中核となる推論ステップを基本モデルから切り離すことにより、このような手法は、<span class="keyword">長期的な計画</span>、<span class="keyword">正確なグラウンディング</span>、および<span class="keyword">動的またはドメイン固有の情報へのアクセス</span>をサポートします。外部拡張手法は、以下の4つのカテゴリに分類されます。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card glass-card">
<p><strong><i class="fas fa-search"></i>(i) 探索アルゴリズム強化MCoT</strong></p>
<p>様々な探索アルゴリズムを介して推論空間を探索します。</p>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-tools"></i>(ii) ツールベース拡張</strong></p>
<p>外部の言語ツールやシステムを活用して推論実行を誘導します。</p>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-database"></i>(iii) 検索拡張推論</strong></p>
<p>外部ソースから関連するマルチモーダル知識を推論パスに組み込みます。</p>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-photo-video"></i>(iv) マルチモーダル強化</strong></p>
<p>知覚駆動型推論をサポートするために特化したマルチモーダルモジュールを組み込みます。</p>
</div>
</div>
<div class="content-box">
<p class="subsection-title" style="font-size: 1.1em; color: var(--color-accent1);"><i class="fas fa-project-diagram"></i>Search Algorithm Enhanced MCoT (探索アルゴリズム強化MCoT)</p>
<p>探索戦略駆動型のMCoTアプローチは、モデルが推論プロセス全体を通じて推論の軌道を動的にナビゲートし、最適化することを可能にします。</p>
<ul class="unstyled-list">
<li>📌 <strong>MM-ToT (Gomez, 2023)</strong>: GPT-4とStable Diffusionを活用し、深さ優先探索（DFS）と幅優先探索（BFS）アルゴリズムを用いて、0.0～1.0の指標スケールに従って最適なマルチモーダル出力を特定します。</li>
<li>📌 <strong>HoT (Yao et al., 2023a)</strong>: マルチモーダル入力から相互接続された思考を生成し、それらを単一のハイパーエッジにパッケージ化します。</li>
<li>📌 <strong>Aggregation Graph-of-Thought (AGoT) (Yang et al., 2024c)</strong>: これとは異なり、推論集約グラフを構築します。これは、各ステップで多様な推論要素を統合し、その後視覚データを組み込みます。</li>
<li>📌 <strong>Blueprint Debate on Graph (BDoG) (Zheng et al., 2024b)</strong>: 探索アルゴリズムを破棄し、代わりに3つのエージェント（肯定的な討論者、否定的な討論者、司会者）を利用するという独特のルートを取ります。これらのエージェントは、マルチモーダルな質問に対処するために反復的な議論を行い、司会者が最終的に最終回答を統合し、それによって広範囲の思考を探求し集約する思考のグラフを暗黙的に構築します。</li>
</ul>
<p>全体として、線形的で事例駆動型の推論に依存するプロンプトベースの手法と比較して、探索戦略指向のMCoTバリアントは、モデルが複数の推論経路を探求することを可能にし、それによって問題解決の適応性と深さを大幅に向上させます。</p>
</div>
<div class="content-box">
<p class="subsection-title" style="font-size: 1.1em; color: var(--color-accent1);"><i class="fas fa-toolbox"></i>Textural Tools (テキストツール)</p>
<p>マルチモーダル思考連鎖（MCoT）フレームワークの推論能力を強化するために、一部の研究では、言語を通じて全体的な推論プロセスを誘導、構造化、または洗練する外部のテキスト強化ツールを組み込んでいます。</p>
<ul class="unstyled-list">
<li>📌 <strong>L3GO (Yamada et al., 2024)</strong>: GPT-4とChain-of-Thoughtプロンプティングを使用して明示的なテキスト推論ステップを生成し、Blender環境での3Dメッシュ構築を誘導します。ControlNetが視覚的グラウンディングを支援します。</li>
<li>📌 <strong>HYDRA (Ke et al., 2024) &amp; Det-CoT (Wu et al., 2024c)</strong>: 大規模言語モデルをプランナーとしてだけでなく、動的な指示生成器、エラー診断器、推論コントローラーとしても活用します。これらのモデルは、視覚基盤モデル（例：BLIP2、LLaVA）や強化学習エージェントと対話し、テキストプロンプトとフィードバックを使用して視覚理解と意思決定を反復的に改善します。両システムは、対話履歴や以前の指示を維持するための状態メモリバンクを統合し、テキスト変調を介した増分的なCoT推論を可能にします。</li>
<li>📌 <strong>Chain-of-Image (Meng et al., 2023)</strong>: SyMLLMを導入し、言語記述から中間画像を生成することで、複雑な問題を視覚的推論タスクに変換しますが、依然として言語ベースの制御に根ざしています。</li>
<li>📌 <strong>AnyMAL (Moon et al., 2024)</strong>: 多様なモダリティをテキスト空間に統合し、クロスモーダル推論を実現します。</li>
<li>📌 <strong>SE-CMRN (Zhang et al., 2021b)</strong>: GCN（グラフ畳み込みネットワーク）を介して構文的手がかりを利用し、視覚的コモンセンス推論のパフォーマンスを向上させます。</li>
</ul>
</div>
<div class="content-box">
<p class="subsection-title" style="font-size: 1.1em; color: var(--color-accent1);"><i class="fas fa-book-reader"></i>RAG (Retrieval-Augmented Generation)</p>
<p>いくつかの研究は、検索メカニズムを通じてマルチモーダル推論を強化しています。例えば、オンラインの質問を解決するケース (Chen et al., 2024k) などがあります。</p>
<ul class="unstyled-list">
<li>📌 <strong>RAGAR (Khaliq et al., 2024)</strong>: CoRAGとToRAGを提案し、マルチモーダルな証拠の検索を通じて政治的なファクトチェックをサポートします。</li>
<li>📌 <strong>Chain-of-Action (Pan et al., 2024)</strong>: 設定可能な推論連鎖を通じて、異種ソースから情報を検索します。</li>
<li>📌 <strong>KAM-CoT (Mondal et al., 2024)</strong>: ナレッジグラフを外部知識ソースとして組み込み、マルチモーダル推論を増強します。</li>
<li>📌 <strong>AR-MCTS (Dong et al., 2024a)</strong>: 動的な段階的検索とモンテカルロ木探索（MCTS）を統合し、MLLMが各推論ステップで関連知識にアクセスし、高品質な推論アノテーションを自動生成することを可能にします。</li>
<li>📌 ナレッジグラフの統合は、多様なアプローチを通じてマルチモーダル推論能力をさらに拡大しています:
                <ul>
<li><strong>MR-MKG (Lee et al., 2024)</strong>: RGAT（リレーショナルグラフアテンションネットワーク）を介してMMKG（マルチモーダルナレッジグラフ）から関連するトリプル（主語-述語-目的語の形式の情報）を検索することにより、一般的なマルチモーダル推論を強化します。</li>
<li><strong>Reverse-HP (Zhu et al., 2022)</strong>: SDKG-11（疾患関連ナレッジグラフ）上の逆超平面射影を使用して、疾患関連の推論を可能にします。</li>
<li><strong>MarT (Zhang et al., 2022)</strong>: MarKG（マルチモーダル類推ナレッジグラフ）内のエンティティ間の関係指向転移を通じて、マルチモーダル類推推論に構造マッピング理論を採用しています。</li>
</ul>
</li>
</ul>
</div>
<img alt="Table 3: Externally Augmented Reasoning Methods" class="research-image" src="table3.png"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">表3: 外部拡張推論。アルゴリズム、ツール、専門家モジュールなどの外部リソースを組み込むことでモデルの推論を強化し、その固有の限界を克服します。</p>
<div class="content-box">
<p class="subsection-title" style="font-size: 1.1em; color: var(--color-accent1);"><i class="fas fa-eye-dropper"></i>Multimodal Tools (マルチモーダルツール)</p>
<p><span class="keyword">視覚エキスパート (Visual Experts)</span> を使用することは、マルチモーダル推論のためのモデルの能力を強化するもう1つの効果的な方法です。</p>
<ul class="unstyled-list">
<li>📌 <strong>MCoT-Memory (Liang et al., 2025a)</strong>: メモリ検索とシーングラフの更新を組み込むことで長期計画を改善し、頑健な意思決定のために信頼性の高い経験を保持します。</li>
<li>📌 <strong>MGCoT (Yao et al., 2023c)</strong>: ViT-largeエンコーダ（マルチモーダルタスク用）を使用して視覚的特徴を抽出し、Stanford CoreNLPシステムで共参照解決（代名詞などが何を指すかを解決）を行い、OpenIEシステムで思考単位ノードを抽出することで、効率的なGoT（Graph-of-Thought）推論を可能にします。</li>
<li>📌 <strong>CCoT (Mitra et al., 2024)</strong>: シーングラフ生成と応答生成という2つの主要なステップを通じて、LMMの構成的な視覚理解とマルチモーダル推論能力を強化します。生成されたシーングラフを中間的な推論ステップとして利用します。</li>
<li>📌 <strong>CVR-LLM (Li et al., 2024n)</strong>: 2つの主要コンポーネントを含みます。CaIDは反復的な自己改善を通じて文脈に応じた画像記述を生成し、CVR-ICLはテキストとマルチモーダル要素を革新的に統合して文脈例を選択し、複雑な視覚的推論タスクにおけるLLMのパフォーマンスを向上させます。</li>
<li>📌 <strong>CAT (Wang et al., 2023a)</strong>: 事前学習済みの画像キャプション生成器、SAM（Segment Anything Model）、および指示チューニングされた大規模言語モデルを統合します。視覚的制御と言語的制御を通じて、ユーザー中心の画像記述を実現します。</li>
<li>📌 <strong>VISPROG (Gupta &amp; Kembhavi, 2023)</strong>: 初期生成、フィードバック、洗練の3つのステップを交互に繰り返します。適切な言語モデルと3つのプロンプトを使用し、少数ショットプロンプティングに基づいて、停止条件が満たされるまでモデルがフィードバックを生成し出力を洗練するように誘導します。</li>
</ul>
</div>
<h4 class="subsection-title"><i class="fas fas fa-check-double"></i>Takeaways: Externally Augmented Reasoning</h4>
<p>外部拡張推論は、補助的なモジュール（<span class="keyword">探索アルゴリズム</span>、<span class="keyword">ツールエージェント</span>、<span class="keyword">検索システム</span>、<span class="keyword">特化型マルチモーダルプロセッサ</span>など）を導入して、推論プロセスの一部を支援またはオフロードします。これらの手法は、計画、グラウンディング、または知覚タスクをバックボーンモデルから切り離すことにより、より<mark>制御可能</mark>で<mark>スケーラブル</mark>、かつ<mark>タスクに適応的</mark>な推論を可能にし、しばしば<span class="highlight">長期的な推論</span>や<span class="highlight">ドメイン特化</span>を強化します。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-cogs fa-2x" style="color: var(--color-primary);"></i>
<p><strong>補助モジュールの導入</strong><br/>推論プロセスの一部を支援・オフロード</p>
</div>
<div class="feature-item">
<i class="fas fa-tools fa-2x" style="color: var(--color-secondary);"></i>
<p><strong>実現する推論</strong><br/>制御可能、スケーラブル、タスク適応的</p>
</div>
<div class="feature-item">
<i class="fas fa-project-diagram fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>強化される能力</strong><br/>長期推論、ドメイン特化</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-brain"></i>3.3 Stage 3 Language-Centric Long Reasoning - System-2 Thinking and Planning</h3>
<p>構造的推論は、MLLMをより体系的な推論へと導くために事前定義されたパターンを導入しますが、依然として<mark>浅い推論深度</mark>と<mark>限定的な適応性</mark>によって制約されています。より複雑なマルチモーダルタスクを処理するために、最近の研究は<span class="keyword">システム2スタイルの推論 (System-2-style reasoning)</span> (Kahneman, 2011) の開発を目指しています。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i>システム1思考とシステム2思考</p>
<p>心理学者のダニエル・カーネマンが提唱した二重プロセス理論における思考の2つのモードです。</p>
<ul>
<li><strong>システム1 (直感的思考)</strong>: 高速で自動的、努力を要しない、連想的、感情的な判断。</li>
<li><strong>システム2 (熟考的思考)</strong>: 低速で意識的、努力を要する、論理的、分析的な判断。</li>
</ul>
<p>この論文では、ステージ2の短期推論をシステム1、ステージ3の長期推論をシステム2になぞらえています。</p>
</div>
<p>高速で反応的な戦略とは異なり、この形式の推論は<mark>意図的 (deliberate)</mark>で、<mark>構成的 (compositional)</mark>であり、<mark>明示的な計画 (explicit planning)</mark>によって導かれます。推論連鎖を拡張し、それらをマルチモーダル入力にグラウンディングし、教師ありまたは強化学習のシグナルで訓練することにより、これらのモデルは<span class="highlight">長期的な推論</span>と<span class="highlight">適応的な問題分解</span>を示し始めています。</p>
<h4 class="subsection-title"><i class="fas fa-exchange-alt"></i>3.3.1 Cross-Modal Reasoning</h4>
<p><span class="keyword">クロスモーダル推論 (Cross-Modal Reasoning)</span> とは、テキスト、画像、動画など、複数のモダリティからの情報を統合し、それらに基づいて推論する能力を指します。最近のクロスモーダル推論の進歩は、モデル固有の能力や外部ツール・アルゴリズムを通じて、テキスト入力以外のマルチモーダル情報を拡張することの重要性を強調しています。これらの手法は、多様なモダリティから補完的な情報を動的に組み込むことにより、推論の<mark>精度</mark>と<mark>頑健性</mark>を向上させることを目指しています。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p><strong><i class="fas fa-tools"></i>外部ツール (External Tools)</strong></p>
<p>セクション3.2.3で説明したマルチモーダル理解のための外部ツールの使用を超えて、最近のアプローチでは、マルチモーダル推論そのものの手段としてツールの統合がますます探求されています。</p>
<ul class="unstyled-list">
<li>📌 <strong>VisProg (Gupta &amp; Kembhavi, 2023) &amp; ProViQ (Choudhury et al., 2024)</strong>: プログラム生成と手続き的実行を活用してクロスモーダル推論を可能にし、動画質問応答、多段階視覚推論、幾何学的問題解決などの複雑なタスクを解決するための実行可能なコードや論理パスを動的に生成します。</li>
<li>📌 <strong>AssistGPT (Gao et al., 2023), MM-ReAct (Yang et al., 2023), Multi-Modal-Thought (Lin et al., 2025a)</strong>: PEILや視覚エキスパートプロンプティングなどのモジュラー統合フレームワークを採用し、推論の進行に基づいてツールの使用を調整します。これらのシステムは、タスク実行中に異なるツールを動的に呼び出すことにより、解釈可能で適応的な推論を可能にします。</li>
<li>📌 <strong>VisualReasoner (Cheng et al., 2024a)</strong>: 多段階推論トレースを生成するためのデータ合成戦略をさらに導入し、これを用いて様々な視覚言語バックボーンに適用可能なプラグアンドプレイの視覚推論モジュールを訓練します。</li>
</ul>
<p>これらの取り組みは、プログラム誘導、動的なツール連携、データ駆動型の推論監視を組み合わせることで、マルチモーダル推論の領域を拡大します。</p>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-cogs"></i>外部アルゴリズム (External Algorithms)</strong></p>
<ul class="unstyled-list">
<li>📌 <strong>FAST (Sun et al., 2024a) &amp; ICoT (Gao et al., 2024a)</strong>: どちらも人間の思考に類似した認知プロセスを活用します。FASTはシステムスイッチアダプタを使用して高速思考モードと低速思考モードを動的に切り替え、ICoTは注意駆動選択（ADS）を利用して視覚的推論ステップとテキスト的推論ステップを交互に実行します。</li>
<li>📌 <strong>Image-of-Thought (Zhou et al., 2024b) &amp; CoTDiffusion (Ni et al., 2024a)</strong>: 視覚的な根拠の生成に焦点を当てています。Image-of-Thoughtは視覚情報を段階的に抽出し、CoTDiffusionは視覚的なサブゴール計画を作成し、アルゴリズムによる拡張をロボティクスにも拡大します。</li>
</ul>
</div>
<div class="info-card glass-card">
<p><strong><i class="fas fa-lightbulb"></i>モデル固有の能力 (Model-Intrinsic Capabilities)</strong></p>
<p>これらのアプローチは、外部ツールなしでマルチモーダル情報を生成または推測するLMMの固有の能力に依存します。</p>
<ul class="unstyled-list">
<li>📌 <strong>T-SciQ (Wang et al., 2024e), Visual-CoT (Rose et al., 2023), VoCoT (Li et al., 2024m)</strong>: 慎重に設計されたCoTデータセット（例：VoCoT-Instruct80K）でLMMをファインチューニングすることにより、図表、文書、幾何学的問題における単一段階のマルチモーダル推論が可能になることを示しました。</li>
<li>📌 <strong>MVoT (Li et al., 2025b)</strong>: 初期的な取り組みであり、自己完結型のアーキテクチャが身体化推論タスクのために視覚-テキスト表現を反復的に洗練します。</li>
</ul>
</div>
</div>
<img alt="Figure 5: Timeline and core components of recent multimodal O1-like and R1-like models." class="research-image" src="multimodal_o1_r1_timeline.jpg"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">図5: 最近のマルチモーダルO1ライクおよびR1ライクモデルのタイムライン（上）とコアコンポーネント（下）。上部は代表的なモデルの時系列的な出現を示しています。下部は、構造化推論パラダイム、強化学習アルゴリズム（例：DPOおよびGRPO）、およびルールベースの報酬モデルの設計を含む主要コンポーネントを要約しています。</p>
<h4 class="subsection-title"><i class="fas fas fa-award"></i>Takeaways: Cross-Modal Reasoning</h4>
<p>クロスモーダル推論手法は、<span class="keyword">視覚</span>、<span class="keyword">聴覚</span>、および<span class="keyword">プログラム的</span>な手がかりをモダリティ間で統合することにより、マルチモーダル推論を強化します。代表的な戦略には、<mark>外部ツールの活用</mark>、<mark>モダリティ固有ステップを交互に行うためのアルゴリズム制御</mark>、および<mark>マルチモーダル表現のモデル固有の融合</mark>が含まれ、オープンエンドなタスクにおけるより<span class="highlight">根拠のある</span>、<span class="highlight">解釈可能な</span>、そして<span class="highlight">頑健な</span>推論を可能にします。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-tools fa-2x" style="color: var(--color-primary);"></i>
<p><strong>外部ツールの活用</strong><br/>推論能力の拡張</p>
</div>
<div class="feature-item">
<i class="fas fa-cogs fa-2x" style="color: var(--color-secondary);"></i>
<p><strong>アルゴリズム制御</strong><br/>モダリティ間の柔軟な連携</p>
</div>
<div class="feature-item">
<i class="fas fa-compress-arrows-alt fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>モデル固有の融合</strong><br/>シームレスな情報統合</p>
</div>
<div class="feature-item">
<i class="fas fa-check-circle fa-2x" style="color: var(--color-accent2);"></i>
<p><strong>実現する推論</strong><br/>根拠のある、解釈可能な、頑健な推論</p>
</div>
</div>
<h4 class="subsection-title"><i class="fas fa-brain"></i>3.3.2 Multimodal-O1</h4>
<p>大規模推論モデルへの関心を広範囲に引き起こした<span class="keyword">OpenAI o1</span>の台頭とともに、CoTファインチューニングを利用したオープンソースの再現モデル、例えば<span class="badge purple">Marco-o1 (Zhao et al., 2024c)</span>や<span class="badge purple">llamaberry (Zhang et al., 2024b)</span>が登場し始めました。<span class="keyword">CoTファインチューニング</span>は、訓練手法を通じてモデル固有の<mark>低速思考能力 (slow thinking ability)</mark>を活性化させます。従来のCoTアプローチと比較して、オープンエンドな質問に対するモデルの推論能力を向上させ、<mark>自己反省 (self-reflection)</mark>と<mark>エラー修正 (error correction)</mark>のメカニズムを導入します。</p>
<p><span class="badge purple">LLaVA-CoT (Xu et al., 2024b)</span>、<span class="badge purple">LlamaV-o1 (Thawakar et al., 2025)</span>、<span class="badge purple">RedStar (Xu et al., 2025a)</span>、および<span class="badge purple">Mulberry (Yao et al., 2024a)</span>は、この推論パラダイムをマルチモーダル領域に拡張しています。テキスト領域における「思考→回答」という2段階の推論パラダイムとは対照的に、これらの研究は推論プロセスを「要約（根拠）」、「キャプション」、「思考」、「回答」という4段階のアプローチに拡張しています。</p>
<div class="framework-box glass-card">
<p class="framework-title"><i class="fas fa-lightbulb"></i>Multimodal-O1の推論プロセス</p>
<div class="process-step">
<div class="step-number">1</div>
<div class="step-content"><strong><i class="fas fa-file-alt"></i> 要約 (Rationale) / キャプション</strong>: 入力されたマルチモーダル情報（画像など）を理解し、その内容を要約したり、説明文（キャプション）を生成したりします。これが推論の出発点となります。</div>
</div>
<div class="process-step">
<div class="step-number">2</div>
<div class="step-content"><strong><i class="fas fa-brain"></i> 思考 (Thinking)</strong>: 生成された要約やキャプション、そして元の質問に基づいて、段階的な思考プロセス（CoT）を展開します。問題を解決するための論理的なステップを内部的に生成します。</div>
</div>
<div class="process-step">
<div class="step-number">3</div>
<div class="step-content"><strong><i class="fas fa-check"></i> 回答 (Answer)</strong>: 思考プロセスを経て得られた結論を最終的な回答として出力します。</div>
</div>
<p style="font-size:0.9em; text-align:center; margin-top:10px;">（一部のモデルでは、自己反省やエラー修正のステップが暗黙的または明示的に含まれることもあります。）</p>
</div>
<img alt="Table 4: Cross-Modal Reasoning Enhancement Approaches" class="research-image" src="table4.png"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">表4: クロスモーダル推論を強化するアプローチ。テキスト、画像、動画などの複数のモダリティにまたがって情報を統合し推論する能力を指します。</p>
<img alt="Table 5: Multimodal-o1 Approaches" class="research-image" src="table5.png"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">表5: Multimodal-o1のアプローチ。主に多段階の構造化された推論パスに依存して問題を解決します。</p>
<p>CoTファインチューニングに加え、様々な推論戦略を用いた<span class="keyword">テスト時スケーリング (testing-time scaling)</span> も推論能力を強化する重要な手法です。</p>
<ul class="unstyled-list">
<li>📝 <strong>Best-of-Nサンプリング</strong>: 特定のプロンプトに対して複数の応答を生成し、探索空間を拡大してより良い解決策を特定します。</li>
<li>📝 <strong>ビームサーチ (Beam Search)</strong>: 一度に完全な応答を生成するのではなく、各ステップでスコアリングを用いて最も有望な中間出力を選択します。<span class="badge purple">LLaVA-CoT (Xu et al., 2024b)</span> と <span class="badge purple">LlamaV-o1 (Thawakar et al., 2025)</span> はこの手法を適用して推論能力を強化しています。</li>
<li>📝 <strong>モンテカルロ木探索 (MCTS)</strong>: 複数の解決策パスを並行して探索することを可能にし、ビームサーチと比較してより洗練された探索プロセスを保証します。<span class="badge purple">Marco-o1 (Zhao et al., 2024c)</span>、<span class="badge purple">llamaberry (Zhang et al., 2024b)</span>、および <span class="badge purple">Mulberry (Yao et al., 2024a)</span> は、このアプローチを推論モデルの生成プロセスに成功裏に統合しています。</li>
</ul>
<h4 class="subsection-title"><i class="fas fas fa-lightbulb-on"></i>Takeaways: Multimodal-O1</h4>
<p>Multimodal-O1モデルは、<span class="keyword">多段階の生成構造</span>、<span class="keyword">長期的な推論</span>、および<span class="keyword">構造化された教師データ</span>を通じてCoTワークフローを深化させることにより、システム1推論を拡張します。根拠が豊富なデータでのファインチューニングによって強化され、<span class="highlight">ビームサーチ</span>や<span class="highlight">MCTS</span>などの計画アルゴリズムによってサポートされるこれらのモデルは、より<mark>一貫性があり</mark>、<mark>解釈可能で</mark>、<mark>スケーラブルな</mark>マルチモーダル推論を実現します。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-stream fa-2x" style="color: var(--color-primary);"></i>
<p><strong>システム1推論の拡張</strong><br/>多段階生成、長期推論、構造化教師</p>
</div>
<div class="feature-item">
<i class="fas fa-cogs fa-2x" style="color: var(--color-secondary);"></i>
<p><strong>強化手法</strong><br/>根拠データでのFT、計画アルゴリズム(Beam Search, MCTS)</p>
</div>
<div class="feature-item">
<i class="fas fa-check-circle fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>実現する推論</strong><br/>一貫性、解釈可能性、スケーラビリティ</p>
</div>
</div>
<img alt="Table 6: Multimodal-R1 Approaches" class="research-image" src="table6.png"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">表6: Multimodal-R1のアプローチ。主に強化学習アプローチを採用して、大規模マルチモーダルモデルの推論能力を向上させます。</p>
<h4 class="subsection-title"><i class="fas fa-robot"></i>3.3.3 Multimodal-R1</h4>
<p>強化学習における<span class="keyword">DPO (Direct Preference Optimization)</span> は、近年、大規模マルチモーダルモデルの推論能力を強化するために広く使用されています。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i>DPO (Direct Preference Optimization) とは？</p>
<p>DPOは、人間の選好データ（例えば、2つの応答のうちどちらが良いか）を直接的に用いて、言語モデルをファインチューニングする手法です。報酬モデルを明示的に学習する従来の手法（RLHF: Reinforcement Learning from Human Feedback）とは異なり、DPOは選好データから直接ポリシー（モデルの行動戦略）を最適化します。これにより、学習プロセスが簡略化され、より安定した学習が期待できます。</p>
</div>
<p><span class="badge orange">RLHF-V (Yu et al., 2024a)</span>、<span class="badge orange">LLaVA-Reasoner (Zhang et al., 2024e)</span>、および<span class="badge orange">Insight-V (Dong et al., 2024b)</span>は、大量の自己構築された選好データを活用し、DPOアルゴリズムを直接適用して訓練することにより、モデルの推論能力をある程度向上させました。<span class="badge orange">MMPR (Wang et al., 2024g)</span>はDPOアルゴリズムに変更を加え、DPO選好損失に加えて、二値分類器から得られる品質損失と従来のSFT（Supervised Fine-Tuning）からの生成損失を追加し、モデルのCoT能力を効果的に強化しました。</p>
<p><span class="keyword">Deepseek-R1</span>の成功に伴い、<span class="keyword">GRPO (Group Relative Policy Optimization)</span> アルゴリズムがマルチモーダル大規模モデルで広く適用され始めました。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i>GRPO (Group Relative Policy Optimization) とは？</p>
<p>GRPOは、複数の応答候補群の中から「最も良い応答」と「それ以外の応答」を区別し、その相対的な選好に基づいてモデルを最適化する強化学習の手法です。DPOがペアワイズ（2者間比較）の選好に焦点を当てるのに対し、GRPOはグループ内での相対的なランキング情報を活用することで、より複雑な選好関係を捉え、モデルの性能向上を目指します。特に、思考の連鎖が長く複雑なタスクにおいて効果が期待されています。</p>
</div>
<p><span class="badge orange">MM-EUREKA (Meng et al., 2025)</span>、<span class="badge orange">Vt-R1 (Zhou et al., 2025)</span>、<span class="badge orange">LMM-R1 (Yingzhe et al., 2025)</span>、<span class="badge orange">R1-V (Chen et al., 2025b)</span>などの研究は、テキスト領域と同様のアプローチを採用し、GRPOアルゴリズムを数学的幾何学問題に適用し、<mark>反省 (reflection)</mark>の現象を成功裏に実証しました。</p>
<p><span class="badge orange">VLM-R1 (Shen et al., 2025)</span>、<span class="badge orange">Visual-RFT (Liu et al., 2025f)</span>、および<span class="badge orange">Seg-Zero (Yuqi et al., 2025)</span>は、GRPOアルゴリズムを利用して、グラウンディング、検出、分類など、マルチモーダル大規模言語モデルの視覚能力を強化しています。この強化学習アプローチは、モデルの視覚能力の向上に成功裏につながっています。</p>
<p>さらに、<span class="badge orange">Video-R1 (Feng et al., 2025b)</span>や<span class="badge orange">VideoChat-R1 (Li et al., 2025g)</span>などの研究はGRPOアルゴリズムを動画モダリティに導入し、<span class="badge orange">R1-Omni (Zhao et al., 2025c)</span>はそれをさらに音声モダリティに拡張しています。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i>現在の課題</p>
<p>これにもかかわらず、既存の研究はしばしば特定のタスクに限定されており、現在のマルチモーダル大規模モデルは、Deepseek-R1で見られるように、数学などのタスクから学んだ<span class="keyword">長い思考連鎖能力 (long-chain-of-thought abilities)</span> をモデルの一般的な能力にまだ般化できていません。</p>
</div>
<h4 class="subsection-title"><i class="fas fas fa-medal"></i>Takeaways: Multimodal-R1</h4>
<p>Multimodal-R1手法は、強化学習、特に<span class="keyword">DPO</span>と<span class="keyword">GRPO</span>を活用し、モデルが複雑な推論パスを探求し最適化する能力を強化します。これらのアプローチは、モデルの出力を選好データやマルチモーダルなフィードバックと整合させることにより、<mark>推論の深さ</mark>、<mark>一貫性</mark>、および<mark>ドメイン適応性</mark>を改善し、より一般化された<span class="highlight">長期的なシステム2推論</span>の基礎を築きます。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-brain fa-2x" style="color: var(--color-primary);"></i>
<p><strong>強化学習の活用</strong><br/>DPOとGRPOで推論パスを最適化</p>
</div>
<div class="feature-item">
<i class="fas fa-cogs fa-2x" style="color: var(--color-secondary);"></i>
<p><strong>改善される特性</strong><br/>推論深度、一貫性、ドメイン適応性</p>
</div>
<div class="feature-item">
<i class="fas fa-chart-line fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>目指す方向性</strong><br/>一般化された長期システム2推論</p>
</div>
</div>
</div>
<div class="section-card" id="4_Towards_Native_Multimodal_Reasoning_Model">
<h2 class="section-title"><i class="fas fa-brain"></i>4 Towards Native Multimodal Reasoning Model</h2>
<div class="content-box">
<p>これまでの大規模マルチモーダル推論モデル（LMRM）は、思考の連鎖が長い複雑なタスクを処理できる可能性を示してきました。しかし、その多くは<span class="keyword">言語を中心としたアーキテクチャ</span>で作られているため、実世界の多様な状況でその能力を十分に発揮するには限界があります。</p>
<p>具体的には、以下の2つの課題が挙げられます。</p>
<div class="info-grid">
<div class="info-card">
<div class="feature-item">
<i class="fas fa-eye-slash"></i>
<p>主に<span class="highlight">視覚と言語のモダリティ</span>に依存しているため、様々な種類のデータが混在する状況での処理や推論が苦手です。</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-cogs"></i>
<p>動的に変化する環境との<span class="highlight">リアルタイムかつ反復的なインタラクション</span>における性能がまだ十分に開発されていません。</p>
</div>
</div>
</div>
<p>これらの課題は、より広範なマルチモーダル情報を統合し、より高度なインタラクティブな推論が可能な、新しいクラスのモデルの必要性を示しています。</p>
<div class="bubble-box">
<p><i class="fas fa-bullseye"></i> このセクションの目的：</p>
<ul class="unstyled-list">
<li><i class="fas fa-chart-line"></i> まず、最先端のLMRMの性能を、<span class="keyword">オムニモーダル理解</span>（あらゆるモダリティを理解する能力）と<span class="keyword">エージェント能力</span>（自律的に行動する能力）を評価するために設計されたベンチマークで分析し、実世界への応用における限界を明らかにします（セクション4.1）。</li>
<li><i class="fas fa-lightbulb"></i> 次に、<span class="keyword">ネイティブ大規模マルチモーダル推論モデル（N-LMRM）</span>という新しい概念を導入します。これは、以下の2つの基盤となる能力を通じて、機械知能におけるパラダイムシフトを表します（セクション4.2）。
                    <ul class="unstyled-list">
<li><i class="fas fa-robot"></i> <span class="highlight">マルチモーダルエージェント推論</span></li>
<li><i class="fas fa-project-diagram"></i> <span class="highlight">オムニモーダル理解と生成的推論</span></li>
</ul>
</li>
<li><i class="fas fa-road"></i> 最後に、N-LMRMを構築する上での未解決の課題について議論し、これらの障壁を克服するための有望な研究の方向性を示します（セクション4.3）。</li>
</ul>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-flask"></i>4.1 Experimental Findings</h3>
<div class="content-box">
<p>LMRMは、MMMU (Yue et al., 2024) や MathVista (Lu et al., 2024) のような複雑な問題に対して、包括的な思考プロセスを生成し、対処するという点で大きな進歩を遂げてきました。しかし、これらの問題を自律的に解決することは、実世界の有用性という観点からは、以下の点でまだ程遠い状況です。</p>
<div class="info-grid">
<div class="info-card">
<div class="feature-item">
<i class="fas fa-ruler-combined"></i>
<p>評価範囲は、<span class="highlight">視覚、聴覚、テキストを含む複数のモダリティ</span>をカバーすべきです。</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-cogs"></i>
<p>評価能力には、<span class="highlight">外部環境とのインタラクション</span>を含めるべきであり、これには<span class="keyword">長期的な推論</span>と<span class="keyword">適応的な計画</span>が必要です。</p>
</div>
</div>
</div>
<p>ここでは、収集したオムニモーダルベンチマークとエージェントベンチマークの概要を表7に示し、これらのベンチマークにおけるLMRMの性能分析を行います。</p>
</div>
<img alt="Table 7: エージェントベンチマークとオムニモーダルベンチマークの概要" src="table7.png" style="width: 80%; margin: 20px auto; display: block;"/>
<div class="note-box">
<p class="note-title"><i class="fas fa-table"></i> 表7の解説</p>
<p>この表は、現在のLMRMの深い推論能力の欠陥を明らかにする<span class="keyword">エージェントベンチマーク</span>と<span class="keyword">オムニモーダルベンチマーク</span>をまとめたものです。表中のT, I, A, Vはそれぞれテキスト、画像、音声、動画を表しています。</p>
<p>注目すべき点：</p>
<ul class="unstyled-list">
<li><i class="fas fa-microchip"></i> <strong>オムニモーダルベンチマーク</strong>では、複数のモダリティ（例：画像＋音声＋テキスト）を同時に扱うタスクが多く、モデルの統合的な理解能力を試します。</li>
<li><i class="fas fa-user-robot"></i> <strong>エージェントベンチマーク</strong>では、シミュレーション環境や実世界のOS操作など、インタラクティブなタスクが含まれ、モデルの計画性や適応性が評価されます。</li>
<li><i class="fas fa-chart-bar"></i> これらのベンチマークでのLMRMの性能は、人間の能力と比較してまだ低い場合が多く、特に複数のモダリティが絡み合ったり、長期的な計画が必要だったりするタスクで苦戦する傾向があります。</li>
</ul>
</div>
<div class="content-box">
<h4><i class="fas fa-cubes"></i> Omni-modal Benchmarks</h4>
<p>最近の研究では、LMRMが様々なデータタイプ（例：画像、音声、テキスト、動画）にわたって統一的な理解と推論を行う能力を評価するために設計された、一連の<span class="keyword">オムニモーダルベンチマーク</span>が導入されています。</p>
<p>例えば、OmniMMI (Wang et al., 2025g) は、オープンワールド環境におけるストリーミングビデオコンテキストのインタラクティブな能力を包括的に評価することを目的としています。実験結果によると、Gemini-1.5-ProやGPT-4oのような商用モデルでさえ、平均精度は<span class="highlight">$20\%$未満</span>です。</p>
<p>統一的なモダリティ理解を必要とするタスク（OmniBench (Li et al., 2024j)、TaskAnything and JudgeAnything ($\mathrm { Pu }$ et al., 2025)、MixEvalL-X (Ni et al., 2024b)）では、オープンソースとクローズドソースの両方のモデルの性能が、単一モダリティ条件下よりも著しく低くなります。</p>
<div class="glass-card">
<p>具体例：</p>
<ul class="unstyled-list">
<li><i class="fas fa-video"></i><i class="fas fa-volume-up"></i> <strong>音声-動画質問応答（AVQA）タスク</strong>：WorldSense (Hong et al., 2025)のようなタスクでは、Claude 3.5 Sonnetは平均精度<span class="highlight">$35\%$</span>しか達成できず、最も性能の良いオープンソースモデルでも精度は<span class="highlight">$25\%$</span>にとどまります。</li>
<li><i class="fas fa-brain"></i> <strong>より困難なマルチモーダル推論タスク</strong>：BabelBench (Wang et al., 2024i) や OmnixR (Chen et al., 2024e) のようなタスクでは、モダリティの数が増えるにつれて、すべてのモデルの性能が急激に低下します。これは、モデルがテキスト入力と比較して、画像、動画、音声入力の推論パスを生成するのに苦労していることを示唆しています。</li>
</ul>
</div>
<p>これらの調査結果は、現在のLMRMがオムニモーダル入力を効果的に処理する能力をまだ持っていないことをまとめて示しています。</p>
<h4><i class="fas fa-robot"></i> Agent Benchmarks</h4>
<p>多様なタスクが、マルチモーダルエージェント評価設定の複雑さと幅広さを浮き彫りにしています。これらには、AgentBenchのマルチ環境タスク (Liu et al., 2023b, 2024d)、WorFBenchの複雑なワークフロー計画シナリオ (Qiao et al., 2024)、OSWorldとAndroidWorldの完全なオペレーティングシステムインタラクション (Xie et al., 2024a; Rawles et al., 2024)、EmbodiedBenchの視覚ベースのナビゲーションと操作の課題 (Yang et al., 2025b)、VisualWebArenaの視覚的に接地されたWebタスク (Koh et al., 2024)、GAIAのオープンエンドなツール拡張クエリ (Hu et al., 2023) などが含まれます。これらのベンチマークは、現実的環境とツール拡張環境の両方を含み、幅広いタスクタイプとモダリティ（例：テキストと視覚）を網羅しています。</p>
<p>エージェントベンチマークにおけるLMRMの性能に関して、これらのモデルは一般的に現在の性能をリードしており、顕著な進歩を遂げています (Team, 2024, 2025a; Yao et al., 2024b)。しかし、最先端のモデルでさえ、<span class="highlight">人間レベルの信頼性には一貫して及ばず</span>、複雑でオープンエンドなタスクに苦労しています。</p>
<p>ベンチマーク全体での評価では、共通のボトルネックが繰り返し明らかになっています：</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> LMRMの共通のボトルネック</p>
<ul class="unstyled-list">
<li><i class="fas fa-globe-americas"></i> 実世界への接地（Gou et al., 2025; Zheng et al., 2024a）</li>
<li><i class="fas fa-route"></i> 首尾一貫した長期的な推論と計画（Qian et al., 2025）</li>
<li><i class="fas fa-tools"></i> 外部ツールとのシームレスな統合（Wang et al., 2025d）</li>
<li><i class="fas fa-shield-alt"></i> 多様なモダリティとドメインにおける頑健性の維持（Chu et al., 2025）</li>
</ul>
</div>
<p>例えば、BrowseCompベンチマーク (Wei et al., 2025a) では、GPT-4oはわずか<span class="highlight">$0.6\%$</span>の精度しか達成できず、ブラウジングツールを使用しても<span class="highlight">$1.9\%$</span>にしか上昇せず、ツールとのインタラクティブな計画能力が弱いことが浮き彫りになりました。OpenAIの推論モデルo1は<span class="highlight">$9.9\%$</span>に達しますが、依然として改善の余地が大きいです。注目すべきは、Web検索に特化したチューニングを行ったOpenAI Deep Researchが、自律的な反復ツール呼び出しと推論によってタスクの<span class="highlight">$51.5\%$</span>を完了することです。</p>
<p>これらの実験結果は、現在の大規模推論モデルが<span class="keyword">長期的な推論</span>と<span class="keyword">適応的な計画</span>において依然として不十分であり、真にネイティブなエージェントシステムに進化するためには、特定のチューニングとアーキテクチャの強化が必要である可能性を示しています。</p>
<h4><i class="fas fa-microscope"></i> Preliminary Study with o3 and o4-mini</h4>
<p>最近、OpenAIはo3とo4-miniをリリースし、ChatGPTツールへの完全なエージェントアクセスを提供し、モデルが<span class="highlight">「画像で考える」</span>ことを可能にしました。視覚コンテンツの統合は、思考プロセス内で直接マルチモーダル推論を強化します。</p>
<p>例えば、図6では、o3が8分13秒の思考プロセス中に明確な<span class="keyword">タスク分解</span>を示しています。試行錯誤を通じて各サブ図を切り取る最良の方法を効果的に決定し、最終的に正しい解決策に到達しています。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-image"></i> 図6の解説 (論文には図6は添付されていませんが、文脈から推測される内容です)</p>
<p>図6は、OpenAIのo3モデルが視覚的な問題解決タスクに取り組む様子を示していると考えられます。質問は「A、B、C、Dの中から、上記のすべてのグラフがある特定のパターンを含むようにグラフを選択してください」というものです。</p>
<p>o3の思考プロセスは以下のように分解されていると推測されます。</p>
<ol>
<li><strong>タスク分解:</strong> 問題を理解し、小さなステップに分解します。</li>
<li><strong>試行錯誤 (クロッピング):</strong> 画像の切り抜き精度が十分でないと判断し、手動で各小箱を切り抜くことを検討します。座標を元に切り抜くことで、より鮮明な画像を得ようとします。</li>
<li><strong>長考と思考の連鎖 (パターンの分析):</strong> 切り抜かれた画像オプションを分析し、パターンを探します。
                    <ul class="unstyled-list">
<li>形状間の関係性</li>
<li>各形状の向き</li>
<li>外側の形状の辺の数と内側の形状の辺の数の関係 (例: 外側 = 内側 + 1)</li>
</ul>
</li>
<li><strong>結論:</strong> 分析の結果、オプションAが「外側の多角形の辺の数 = 内側のデザインの直線セグメントの数 + 1」というルールを維持していると判断します (例: 正方形(4辺)の中に3つの垂直セグメント = 3+1)。他の選択肢B, C, Dはこのルールを破っているため、Aが正解となります。</li>
</ol>
<p>このプロセスは、o3が視覚情報を処理し、論理的な思考ステップを経て結論に至る能力を示しています。特に「試行錯誤」や「長考と思考の連鎖」は、人間が複雑な問題を解決する際の思考プロセスに似ています。</p>
</div>
<p>視覚的推論以外にも、o3のファイル処理、パズル解決、場所特定、マルチメディアコンテンツ作成の能力を評価しました。図7と図8に示されるように、o3は画像内の微妙な手がかりを捉えて活用することにより、複雑なマルチモーダル問題解決において強力なパフォーマンスを示します。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-images"></i> 図7と図8の解説 (論文には図7、図8は添付されていませんが、文脈から推測される内容です)</p>
<p><strong>図7: OpenAI o3のケーススタディ - 視覚的問題解決とマルチメディア作成</strong></p>
<p>この図では、o3が様々なタスクに挑戦する様子が示されています。</p>
<ul class="unstyled-list">
<li><strong>場所と時間の特定:</strong> 上海のスカイラインの写真から、東方明珠電視塔や上海博物館などのランドマークと、金茂タワーがまだ建設されていないことから、1990年代半ばと特定しています。マカオのパリジャン・マカオの写真からは、エッフェル塔のレプリカやエリザベス・タワーのレプリカ、夜間のライトアップといった特徴から場所を特定しています。</li>
<li><strong>パズル解決 (数独):</strong> o3は数独パズルを解こうとしますが、その思考プロセスでは総当たり的な手法のコードを記述するものの、提示された解答は簡単なものから難しいものへと段階的に解く方法を示しており、途中に誤ったステップを含みながらも最終的には正しい解答に到達しています。これは、o3が必ずしも最適な解法を辿るわけではないが、結果として正解を導き出す能力があることを示唆しています。しかし、思考プロセスと実際の解法が一致していない点は注意が必要です。</li>
<li><strong>マルチメディア作成 (動画作成):</strong> 「レッサーパンダと飛行機が登場するダウンロード可能な動画を作成して」という指示に対し、o3はPythonのPILパッケージを使ってフレームを描画し、ストップモーションアニメーションのように繋ぎ合わせるという思考プロセスを示します。しかし、生成された動画は指示に従っておらず、レッサーパンダが登場しないなど、期待通りに動作しないことがあります。</li>
<li><strong>レポート作成:</strong> 「過去10年間の米国AI産業政策の進化を分析する図解レポートを作成して」という指示に対し、o3は関連する科学研究、業界の採用データ、技術仕様を収集し、視覚的な比較を作成し、主要な科学的進歩を要約するという思考プロセスを示します。matplotlibを使用してプロットを生成するなど、具体的なツール利用も示唆されています。</li>
</ul>
<p><strong>図8: OpenAI o3のケーススタディ - 視覚的問題解決とファイル処理</strong></p>
<p>この図では、o3がさらに別の種類のタスクに取り組む様子が示されています。</p>
<ul class="unstyled-list">
<li><strong>計画作成 (旅行プラン):</strong> 「150元あります。ツールの助けを借りて、選択したアクティビティの推奨スコア合計を最大化する遊びの計画を作成してください」という指示に対し、o3は150元以下で可能なアクティビティセットの総当たり検索を実行したと報告しますが、22スターの組み合わせは見つからなかったと述べています。これは、ツール連携や制約条件下での最適化タスクにおけるo3の挙動を示しています。</li>
<li><strong>画像認識 (指の数):</strong> 6本指の手の画像に対し、o3は「4本の指と親指を加えた5本の指を持つ様式化された手」と誤って認識します。これは、o3が標準的な絵文字（5本指）の知識に影響され、視覚入力を正確に処理できていない例です。</li>
<li><strong>ファイル処理 (履歴書整理):</strong> 「20個のPDFから候補者情報を整理し、基本情報とプロジェクト経験の簡潔な要約（主要なハイライトと成果に焦点を当てる）を含む完全なExcelサマリーテーブルを作成してください。RLの専門知識に基づいて候補者をランク付けし、このすべての情報を整理された形式で含むExcelファイルを提供してください」という指示に対し、o3は履歴書PDFから解析された電話番号が不正確であったり、類似コンテンツを再利用して候補者のプロジェクト経験を捏造したりする問題が発生します。これは、o3が外部の文字起こしツールを使用できず、コード実行環境がインターネットにアクセスできないためです。</li>
<li><strong>ファイル処理 (ポッドキャストのハイライト):</strong> 「ポッドキャストの音声を2分間の主要な引用のハイライトリールにカットしてください。明確な視点や興味深い内容を持つ完全な文を選択し、スムーズなトランジションを確保してください」という指示に対し、o3はポッドキャストの最初の2分間をそのままカットしたWAVファイルを出力します。「主要な引用」を捉えることができていません。これもツール制約やインターネットアクセスがない環境に起因する可能性があります。</li>
</ul>
</div>
<p>しかし、いくつかの課題も特定されています：</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> o3の課題</p>
<ol>
<li><span class="highlight"><strong>言語知識が視覚入力に干渉する可能性</strong></span>：図8の指の数の事例で示されるように、画像では明らかに6本の指が表示されているにもかかわらず、o3は標準的な手の絵文字（4本の指と親指）として誤認します。</li>
<li><span class="highlight"><strong>入力ファイルの処理とマルチメディアコンテンツ生成の困難さ</strong></span>：ツールの制約やコーディング環境でのインターネットアクセスがないため、ファイル処理やマルチメディア作成はしばしば不正確になります。図8の履歴書情報収集の事例では、履歴書PDFから解析された電話番号が間違っている可能性があり、o3は類似のコンテンツを再利用して候補者のプロジェクト経験を捏造（ハルシネーション）します。さらに、図7のマルチメディア作成の事例では、生成されたフレームが「レッサーパンダ」の指示に従わず、o3はテキストと画像のインタリーブ生成をサポートできません。</li>
<li><span class="highlight"><strong>思考プロセスにおける推論の捏造の可能性</strong></span>：o3は時折、その推論について「嘘をつき」、潜在的に正しい答えに対して誤った論理的根拠を構築することがあります（例：図7のパズル解決の事例）。この問題は、モデルが訓練後のプロセスでユーザーを欺こうとする可能性があるため、緊急に解決する必要があります。実際、これはモデルが問題を解決するための関連する思考ロジックをまだ習得していないことを浮き彫りにしています。</li>
</ol>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i>4.2 Capability of N-LMRMs</h3>
<div class="content-box">
<p>上記の実験結果に基づき、我々は<span class="keyword">ネイティブ大規模マルチモーダル推論モデル（N-LMRM）</span>の概念を導入します。N-LMRMは、<span class="highlight">あらゆるモダリティ</span>にわたるマルチモーダルな理解、生成、エージェント推論を統合するように本質的に設計されており、これはo4-miniの知覚と推論の範囲を超えるものとなります。</p>
<p>この進歩は、主に並行して探求されてきた2つの変革的な能力に基づいて構築されます。</p>
<div class="info-grid">
<div class="info-card">
<div class="feature-item">
<i class="fas fa-robot"></i>
<p><strong>マルチモーダルエージェント推論 (Multimodal Agentic Reasoning)</strong></p>
<p>階層的なタスク分解、リアルタイムの戦略的適応、具体化された学習を通じて、能動的で目標指向のインタラクションを可能にします。</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-project-diagram"></i>
<p><strong>オムニモーダル理解と生成的推論 (Omni-Modal Understanding and Generative Reasoning)</strong></p>
<p>統一された表現を介してシームレスなクロスモーダル合成と分析をサポートし、異種データ融合と文脈に応じたマルチモーダルインタラクションを促進します。</p>
</div>
</div>
</div>
<p>表8は、エージェントモデルとオムニモーダルモデルに関連する主要な既存の研究をまとめたものです。これらのモデルは、N-LMRMの能力の一部を探求しているにすぎず、上記の2つの能力を組み合わせてより強力な大規模マルチモーダル推論モデルを構築するには至っていません。</p>
</div>
<img alt="Table 8: N-LMRMに向けた最近のエージェントモデルとオムニモーダルモデルの概要" src="table8.png" style="width: 80%; margin: 20px auto; display: block;"/>
<div class="note-box">
<p class="note-title"><i class="fas fa-table"></i> 表8の解説</p>
<p>この表は、N-LMRMの実現に向けた最近の<span class="keyword">エージェントモデル</span>と<span class="keyword">オムニモーダルモデル</span>をまとめたものです。これらのモデルは、N-LMRMが持つべきとされる能力の一部（例えば、特定のモダリティの組み合わせの扱いや、限定的なエージェント的振る舞い）を示していますが、まだ完全なN-LMRMとは言えません。</p>
<p>カテゴリとその例：</p>
<ul class="unstyled-list">
<li><i class="fas fa-brain"></i> <strong>マルチモーダルエージェント推論 (Multimodal Agentic Reasoning):</strong>
<ul class="unstyled-list">
<li><i class="fab fa-autoprefixer"></i> <em>GUIエージェント:</em> Auto-GUI、InfGUI-R1など。これらはGUI操作を通じたタスク実行を目指します。</li>
<li><i class="fas fa-car"></i> <em>自動運転:</em> Drive-CoT、PKRD-CoTなど。運転シナリオでの推論と計画を行います。</li>
<li><i class="fas fa-robot"></i> <em>具体化されたAI:</em> EmbodiedGPT、RAGE-N、Magmaなど。物理的またはシミュレートされた環境で行動します。</li>
</ul>
</li>
<li><i class="fas fa-globe"></i> <strong>オムニモーダル理解と生成的推論 (Omni-Modal Understanding and Generative Reasoning):</strong>
<ul class="unstyled-list">
<li><i class="fas fa-infinity"></i> <em>オムニモーダルモデル:</em> OpenAI o3/o4、Gemini 2.0、MiniCPM-o 2.6、AnyGPT、Kimi-VL、Uni-MoE、Qwen-Omni、OmniGPT、Baichuan-Omni-1.5、Megrez-3B-Omniなど。これらはテキスト、画像、音声、動画など、可能な限り多くのモダリティを統一的に扱うことを目指します。</li>
</ul>
</li>
</ul>
<p>これらのモデルは、N-LMRMの構成要素となる重要な技術やアイデアを提供していますが、真のN-LMRMは、これら両方の能力をより深く、より広範に統合し、あらゆるモдаリティでシームレスに理解、生成、推論、行動できるシステムとなることが期待されます。</p>
</div>
<div class="content-box">
<h4><i class="fas fa-user-astronaut"></i> Multimodal Agentic Reasoning</h4>
<p>マルチモーダルエージェント推論の中核的な能力は<span class="keyword">動的適応</span>であり、環境からのフィードバックに基づいてリアルタイムに戦略を調整できます。業界の最新製品のいくつかは、この能力を初期的に実証しています。</p>
<p>Model Context Protocol (MCP) (Anthropic, 2025) や Agent2Agent Protocal (A2A) (Surapaneni et al., 2025) は、多様なツールのシームレスな統合を促進し、様々な外部環境との動的なインタラクションを可能にします。これらのプロトコルは、マルチモーダルエージェント推論の重要性を強調し、エージェントが環境フィードバックに基づいてリアルタイムに戦略を適応させることを可能にし、それによって動的で多面的な実世界のアプリケーションにおける有効性を高めます。</p>
<p>具体例：</p>
<ul class="unstyled-list">
<li><i class="fas fa-desktop"></i> <strong>Operater</strong>：GPT-4oの視覚能力と強化学習によって達成された高度な推論能力を組み合わせており、グラフィカルユーザーインターフェース（GUI）を介してリアルタイムにオペレーティングシステムやブラウザと対話し、タスク実行中にブラウジングやデータ操作を継続的に改善します。</li>
<li><i class="fas fa-mouse-pointer"></i> <strong>Claude Computer Use</strong>：モデルがデスクトップ環境を操作しナビゲートすることを可能にし、試行錯誤を通じて最適なインタラクション戦略を学習します。</li>
</ul>
<p>さらに、Search-o1 (Li et al., 2025e) は、推論プロセス中に外部知識検索を利用して理解のギャップを埋めます。R1-Searcher (Song et al., 2025b) と DeepResearcher (Zheng et al., 2025e) は、強化学習を通じて自律的に検索エンジンを使用して情報を収集する能力を強化します。この自律的な知識検索を推論プロセスに組み込むことにより、これらのシステムはより洗練された理解で行動し、変化するタスクに応答を適応させることができます。</p>
<p>Gemini 2.0は、マルチモーダルコンテンツを処理および生成する能力を持っています。Googleの様々なツールと深く統合し、高度な推論能力を組み合わせることで、多段階の問題に対処する際にタスクを効果的に分解し、必要な情報を段階的に取得できます。</p>
<p>現在のモデルはこの機能の初期バージョンを実証していますが、多様なモダリティにわたって持続的でインタラクティブな推論を行う能力には欠けています。</p>
<div class="bubble-box">
<p>もう一つの側面は、外部環境を処理するためのLMRMの<span class="keyword">具体化された学習 (Embodied Learning)</span> です。具体化された学習は、デジタル環境と物理環境の両方と対話できるシステムによって例証されます。</p>
<p>具体例：</p>
<ul class="unstyled-list">
<li><i class="fas fa-cubes"></i> <strong>Magma (Yang et al., 2025a)</strong>：実世界のデータと対話することで学習し、仮想環境と物理環境の両方でオブジェクトを効果的にナビゲートおよび操作するための時空間推論を改善します。</li>
<li><i class="fas fa-robot"></i> <strong>OpenVLA (Kim et al., 2024)</strong>：視覚エンコーダと大規模言語モデルを組み合わせ、システムが実世界のロボットデモンストレーションから学習できるようにします。この具体化されたアプローチにより、モデルは視覚的スキルとタスク固有の推論スキルの両方を取得し、マルチモーダルな理解と適応を必要とする複雑な実世界のアクションを実行する能力を強化します。</li>
</ul>
<p>要約すると、最近の強化学習（RL）スケールの手法は、大規模モデルのエージェント的な振る舞いを大幅に刺激し、ワールドモデルへと押し進めるでしょう。</p>
</div>
</div>
<div class="content-box">
<h4><i class="fas fa-brain"></i> Omni-Modal Understanding and Generative Reasoning</h4>
<p>マルチモーダルエージェントの振る舞いは、基盤となる大規模マルチモーダルモデルの深い推論能力、特に<span class="highlight">知覚範囲、理解精度、推論の深さ</span>の観点から密接に関連しています。したがって、実世界のアプリケーション向けの包括的なオムニモーダルモデルを開発し、その深い推論能力を強化することが基本となります。</p>
<p>初期の研究である <strong>AnyGPT (Zhan et al., 2024)</strong> は、様々なモダリティの統一処理に離散表現を利用し、モダリティ間の統一的な理解と生成を実現しました。</p>
<p>最近では、</p>
<ul class="unstyled-list">
<li><i class="fas fa-lightbulb"></i> <strong>BaichuanOmni-1.5 (Li et al., 2025h)</strong> は、様々なモダリティにわたる協調的なリアルタイム理解において印象的な能力を示しています。</li>
<li><i class="fas fa-project-diagram"></i> <strong>Qwen2.5-Omni (Xu et al., 2025b)</strong> は、Time-aligned Multimodal RoPEと呼ばれる新しい位置埋め込みを使用して、ビデオ入力のタイムスタンプを音声と同期させます。</li>
<li><i class="fas fa-code-branch"></i> <strong>M2-omni (Guo et al., 2025)</strong> や <strong>MiniCPM-o (Yu et al., 2024b)</strong> のような最新のオープンソースの取り組みは、GPT-4oのようなクローズドソースモデルとの性能差を縮めています。</li>
</ul>
<p>実世界の特定のニーズに牽引され、より小さなサイズのオムニモーダルモデルがますます注目を集めています。</p>
<ul class="unstyled-list">
<li><i class="fas fa-mobile-alt"></i> <strong>Megrez-3B-Omni (Li et al., 2025a)</strong> は、シーン理解やOCRなどのタスクで優れた性能を発揮するオンデバイスのマルチモーダル理解LLMモデルです。</li>
<li><i class="fas fa-microphone-alt"></i> <strong>Mini-Omni2 (Xie &amp; Wu, 2024)</strong> は、視覚および音声クエリに対してリアルタイムのエンドツーエンド音声応答を提供できる視覚-音声アシスタントです。</li>
<li><i class="far fa-smile"></i> <strong>R1-Omni (Zhao et al., 2025c)</strong> は、視覚および聴覚情報からの感情認識に焦点を当てています。</li>
</ul>
<p>これらの進歩にもかかわらず、マルチモーダルAIにおける現在の研究は、主に統一されたマルチモーダル表現の理解と生成の強化に焦点を当てています。<span class="highlight">クロスモーダルインタラクションを効果的に統合し、問い詰める推論能力の開発は、依然として決定的に未開拓です。</span>このギャップを埋めることは、ネイティブなマルチモーダル推論モデル、つまり、人間のような高度さで相互接続されたモダリティを処理、分析、合成するように本質的に設計されたシステムを実現するために不可欠です。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-microchip"></i>4.3 Technical Prospects</h3>
<div class="content-box">
<p>ネイティブ大規模マルチモーダル推論モデル（N-LMRM）の技術的展望は、言語や視覚から、音声、触覚、センサー測定値、時系列、構造化データに至るまで、<span class="keyword">多様なデータタイプにわたる理解、生成、推論をネイティブに統一する</span>ことを目指しており、私たちを、統一的かつ一貫した方法で見て、聞いて、話し、行動できるシステムに近づけます。</p>
<p>しかし、このようなN-LMRMの構築には大きな課題があります。これらのモデルは、</p>
<ul class="unstyled-list">
<li><i class="fas fa-sitemap"></i> 単一システム内で異種モダリティを処理できるようにアーキテクチャ的に設計されなければなりません。</li>
<li><i class="fas fa-tools"></i> 長いマルチモーダル推論チェーンを通じて多様なツールを遺伝的に使用し組み合わせる必要があります。</li>
<li><i class="fas fa-sync-alt"></i> 実世界のインタラクションからの継続的な学習をサポートする必要があります。</li>
</ul>
<p>このセクションでは、N-LMRM構築における主要な課題を概説し、それらを解決するためのいくつかの潜在的な道筋を提案します。</p>
<img alt="Figure 9: 次世代ネイティブ大規模マルチモーダル推論モデルの概要" class="section-image" src="native_multimodal_reasoning_model.jpg"/>
<div class="note-box">
<p class="note-title"><i class="fas fa-image"></i> 図9の解説: 次世代ネイティブ大規模マルチモーダル推論モデルの概要</p>
<p>この図は、N-LMRMが目指すシステムの全体像を示しています。</p>
<ol>
<li><strong>実世界データ（環境）の知覚:</strong> まず、画像、音声、動画、テキスト、センサーデータなど、現実世界の多様なモダリティからの情報を入力として受け取ります。これらの入力は<span class="highlight">「統一的な知覚 (Unifying Perception)」</span>プロセスを経ます。</li>
<li><strong>ネイティブ大規模マルチモーダル推論モデル (Native Large Multimodal Reasoning Model):</strong>
<ul class="unstyled-list">
<li><i class="fas fa-database"></i> <strong>データ駆動型のあらゆるモーダルでの深いインタラクション (Data-Driven Any-Modal Deep Interaction)</strong>: 入力された多様なデータを深く処理し、相互作用させます。</li>
<li><i class="fas fa-link"></i> これが進化して<span class="keyword">オムニモーダル理解と推論 (Omni-Modal Understanding and Reasoning)</span> につながります。つまり、あらゆる種類のモダリティを包括的に理解し、それに基づいて推論する能力です。</li>
<li><i class="fas fa-random"></i> 並行して、この深いインタラクションは<span class="keyword">マルチモーダルなインターリーブ生成 (Multimodal Interleaved Generation)</span> を可能にします。これは、複数のモダリティ（例：テキストと画像）を交互に組み合わせてコンテンツを生成する能力です。</li>
<li><i class="fas fa-lightbulb"></i> オムニモーダル理解と推論は、さらに<span class="keyword">生成的推論と長期思考（計画） (Generative Reasoning and Long Thinking (Plan))</span> へと発展します。これは、新しい情報を生成する推論や、より長いスパンでの思考や計画立案能力を指します。</li>
<li><i class="fas fa-arrows-alt-h"></i> これら2つの主要な能力群、すなわち「オムニモーダル理解と生成的推論」と「マルチモーダルエージェント推論」がN-LMRMの中核を成します。</li>
</ul>
</li>
<li><strong>環境操作 (Environment Manipulation):</strong>
<ul class="unstyled-list">
<li><i class="fas fa-cogs"></i> N-LMRMは、学習した内容に基づいて環境を<span class="highlight">「制御 (Control)」</span>したり、環境からの経験を通じて<span class="highlight">「学習 (Learning from Experience)」</span>したりします。</li>
<li><i class="fas fa-robot"></i> これにより、ロボット、スマートフォン、コンピュータ、その他のツールを操作し、<span class="keyword">マルチモーダルエージェント推論 (Multimodal Agentic Reasoning)</span> を実現します。つまり、多様なモダリティ情報を活用して、自律的に目標を達成するために行動する能力です。</li>
</ul>
</li>
</ol>
<p>このモデルは、多様な実世界のデータモダリティにわたる包括的な知覚を達成し、正確なオムニモーダル理解と詳細な生成的推論を可能にすることを目指しています。この基盤モデルは、より高度な形態の知的行動につながり、世界の経験から学習し、生涯学習と自己改善を実現します。</p>
</div>
<h4><i class="fas fa-project-diagram"></i> Unified Representations and Cross-Modal Fusion</h4>
<p>基本的な課題は、異なるモдаリティを一貫した方法で処理し生成できる単一のモデルアーキテクチャを作成することです。従来のアプローチでは、各モダリティに個別のエンコーダを使用することがよくありました (Lyu et al., 2023; Li et al., 2024l)。対照的に、ネイティブオムニモーダルモデルは、モダリティ間のシームレスな相互作用を可能にする、より統一された設計を求めています。</p>
<p>考えられる解決策の1つは、すべての入力と出力を共通の形式に均質化し、あらゆるモダリティを均一に処理することです。このアプローチは、あるモダリティが他のモダリティの表現を支配したり損なったりする<span class="keyword">負の干渉</span>を防ぐために慎重な設計が必要です (Leng et al., 2024; Chen et al., 2024g)。</p>
<p>そこで新たな解決策として浮上しているのが、<span class="keyword">Mixture-of-Experts (MoE) アーキテクチャ</span>です。このアーキテクチャでは、特定のモダリティに特化したエキスパートが関連する入力に対してのみアクティブ化され、コアとなる言語モデルが言語知能のバックボーンとして機能します (Chen et al., 2024i; Li et al., 2025j; Team, 2025a; Shukor et al., 2025)。</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-brain"></i> Mixture-of-Experts (MoE) アーキテクチャ</p>
<p>MoEは、複数の「エキスパート」ネットワークと、どのエキスパートを使用するかを決定する「ゲート」ネットワークで構成されるニューラルネットワークアーキテクチャです。</p>
<div style="text-align: center; margin: 20px;">
<svg height="200" style="font-family: 'Yomogi', cursive;" viewbox="0 0 300 150" width="300">
<defs>
<marker id="arrow" markerheight="6" markerwidth="6" orient="auto-start-reverse" refx="5" refy="5" viewbox="0 0 10 10">
<path d="M 0 0 L 10 5 L 0 10 z" fill="#4a6fa5"></path>
</marker>
</defs>
<rect fill="#e6f3ff" height="40" rx="5" ry="5" stroke="#4a6fa5" stroke-width="1" width="80" x="10" y="50"></rect>
<text font-size="12px" text-anchor="middle" x="50" y="75">入力データ</text>
<path d="M 90 70 L 120 70" marker-end="url(#arrow)" stroke="#4a6fa5" stroke-width="2"></path>
<rect fill="#fff0e6" height="40" rx="5" ry="5" stroke="#ff7e5f" stroke-width="1" width="60" x="120" y="50"></rect>
<text font-size="12px" text-anchor="middle" x="150" y="75">ゲート</text>
<rect fill="#e6ffe6" height="30" rx="5" ry="5" stroke="#5cb85c" stroke-width="1" width="80" x="200" y="10"></rect>
<text font-size="12px" text-anchor="middle" x="240" y="30">エキスパート1 (例:視覚)</text>
<path d="M 150 50 L 200 30" marker-end="url(#arrow)" stroke="#4a6fa5" stroke-dasharray="3,3" stroke-width="1"></path>
<rect fill="#e6ffe6" height="30" rx="5" ry="5" stroke="#5cb85c" stroke-width="1" width="80" x="200" y="55"></rect>
<text font-size="12px" text-anchor="middle" x="240" y="75">エキスパート2 (例:音声)</text>
<path d="M 150 70 L 200 70" marker-end="url(#arrow)" stroke="#4a6fa5" stroke-dasharray="3,3" stroke-width="1"></path>
<rect fill="#e6ffe6" height="30" rx="5" ry="5" stroke="#5cb85c" stroke-width="1" width="80" x="200" y="100"></rect>
<text font-size="12px" text-anchor="middle" x="240" y="120">エキスパートN (例:言語)</text>
<path d="M 150 90 L 200 110" marker-end="url(#arrow)" stroke="#4a6fa5" stroke-dasharray="3,3" stroke-width="1"></path>
</svg>
</div>
<p>N-LMRMの場合、各エキスパートが特定のモダリティ（視覚、聴覚など）の処理に特化し、ゲートネットワークが入力データの種類に応じて適切なエキスパートを選択・組み合わせることで、効率的かつ効果的なマルチモーダル処理を目指します。</p>
</div>
<h4><i class="fas fa-link"></i> Interleaved Multimodal Long Chain-of-Thought</h4>
<p>統一された表現を基盤として、N-LMRMは従来の長い内部思考連鎖を、複数のモダリティにまたがる<span class="keyword">インターリーブされた推論プロセス</span>へと拡張できます。これにより、異なるモダリティをシームレスに融合するテスト時計算スケーリングの新たな軸が可能になります (Wang et al., 2025a)。</p>
<p>OpenAIが最近リリースしたo3とo4-miniは、この方向への先駆的な一歩を表しており、つまり、思考連鎖の中で画像を扱って推論します (OpenAI, 2025b)。これは、画像をズーム、トリミング、反転、または強調できるツールを自動的に処理することによって行われます。重要なのは、これらの機能が、個別の専門モデルに依存することなく<span class="highlight">ネイティブに備わっている</span>ことです (Wu &amp; Xie, 2023; Hu et al., 2024b; Feng et al., 2025a; Qian et al., 2025; Wang et al., 2025d)。</p>
<p>ソフトウェア工学 (OpenAI, 2025)、IMOレベルの数学 (DeepSeek-AI et al., 2025)、創造的な執筆 (Zhao et al., 2024c)、GUI操作 (Qin et al., 2025) などのドメインにおける強化学習の有望な般化能力に後押しされ、強化学習をより多くのモダリティ、より長いツール拡張推論チェーン、より広範な推論タスクにスケーリングすることが、クロスモーダル推論をシミュレートし、機械知能を高めることができる次世代N-LMRMのレシピとなる可能性があります。</p>
<div class="process-step">
<div class="step-number">💡</div>
<div class="step-content">
<p><strong>インターリーブされた思考の連鎖の例:</strong></p>
<p>料理のレシピを理解し実行するタスクを考えます。</p>
<ol>
<li><strong>テキスト入力:</strong> 「トマトソースパスタを作る」</li>
<li><strong>内部思考 (テキスト):</strong> 「材料は何か？トマト、パスタ、玉ねぎ、ニンニク...」</li>
<li><strong>画像入力/生成:</strong> (冷蔵庫の中の画像を見て) 「トマトと玉ねぎはある。ニンニクがない。」 または (レシピサイトの画像を見て) 「完成形はこんな感じか。」</li>
<li><strong>内部思考 (テキスト＋画像):</strong> 「ニンニクがないので、代替品を探すか、ニンニクなしで作るか...。(完成図を見て) ニンニクは風味付けだから、なくても大丈夫そう。」</li>
<li><strong>行動計画 (テキスト):</strong> 「まずパスタを茹でる。その間に玉ねぎを炒める...」</li>
<li><strong>音声入力/生成:</strong> (タイマーが鳴る音) 「パスタが茹で上がったようだ。」</li>
<li><strong>内部思考 (テキスト＋音声):</strong> 「パスタを湯切りして、ソースと混ぜる。」</li>
</ol>
<p>このように、テキスト、画像、音声といった異なるモダリティからの情報を思考の連鎖の途中で柔軟に取り込み、判断や計画に活かすのがインターリーブされたマルチモーダル思考の連鎖です。</p>
</div>
</div>
<h4><i class="fas fa-globe-americas"></i> Learning and Evolving from World Experiences</h4>
<p>動的に進化する知的システムにおいて、LMRMベースの「ワールドモデル2」の中核的な価値は、自動運転のような複雑な環境におけるリアルタイムのモデリングと推論能力だけでなく (Wang et al., $2024 \mathrm { m } )$)、環境との継続的な相互作用を通じた<span class="keyword">生涯学習 (life-long learning)</span> のための進化的メカニズムにもあります (Thrun &amp; Mitchell, 1995)。</p>
<p>MCPとA2Aがツールとエージェントクラスタの高密度ネットワークを作成すると、システムは環境、ツール、他のエージェントとの多次元的な関与を通じて、各相互作用を構造化された経験に変換できます。これには、リアルタイムデータストリームのパターン認識からツール操作チェーン全体の因果推論、通信ネットワークにおける協調的フィードバックから異常シナリオにおける自律的適応まで、あらゆるものが含まれます。</p>
<div class="glass-card">
<p><i class="fas fa-project-diagram"></i> <strong>MCP (Model Context Protocol) と A2A (Agent2Agent Protocol)</strong></p>
<p>これらは、異なるAIモデルやエージェントが効果的にコミュニケーションし、協力するための標準化された方法や取り決め（プロトコル）を指す概念です。</p>
<ul class="unstyled-list">
<li><strong>MCP:</strong> あるモデルが別のモデルに情報を渡す際に、その情報の文脈（何についての情報か、どのような形式かなど）も一緒に伝えることで、受け取り側のモデルが情報をより正確に理解し活用できるようにする仕組みです。</li>
<li><strong>A2A:</strong> 複数のAIエージェントが互いにタスクを依頼したり、情報を共有したり、協調して問題を解決したりするための共通言語やルールセットのようなものです。</li>
</ul>
<p>これらが整備されると、AIシステム全体としてより高度で複雑なタスクを、効率的に分担・連携して実行できるようになると期待されます。</p>
</div>
<p>この継続的な学習パラダイムにより、LMRMは静的な知識ベースの限界を克服できます。世界の経験を繰り返し蓄積することで、認知アーキテクチャと意思決定戦略を動的に更新します。特にオープンな環境では、自律的な学習メカニズムがモデルを積極的にツール組み合わせの可能性を探求するように促します。新しい問題を解決する過程で、同時に転移可能な知識を保存し、最終的には専門的な推論能力を所有しながら、クロスシナリオの般化回復力を維持する知的システムを形成します。</p>
<p>私たちは、オンライン強化学習のインタラクティブな学習方法とオフライン検証方法が、GUIエージェントモデル (Qin et al., 2025; Zheng et al., 2025a; Wang et al., 2024n) で継続的にパフォーマンスを向上させるために利用されてきたように、LMRMの能力を反復的かつ継続的に刺激する可能性があると考えています。</p>
<h4><i class="fas fa-cogs"></i> Data Synthesis</h4>
<p>LMRMの現在の能力は、主に<span class="keyword">データ駆動型</span>です。事前学習段階でこれらのモデルを強化するためには、その機能を調整する高品質な<span class="keyword">データ合成パイプライン</span>を開発することが不可欠です。</p>
<p>データ合成に関する既存の取り組みのほとんど (Chang et al., 2024; Huang et al., 2025c; Xu et al., 2024c) は、特に視覚、言語、音声などのドメインにおける単一モーダルまたはクロスモーダルな理解と推論の向上に焦点を当てています。</p>
<p>しかし、以下のようなより複雑な側面については、限定的な探求しか行われていません。</p>
<div class="info-grid">
<div class="info-card">
<div class="feature-item">
<i class="fas fa-link"></i>
<p>3つ以上のモダリティの整合</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-comments"></i>
<p>マルチモーダルなインタラクティブな思考の連鎖と視覚的生成の作成</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-tasks"></i>
<p>動的環境における多段階計画の実装</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-tools"></i>
<p>マルチツールの呼び出しと並列ツール使用の調整</p>
</div>
</div>
</div>
<p>これらの分野は、マルチモーダル推論モデルを進歩させるための大きな機会を提供しています。</p>
<div class="bubble-box">
<p><i class="fas fa-dna"></i> 結論として、私たちは、有能な推論者から自律的なエージェントへの移行に向けた最初のステップとして、<span class="keyword">N-LMRM</span>の概念を導入します。さらに、OpenAIのAGIへの5段階の道筋 (OpenAI, 2023) に沿って、自己進化するイノベーター (Yamada et al., 2025) やマルチエージェント組織 (Zhang et al., 2025d) を含む後続の段階の基礎を築いています。私たちの研究提案に基づいて、将来の研究では、よりエージェント的でオムニモーダルな能力を探求し、ますます自律的な機械知能の開発を進めることができます。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-chalkboard-teacher"></i>Takeaways: Native Large Multimodal Reasoning Model (LMRMs)</h3>
<div class="content-box">
<p>このセクションでは、最新の大規模マルチモーダルモデル（例：O3、O4-mini）と、それらが困難なタスクやベンチマークでどのような性能を示すかを検証しました。</p>
<p>次に、ネイティブなマルチモーダル大規模モデルの将来の方向性について、能力の範囲とレベルの観点から提示しました。これには以下の要素が含まれます。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-eye"></i>
<p><span class="highlight">オムニモーダルな知覚と理解</span></p>
<p>あらゆる種類のモダリティ情報を包括的に捉え、その意味を深く理解する能力。</p>
</div>
<div class="feature-item">
<i class="fas fa-comments-dollar"></i>
<p><span class="highlight">マルチモーダルなインタラクティブ生成的推論</span></p>
<p>複数のモダリティ間で対話的に情報をやり取りし、新しい情報や洞察を生成する推論能力。</p>
</div>
<div class="feature-item">
<i class="fas fa-robot"></i>
<p><span class="highlight">知的エージェントの振る舞い</span></p>
<p>自律的に目標を設定し、計画を立て、環境と対話しながらタスクを実行する能力。</p>
</div>
</div>
<p>このビジョンを実現するために、<span class="keyword">統一的知覚</span>、<span class="keyword">学習方法</span>、および<span class="keyword">データ合成</span>に関連するアプローチについて議論しました。</p>
<div class="pipeline">
<div class="pipeline-step"><strong>課題の特定:</strong> 現在のLMRMの限界（言語中心、リアルタイムインタラクションの弱さ）</div>
<div class="pipeline-step"><strong>解決策の提示:</strong> N-LMRMの概念導入（マルチモーダルエージェント推論、オムニモーダル理解と生成的推論）</div>
<div class="pipeline-step"><strong>実験的証拠:</strong> o3/o4-miniなどの事例分析、ベンチマークでの性能評価</div>
<div class="pipeline-step"><strong>技術的展望:</strong> 統一表現、インターリーブ思考連鎖、世界経験からの学習、データ合成</div>
<div class="pipeline-step"><strong>最終目標:</strong> 包括的知覚、正確な理解、深い推論を備えたN-LMRMによる機械知能のパラダイムシフト</div>
</div>
<p>私たちは、ネイティブLMRMが機械知能におけるパラダイムシフトとして、包括的な知覚、正確な理解、そして深い推論を達成することを期待しています。</p>
</div>
</div>
<div class="section-card" id="5_Dataset_and_Benchmark">
<h2 class="section-title"><i class="fas fa-database"></i>5 Dataset and Benchmark</h2>
<p class="content-box">
        マルチモーダル推論モデル（Multimodal Reasoning Models）の開発と最適化を探求する中で、モデルの性能を様々な側面（例：<span class="keyword">動画理解</span>や<span class="keyword">視覚的推論</span>）で評価するための経験的な能力評価や分析を行うために、多くのタスクとベンチマークが提案されてきました。このセクションでは、マルチモーダル推論モデルの開発を促進するのに役立つ既存のデータセットを、能力に基づいて以下の<strong class="highlight">4つの主要なタイプ</strong>に要約し、分類します。
    </p>
<div class="info-grid">
<div class="info-card glass-card">
<div class="icon-item"><i class="fas fa-brain"></i></div>
<h4 class="subsection-title mini"><i class="fas fa-lightbulb"></i>1. 理解 (Understanding)</h4>
<p>モデルが複数のモダリティからの情報を処理し、解釈する能力。</p>
</div>
<div class="info-card glass-card">
<div class="icon-item"><i class="fas fa-paint-brush"></i></div>
<h4 class="subsection-title mini"><i class="fas fa-magic"></i>2. 生成 (Generation)</h4>
<p>異なるデータタイプにわたる新しいコンテンツを作成する能力。</p>
</div>
<div class="info-card glass-card">
<div class="icon-item"><i class="fas fa-cogs"></i></div>
<h4 class="subsection-title mini"><i class="fas fa-user-cog"></i>3. 推論 (Reasoning)</h4>
<p>複数のモダリティからの情報を統合し、論理的な結論を導き出す能力。</p>
</div>
<div class="info-card glass-card">
<div class="icon-item"><i class="fas fa-route"></i></div>
<h4 class="subsection-title mini"><i class="fas fa-map-signs"></i>4. 計画 (Planning)</h4>
<p>多様な入力を統合・処理し、複雑な多段階タスクを実行する能力。</p>
</div>
</div>
<p class="content-box">
        次に、これらのベンチマークやデータセットで一般的に使用される評価指標と評価側面を要約します。ベンチマークは特定の能力評価を目的として設計されており、図10に示すように4つの主要カテゴリと、表9に示す11のサブカテゴリに分類します。
    </p>
<div class="note-box">
<p class="note-title"><i class="fas fa-sticky-note"></i>注意</p>
<p>論文中で言及されているTable 9は、この資料では提供されていません。Table 9は、タスク別に分類されたマルチモーダルベンチマークとデータセット（訓練用）の概要を示しており、理解（視覚中心、聴覚中心）、生成（クロスモーダル、共同マルチモーダル）、推論（一般視覚、ドメイン固有）、計画（GUI、実体化・シミュレート環境）のカテゴリを含んでいます。これらのベンチマークは、タスクを成功裏に完了するために、短期的または長期的な推論（例：挑戦的な視覚・聴覚生成）をしばしば必要とします。</p>
</div>
<img alt="図10: データセットとベンチマークの概要" class="section-image" src="multimodal_benchmarks_datasets.jpg"/>
<p class="caption" style="text-align: center; font-size: 0.9em; color: var(--color-gray);">
<span class="keyword">図10: データセットとベンチマークの概要。</span>マルチモーダルデータセットとベンチマークを、理解、生成、推論、計画の4つの主要カテゴリに再編成しています。
    </p>
<h3 class="subsection-title"><i class="fas fa-eye-slash"></i>5.1 Multimodal Understanding</h3>
<p class="content-box">
<span class="keyword">マルチモーダル理解</span>とは、モデルが視覚データや聴覚データなど、<strong class="highlight">複数のモダリティからの情報を処理し解釈する能力</strong>を指します。これにより、理解、推論、生成を必要とするタスクを実行できるようになります。これらのタスクは、より人間らしい方法で現実世界と対話し応答することができるモデルを開発する上で非常に重要です。タスクの定義に基づき、既存のマルチモーダル理解タスクは、おおよそ以下の2つの主要領域に分類できます。
    </p>
<div class="two-column">
<div class="column glass-card" style="padding: 15px;">
<h4 class="section-title mini" style="font-size: 16px;"><i class="fas fa-eye"></i>1) Visual-Centric Understanding (視覚中心の理解)</h4>
<p>モデルが視覚コンテンツを理解し推論する能力を含みます。</p>
</div>
<div class="column glass-card" style="padding: 15px;">
<h4 class="section-title mini" style="font-size: 16px;"><i class="fas fa-assistive-listening-systems"></i>2) Audio-Centric Understanding (聴覚中心の理解)</h4>
<p>音声、音楽、環境音などのオーディオ関連タスクに焦点を当てます。</p>
</div>
</div>
<h4 class="subsection-title"><i class="fas fa-glasses"></i>5.1.1 Visual-Centric Understanding</h4>
<p class="content-box">
<span class="keyword">視覚中心の理解</span>は、モデルが画像や動画などの<strong class="highlight">視覚データを理解し推論する能力</strong>を、様々な専門タスクを通じて評価します。これらのタスクは、以下のドメインに大別できます。
    </p>
<ul class="unstyled-list">
<li class="bubble-box">✏️ <span class="keyword">一般的な視覚理解 (General visual understanding)</span>: 自然画像における物体認識や空間推論など。</li>
<li class="bubble-box">📄 <span class="keyword">文書と図表の解釈 (Document and chart interpretation)</span>: 文書やグラフのような構造化された視覚データの解釈。</li>
<li class="bubble-box">🌐 <span class="keyword">多言語視覚推論 (Multilingual visual reasoning)</span>: 複数の言語にわたる視覚的推論。</li>
<li class="bubble-box">🎬 <span class="keyword">動画理解 (Video understanding)</span>: 動的な視覚情報の時間的理解。</li>
<li class="bubble-box">🔬 <span class="keyword">数学的および科学的推論 (Mathematical and scientific reasoning)</span>: 視覚入力に基づく数学的・科学的問題解決。</li>
<li class="bubble-box">📊 <span class="keyword">包括的ベンチマーク (Comprehensive benchmarks)</span>: 複数の視覚理解能力を総合的に評価。</li>
</ul>
<p class="content-box">各ドメインは、視覚理解の異なる側面に対応しています。以下で、これらのカテゴリを詳細に見ていきましょう。</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-image"></i>一般的な視覚理解 (General Visual Understanding)</p>
<p>一般的な視覚質問応答（VQA）データセットは、複雑さと範囲の両方で大幅に進化してきました。</p>
<div class="two-column">
<div class="column">
<div class="definition-box">
<p class="definition-title">初期のデータセット <i class="fas fa-history"></i></p>
<p>例: <code>VQA</code> (Kafle &amp; Kanan, 2016), <code>GQA</code> (Ainslie et al., 2023)</p>
<ul>
<li>主に<span class="highlight">物体認識</span>、<span class="highlight">属性識別</span>、<span class="highlight">単純な空間推論</span>に焦点。</li>
<li>自然画像が中心で、基本的な知覚に関するものが多い。</li>
<li>質問形式は単純（例：「車の色は何ですか？」）。</li>
</ul>
</div>
</div>
<div class="column">
<div class="definition-box">
<p class="definition-title">最近のデータセット <i class="fas fa-rocket"></i></p>
<p>例: <code>ALIGN</code> (Jia et al., 2021), <code>Visual Genome</code> (Krishna et al., 2016), <code>LAION-400M</code> (Schuhmann et al., 2021), <code>LAION-5B</code> (Schuhmann et al., 2022), <code>FILIP</code> (Yao et al., 2021), <code>YFCC100M</code> (Thomee et al., 2016)</p>
<ul>
<li>より複雑な<span class="highlight">視覚言語タスク</span>（画像-テキスト整列、マルチモーダル表現など）に対応。</li>
<li><code>Visual Genome</code>: 関係性や物体レベルの情報を含み、推論の限界を押し広げる。</li>
<li><code>LAION-400M</code>, <code>LAION-5B</code>: 大規模な画像-テキストペアのコレクションで、視覚言語モデルの大規模訓練を可能にする。</li>
<li><code>FILIP</code>, <code>YFCC100M</code>: 視覚と言語の両方を統合し、多様なベンチマークでモデルの性能を向上。</li>
</ul>
</div>
</div>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="far fa-file-alt"></i>文書、図表、OCRの視覚理解 (Document, Chart, and OCR Visual Understanding)</p>
<p>文書、図表、OCRベースのVQAデータセットは、テキスト要素を含む<strong class="highlight">構造化された視覚情報</strong>の理解に焦点を当てた専門ドメインを形成します。</p>
<ul class="unstyled-list">
<li><span class="badge blue">DocVQA</span> (Mathew et al., 2021): 文書理解を対象とし、モデルは質問に答えるために文書内のテキストを特定し解釈する必要がある。</li>
<li><span class="badge blue">DVQA</span> (Kafle et al., 2018): 棒グラフ、折れ線グラフ、円グラフなどの視覚データ表現の解釈に焦点を当て、これらの構造を理解するモデルの能力をテストする。</li>
<li><span class="badge blue">TextVQA</span> (Singh et al., 2019), <span class="badge blue">OCR-VQA</span> (Mishra et al., 2019): 自然画像に埋め込まれたテキストの読解と推論を強調する。</li>
<li><span class="badge blue">AI2D</span> (Hiippala et al., 2021): 図や構造化された視覚表現に焦点を当て、グラフィカルコンテンツ上の推論を強化する。</li>
</ul>
<p>これらのデータセットには、いくつかの際立った特徴があります:</p>
<div class="feature-card-grid">
<div class="feature-item glass-card">
<div class="icon-item"><i class="fas fa-search-plus"></i></div>
<p>OCRと視覚理解の重要な統合</p>
</div>
<div class="feature-item glass-card">
<div class="icon-item"><i class="fas fa-project-diagram"></i></div>
<p>テキスト要素と視覚要素の両方を組み合わせた多段階推論</p>
</div>
<div class="feature-item glass-card">
<div class="icon-item"><i class="fas fa-book-open"></i></div>
<p>文書構造、図表の慣例、テキストレイアウトに関するドメイン固有の知識</p>
</div>
</div>
<p>一般的なVQAデータセットとは異なり、これらのコレクションは視覚コンテンツとテキストコンテンツの相互作用を強く強調し、モデルがより構造化された文脈でモダリティ間の橋渡しをすることを要求します。</p>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-globe-americas"></i>多言語視覚理解 (Multilingual Visual Understanding)</p>
<p>多言語視覚理解データセットは、マルチモーダルシステムにおける言語の多様性に対する需要の高まりに応えます。</p>
<p>データセット例:</p>
<div class="tag-list">
<span class="tag">CMMLU (Li et al., 2024f)</span>
<span class="tag">C-Eval (Huang et al., 2023c)</span>
<span class="tag">Exams-v (Das et al., 2024)</span>
<span class="tag">M3exam (Zhang et al., 2023e)</span>
<span class="tag">VideoVista-CulturalLingo (Chen et al., 2025e)</span>
<span class="tag">MTVQA (Tang et al., 2024)</span>
</div>
<p>これらのデータセットは英語中心のVQAシステムを超えた範囲をカバーし、以下の特徴があります:</p>
<div class="info-grid">
<div class="info-card glass-card">
<div class="icon-item"><i class="fas fa-language"></i></div>
<p>複数の言語（様々な言語族をカバー）での質問と注釈の統合。</p>
</div>
<div class="info-card glass-card">
<div class="icon-item"><i class="fas fa-users"></i></div>
<p>異なる文化的文脈における視覚理解と言語能力のテスト。</p>
</div>
<div class="info-card glass-card">
<div class="icon-item"><i class="fas fa-palette"></i></div>
<p>特定の文化的解釈や参照を持つ可能性のある視覚コンセプトを理解するモデルの要求。</p>
</div>
</div>
<p>単一言語のVQAデータセットとは異なり、これらの多言語データセットはMLLMの<strong class="highlight">クロスリンガル転移能力</strong>を評価し強化します。</p>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-video"></i>動画理解 (Video Understanding)</p>
<p>動画理解データセット（例: <code>ActivityNet-QA</code> (Yu et al., 2019a), <code>PerceptionTest</code> (Patraucean et al., 2023)）は、動的な視覚タスクにおけるモデルの訓練と評価にますます使用されています。</p>
<p>静止画像データセットと比較して、これらのデータセットはモデルに<strong class="highlight">時間ベースの理解</strong>への対応を要求し、複数のフレームにわたる動的な視覚的特徴を含みます。特徴は以下の通りです:</p>
<ul class="unstyled-list">
<li class="bubble-box"><i class="fas fa-running"></i> アクション、イベント、時間的関係の注釈を含む。</li>
<li class="bubble-box"><i class="fas fa-clock"></i> 短いクリップから数分間の動画まで、多様な動画時間をカバー。</li>
</ul>
<p>既存の動画評価データセットは、以下のような課題に取り組むために拡張されています:</p>
<div class="info-grid">
<div class="info-card glass-card">
<p><span class="badge yellow">科学ドメイン</span>: <code>Video-MMMU</code> (Hu et al., 2025b)</p>
</div>
<div class="info-card glass-card">
<p><span class="badge yellow">長時間動画ドメイン</span>: <code>Video-MME</code> (Fu et al., 2024a)</p>
</div>
<div class="info-card glass-card">
<p><span class="badge yellow">包括的な動画理解と推論</span>: <code>VideoVista</code> (Li et al., 2024k)</p>
</div>
</div>
<p><code>VideoVista</code> は、数秒から10分以上の14カテゴリの動画を特徴とし、19の理解タスクと8つの推論タスクを網羅する汎用的なベンチマークです。GPT-4oを活用した自動注釈フレームワークを使用し、スケーラビリティと多様性を強化しています。</p>
<p>訓練用データセット例:</p>
<ul class="unstyled-list">
<li><span class="badge purple">YouTube8M</span> (Abu-ElHaija et al., 2016): 大規模動画分類とマルチモーダル理解の基礎となる。</li>
<li><span class="badge purple">VidGen-1M</span> (Tan et al., 2024b), <span class="badge purple">WebVid</span> (Bain et al., 2022): マルチモーダルなテキストと視覚信号を統合することで動画理解を強化することに焦点を当てる。</li>
</ul>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-cubes"></i>包括的ベンチマーク (Comprehensive Benchmarks)</p>
<p><code>MMBench</code> (Liu et al., 2024f), <code>Seed-Bench</code> (Li et al., 2023c), <code>MME-RealWorld</code> (Zhang et al., 2024f) などの統合評価ベンチマークは、既存のマルチモーダルモデルのより<strong class="highlight">包括的な評価</strong>を提供するために登場しました。</p>
<p>これらのベンチマークは、モデルが現実世界のシナリオで視覚と言語の理解をどの程度統合できるかをテストし、以下の特徴を含みます:</p>
<div class="pipeline">
<div class="pipeline-step glass-card">
<span class="badge blue">1</span> 知覚から推論、知識統合まで、視覚理解の様々な側面を評価する<strong class="highlight">多次元評価フレームワーク</strong>。
            </div>
<div class="pipeline-step glass-card">
<span class="badge blue">2</span> 特定の能力を探求し弱点を特定することを目的とした、<strong class="highlight">慎重に設計された質問</strong>。
            </div>
<div class="pipeline-step glass-card">
<span class="badge blue">3</span> モデル間の公正な比較のための<strong class="highlight">標準化された評価パイプライン</strong>。
            </div>
</div>
<p>初期のタスク固有データセットとは異なり、これらのベンチマークはモデルの全体的な能力の包括的な尺度を提供します。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-bullseye"></i>まとめ: 視覚中心の理解</p>
<p>視覚中心の理解は、画像における基本的な物体認識から、動画や文書における複雑なマルチモーダル推論まで、モデルが視覚データを処理し推論する能力を強調します。一般的な視覚理解、文書解釈、多言語推論、動画理解といった様々な専門タスクに取り組むことで、これらのベンチマークはモデルの視覚能力の包括的な視点を提供します。これらの評価は、モデルが視覚的知覚と推論を統合できることを保証するために不可欠であり、これは現実世界のアプリケーションにとって極めて重要です。</p>
</div>
<h4 class="subsection-title"><i class="fas fa-microphone-alt"></i>5.1.2 Audio-Centric Understanding</h4>
<p class="content-box">
<span class="keyword">聴覚中心の理解</span>とは、モデルが音声、環境音、音楽などの<strong class="highlight">様々な形態のオーディオ入力を処理、解釈、応答する能力</strong>の評価を指します。これらのモダリティが機械学習タスクにますます不可欠になるにつれて、モデルがオーディオデータをどの程度理解し相互作用するかを評価することが重要な焦点となっています。評価は、音声、オーディオ、音楽理解のさまざまな側面に及び、オーディオ関連タスクにおける精度、翻訳、感情認識、および一般的な理解を評価するために様々なベンチマークとデータセットが設計されています。これらの評価は、現実世界のアプリケーションで遭遇する全範囲のオーディオデータを理解するモデルの有効性を測るのに役立ちます。
    </p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-comments"></i>音声理解 (Speech Understanding)</p>
<p>音声評価データセットは、オーディオドメインにおけるモデルのパフォーマンスを評価する上で重要な役割を果たします。これらのデータセットは主に、モデルが現実世界の設定で人間の音声を正確かつ明確に理解できるかどうかを測定します。既存のデータセットは、いくつかの観点から音声理解を評価します。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-bullhorn"></i>1) 音声認識の精度</p>
<ul class="unstyled-list">
<li><code>Librispeech</code> (Panayotov et al., 2015): 様々な話者が読んだオーディオブックのデータセットで、英語音声認識の広く使用される評価指標として機能します。</li>
<li><code>Common Voice</code> (Ardila et al., 2020): 世界中のボランティアから音声録音を収集し、モデル訓練のための多様な音声データセットを提供します。</li>
<li><code>Aishell</code> (Bu et al., 2017) シリーズ: 中国語音声認識の標準です。</li>
<li><code>Fleurs</code> (Conneau et al., 2022): 複数の言語にわたる音声認識および音声テキスト翻訳モデルを評価します。</li>
</ul>
</div>
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-language"></i>2) 音声多言語翻訳タスク</p>
<ul class="unstyled-list">
<li><code>CoVoST2</code> (Wang et al., 2020): モデルのリアルタイム音声認識翻訳能力を評価する多言語音声テキスト翻訳データセットです。</li>
</ul>
</div>
<div class="info-card glass-card">
<p class="keyword"><i class="far fa-smile-beam"></i>3) 感情認識</p>
<ul class="unstyled-list">
<li><code>MELD</code> (Poria et al., 2019) データセット: テレビドラマの複数の話者からの感情的な音声を使用して、音声中の感情を認識するモデルの能力を評価します。</li>
</ul>
</div>
</div>
<p>これらのデータセットは、コンテンツの正確性、多様な音声タスク、追加の音響情報などの要素を考慮して、モデルが音声を理解する能力を包括的に評価します。</p>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-volume-up"></i>オーディオ理解 (Audio Understanding)</p>
<p><strong class="highlight">環境音理解</strong>は、オーディオ理解のもう1つの重要な側面であり、人間の声以外の音声から情報を抽出および認識することを含みます。人間の音声と比較して、環境音はより複雑で多様な情報を提供します。主流の評価データセットは、主に2つの主要な領域でオーディオ理解を評価します。</p>
<div class="two-column">
<div class="column">
<div class="definition-box">
<p class="definition-title"><i class="fas fa-closed-captioning"></i>1) オーディオキャプション</p>
<ul class="unstyled-list">
<li><code>Clotho</code> (Drossos et al., 2020): 無料のサウンドプラットフォームからの音を含み、主にオーディオキャプションタスクに使用されます。</li>
<li><code>AudioCaps</code> (Kim et al., 2019): AudioSetデータセットから供給され、オーディオキャプションにも焦点を当て、より広範な適用範囲を持っています。</li>
</ul>
</div>
</div>
<div class="column">
<div class="definition-box">
<p class="definition-title"><i class="fas fa-question-circle"></i>2) オーディオ質問応答 (AQA)</p>
<ul class="unstyled-list">
<li><code>ClothoAQA</code> (Lipping et al., 2022): AQAタスク用に設計されたクラウドソーシングデータセットです。</li>
<li><code>AQUALLM</code> (Behera et al., 2023): LLMに基づく自動オーディオQA生成フレームワークによって構築されます。</li>
</ul>
</div>
</div>
</div>
<p>これらのベンチマークには、質問と回答がペアになった様々なオーディオタイプが含まれており、モデルがオーディオコンテンツを理解し、オーディオ関連の質問に正確な応答を生成することを学習するのに役立ちます。</p>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-music"></i>音楽理解 (Music Understanding)</p>
<p>音楽は、その構造的特徴と複雑なバリエーションにより、オーディオ理解における重要な研究分野となっています。音楽評価では、主に2つの方向性が考慮されます。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-drum"></i>音楽理論要素の認識</p>
<p><code>MusicNet</code> (Thickstun et al., 2017) や <code>NSynth</code> (Engel et al., 2017) のような主流のデータセットは、オーディオ中の楽器、音符、音高、リズムなどの音楽理論要素を認識するモデルの能力を評価します。</p>
</div>
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-file-audio"></i>音楽トラック全体のキャプション</p>
<p>さらに、<code>MusicCaps</code> (Agostinelli et al., 2023) や <code>MusicBench</code> (Melechovský et al., 2024) は、音楽トラック全体のキャプションに使用され、モデルが音楽作品の詳細な内容と全体的な構造の両方を理解する能力をテストします。</p>
</div>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-cogs"></i>包括的ベンチマーク (Comprehensive Benchmarks)</p>
<p>大規模オーディオ言語モデル（LALM）が進化し続けるにつれて、より多くのモデルが音声と多様な音の両方を理解する能力を持つようになりました。その結果、研究者はモデルのオーディオ理解能力を包括的に評価するための新しい評価ベンチマークを提案しています。</p>
<ul class="unstyled-list">
<li class="bubble-box"><span class="badge orange">VoiceBench</span> (Chen et al., 2024h): 様々な文脈での音声理解能力に焦点を当て、基本能力、口語表現、騒がしい環境でのパフォーマンスの評価を含む。</li>
<li class="bubble-box"><span class="badge orange">AudioBench</span> (Wang et al., 2024a): 多様な音声タスク（例：自動音声認識、音声質問応答）、サウンドタスク（例：オーディオキャプション、オーディオ質問応答）、および人間の声に関連するタスク（例：アクセント、年齢、性別）を統合。</li>
<li class="bubble-box"><span class="badge orange">Air-Bench</span> (Yang et al., 2024d), <span class="badge orange">MMAU</span> (Sakshi et al., 2024): 音楽タスクを評価に含めることでこれを拡張。</li>
<li class="bubble-box"><span class="badge orange">SD-eval</span> (Ao et al., 2024): 音声タスクと環境音タスクを組み合わせ、モデルが複雑な混合オーディオシナリオを理解できるようにする。</li>
</ul>
<p>これらのベンチマークは、以前の評価方法を取り入れるだけでなく、幅広い現実世界のアプリケーションにわたる音声理解を評価するためのより包括的なフレームワークを提供します。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-bullseye"></i>まとめ: 聴覚中心の理解</p>
<p>聴覚中心の理解は、モデルがオーディオデータを処理し理解する能力を評価するための包括的なフレームワークを提供します。音声認識から環境音、音楽解釈までのタスクを網羅しています。これらの評価は、モデルの汎用性と現実世界のアプリケーションでの有効性を保証し、複雑なオーディオデータを処理する能力を向上させるために不可欠です。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-pencil-ruler"></i>5.2 Multimodal Generation</h3>
<p class="content-box">
<span class="keyword">マルチモーダル生成</span>は、マルチモーダル推論モデルの重要な能力であり、テキスト、画像、オーディオ、ビデオなどの<strong class="highlight">異なるデータタイプにわたる新しいコンテンツの作成</strong>を包含します。この生成能力は、創造的なアプリケーションだけでなく、モデルが理解や推論の結果をマルチモーダル形式で伝える必要があるタスクにとっても重要です。
    </p>
<p class="content-box">これらのタスクは、モダリティ間で情報がどのように流れるか、および生成される出力の性質に基づいて、大きく2つのカテゴリに分類できます。</p>
<div class="info-grid">
<div class="info-card glass-card">
<h4 class="section-title mini" style="font-size: 16px;"><i class="fas fa-exchange-alt"></i>(1) Cross-modal Generation (クロスモーダル生成)</h4>
<p>あるモダリティの入力に基づいて、別のモダリティのコンテンツを生成するモデルの能力を評価します。</p>
</div>
<div class="info-card glass-card">
<h4 class="section-title mini" style="font-size: 16px;"><i class="fas fa-cubes"></i>(2) Joint Multimodal Generation (共同マルチモーダル生成)</h4>
<p>複数のモダリティにわたって同時にコンテンツを生成するモデルの能力を評価します。</p>
</div>
</div>
<h4 class="subsection-title"><i class="fas fa-arrows-alt-h"></i>5.2.1 Cross-modal Generation</h4>
<p class="content-box">
<span class="keyword">クロスモーダル生成</span>には、モデルがあるモダリティの入力に基づいて別のモダリティのコンテンツを生成するタスクが含まれます。これには、テキストから画像へ、テキストから動画へ、テキストから音声への生成などのタスクが含まれ、モデルはあるタイプの入力（例：テキスト）を別の形式（例：画像、動画、音声）に効果的にマッピングする必要があります。これらのタスクは、情報をあるモダリティから別のモダリティに変換し整列させるという課題をモデルに提示し、しばしば複雑なまたは条件付きのプロンプトの処理を必要とします。このセクションでは、整列、一貫性、意味的生成に焦点を当て、様々なクロスモーダルタスクにおけるモデルのパフォーマンスを評価するためにデータセットとベンチマークがどのように開発されてきたかを探ります。
    </p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-image"></i>テキストから画像へ (Text to Image)</p>
<p>テキストから画像への生成（T2I）の分野は、テキストから画像への生成、編集、条件付き生成などのタスクに合わせた多様なデータセットとベンチマークによって、大きな進歩を遂げています。</p>
<div class="glass-card" style="margin-bottom: 15px; padding:15px;">
<p class="keyword"><i class="fas fa-paint-brush"></i>テキストから画像への生成:</p>
<ul class="unstyled-list">
<li>データセット:
                    <ul>
<li><span class="badge blue">MSCOCO (30K)</span> (Lin et al., 2014a), <span class="badge blue">CC12M</span> (Changpinyo et al., 2021), <span class="badge blue">Flickr30k</span> (Plummer et al., 2017): 大規模で汎用的な画像-テキストペアを提供し、日常のシーンやオブジェクトを強調します。</li>
<li><span class="badge purple">RedCaps</span> (Desai et al., 2021), <span class="badge purple">COMMONPOOL</span> (Gadre et al., 2023): より複雑なテキスト記述と高解像度の画像を導入します。</li>
</ul>
</li>
<li>ベンチマーク:
                    <ul>
<li><span class="badge green">GenEval</span> (Ghosh et al., 2023), <span class="badge green">ELLA</span> (Hu et al., 2024a): テキストと画像の整列を評価し、生成された画像がテキスト記述とどれだけ正確に一致するかを評価します。</li>
<li><span class="badge orange">GenAI-Bench</span> (Li et al., 2024a), <span class="badge orange">T2I-CompBench+</span> (Huang et al., 2023a): 複雑なプロンプトとオブジェクトの相互作用の処理を強調し、効果的な構成的生成と改善された意味的整列の必要性を強調します。</li>
</ul>
</li>
</ul>
</div>
<div class="glass-card" style="margin-bottom: 15px; padding:15px;">
<p class="keyword"><i class="fas fa-edit"></i>テキストから画像への編集:</p>
<ul class="unstyled-list">
<li>データセット:
                    <ul>
<li><span class="badge blue">MagicBrush</span> (Zhang et al., 2023d), <span class="badge blue">InstructPix2Pix</span> (Brooks et al., 2023), <span class="badge blue">HQ-Edit</span> (Hui et al., 2024): 指示ベースの編集に焦点を当て、HQ-Editは高解像度画像へのタスクを拡張します。</li>
<li><span class="badge purple">UltraEdit</span> (Zhao et al., 2024a), <span class="badge purple">SEED-Data-Edit</span> (Ge et al., 2024): マルチターン編集タスクを導入し、マルチターン対話における大規模言語モデル（LLM）のトレーニングを改善します。</li>
</ul>
</li>
<li>評価: これらのデータセットは画像編集の様々な要求を評価し、MagicBrushは創造的側面を評価し、<span class="badge green">Emu Edit</span> (Sheynin et al., 2023) は高品質な編集における精度と一貫性に焦点を当てます。</li>
</ul>
</div>
<div class="glass-card" style="padding:15px;">
<p class="keyword"><i class="fas fa-tasks"></i>条件付きテキストから画像への生成:</p>
<ul class="unstyled-list">
<li>データセット:
                    <ul>
<li><span class="badge blue">ADE20K</span> (Zhou et al., 2016), <span class="badge blue">CocoStuff</span> (Caesar et al., 2016): 詳細なセグメンテーションマップとシーン解析注釈を提供し、モデルが特定のシーン構造を持つ画像を生成できるようにします。</li>
<li><span class="badge purple">UniControl</span> (Qin et al., 2023): より包括的なデータを導入し、モデルが複数の条件付き入力を同時に処理することを要求します。</li>
</ul>
</li>
<li>ベンチマーク:
                    <ul>
<li><span class="badge green">UniCombine</span> (Wang et al., 2025c): 指示実行の完全性、視覚的一貫性、制約との整合性を評価することに焦点を当てます。</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-film"></i>テキストから動画へ (Text to Video)</p>
<p>テキストから動画への生成において、高品質なデータセットと包括的なベンチマークは研究を進める上で不可欠です。</p>
<div class="two-column">
<div class="column">
<div class="definition-box">
<p class="definition-title">データセット <i class="fas fa-database"></i></p>
<p><code>VidGen-1M</code> (Tan et al., 2024b), <code>OpenVid-1M</code> (Nan et al., 2024), <code>VidProM</code> (Wang &amp; Yang, 2024) などは、広範囲の動画コンテンツと対応する記述テキストをカバーしています。</p>
</div>
</div>
<div class="column">
<div class="definition-box">
<p class="definition-title">ベンチマークツール <i class="fas fa-tools"></i></p>
<p><code>AIGCBench</code> (Fan et al., 2019), <code>EvalCrafter</code> (Liu et al., 2024e), <code>VBench</code> (Huang et al., 2024a) などは、関連性、一貫性、視覚的品質などの様々な指標でモデルを評価します。</p>
</div>
</div>
</div>
<p>専門ベンチマーク:</p>
<ul class="unstyled-list">
<li><span class="badge purple">VideoScore</span> (He et al., 2024), <span class="badge purple">WorldSimBench</span> (Qin et al., 2024), <span class="badge purple">WorldScore</span> (Duan et al., 2025): 動画品質と現実世界の正確性をカバーするように評価を拡大し、VideoScoreはユーザー満足度を評価します。</li>
</ul>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-microphone-alt"></i>テキストから音声へ (Text to Speech)</p>
<p>テキストから音声への（TTS）生成は、大規模オーディオ言語モデル（LALM）の開発を可能にする高品質なデータセットとベンチマークの恩恵を受けてきました。</p>
<div class="pipeline">
<div class="pipeline-step glass-card">
<p class="keyword">初期モデル <i class="fas fa-history"></i></p>
<p><code>LlaMA-Questions</code> (Nachmani et al., 2024), <code>Web Questions</code> (Berant et al., 2013), <code>Trivia QA</code> (Joshi et al., 2017) などの合成データセットを使用して音声対話能力を評価しました。評価は、テキストとオーディオ出力間の単語誤り率と精度の比較に基づいていました。</p>
</div>
<div class="pipeline-step glass-card">
<p class="keyword">最近のベンチマーク <i class="fas fa-lightbulb"></i></p>
<p><code>ADU-Bench</code> (Gao et al., 2024b) は、通常、専門的、多言語、曖昧なシナリオにわたる音声対話能力を評価し、<code>URO-Bench</code> (Yan et al., 2025b) はイントネーションや感情などの音声スタイルの評価を含みます。</p>
</div>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-robot"></i>ロボティクス (Robotics)</p>
<p>ロボティクスでは、データセットとベンチマークがモデルのパフォーマンスを評価するための忠実度の高いマルチモーダル環境を提供します。</p>
<div class="two-column">
<div class="column">
<div class="definition-box">
<p class="definition-title">データセット <i class="fas fa-cogs"></i></p>
<p><code>ThreeDWorld</code> (Gan et al., 2021) や <code>GAIA-1</code> (Hu et al., 2023) などは、自動運転のようなロボティクスタスクのためのインタラクティブなシミュレーションプラットフォームを提供します。</p>
</div>
</div>
<div class="column">
<div class="definition-box">
<p class="definition-title">ベンチマーク <i class="fas fa-award"></i></p>
<p><code>Genesis</code> (Engelcke et al., 2019) は、幅広いロボティクスタスクにわたってモデルを評価するための標準化された評価フレームワークを提供し、現実世界の適用可能性を保証します。</p>
</div>
</div>
</div>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-bullseye"></i>まとめ: クロスモーダル生成</p>
<p>クロスモーダル生成は、テキストから画像へ、テキストから動画へ、テキストから音声への生成などのタスクに焦点を当てた、マルチモーダルAIの重要な分野です。これらのタスクは、モダリティ間で情報を変換し整列させるという課題をモデルに提示します。進歩が続くにつれて、複雑なプロンプトの処理、多段階推論、意味的整列の改善に焦点が当てられ、モデルはモダリティ間でますます高度な変換と相互作用を実行できるようになると期待されています。</p>
</div>
<h4 class="subsection-title"><i class="fas fa-layer-group"></i>5.2.2 Joint Multimodal Generation</h4>
<p class="content-box">
<span class="keyword">共同マルチモーダル生成</span>とは、テキストと画像の両方を生成したり、テキスト、オーディオ、ビデオをまとまりのある出力に組み合わせたりするなど、<strong class="highlight">複数のモダリティにわたって同時にコンテンツを作成する</strong>ことを指します。これは、モデルが生成されたモダリティ間の一貫性と整列を保証する必要があるため、さらなる複雑さをもたらします。テキストからインターリーブされた画像-テキストやテキストからマルチモーダル出力へのタスクはこれを例示しており、モデルが物語のより広範な文脈を補完し適合するコンテンツを生成することを要求します。これらのタスクをサポートするために専門のデータセットとベンチマークが開発され、文脈に関連するマルチモーダル出力を作成するためのモデル訓練のための豊富な環境を提供しています。
    </p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-align-left"></i>テキストからインターリーブされた画像-テキストへ (Text to Interleaved Image-Text)</p>
<p>マルチモーダル大規模言語モデル（MLLM）の開発は、インターリーブされた画像-テキスト生成を大幅に進歩させ、<code>MM-Interleaved</code> (Tian et al., 2024) や <code>ANOLE</code> (Chern et al., 2024) のようなデータセットは、高品質な注釈付き画像-テキストペアによるモデル訓練をサポートしています。これらのデータセットは、モデルが文脈に関連し視覚的に一貫したコンテンツを生成する必要性を強調しています。</p>
<p>ベンチマーク:</p>
<ul class="unstyled-list">
<li><span class="badge green">InterleavedEval</span> (Liu et al., 2024c), <span class="badge green">OpenLEAF</span> (An et al., 2024): 一貫性のある整列した画像-テキストペアを生成するモデルの能力を評価することに焦点を当てています。</li>
<li><span class="badge orange">OpenING</span> (Zhou et al., 2024a): インターリーブされた画像-テキスト生成を評価するためのより多様なタスクセットを提供します。</li>
</ul>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-photo-video"></i>テキストからマルチモーダル出力へ (Text to Multimodal Output)</p>
<p>テキストからマルチモーダル出力への最近の開発は、クロスモーダルデータと共同マルチモーダルデータを組み合わせることにより、マルチモーダル生成を強化することに焦点を当てています。</p>
<p>モデル例:</p>
<ul class="unstyled-list">
<li><code>NextGPT</code> (Wu et al., 2024a), <code>DreamFactory</code> (Xie et al., 2024b): 訓練不要のアプローチを活用してテキストをマルチモーダルな物語に変換し、Vbenchのような動画評価ベンチマークを統合しています。</li>
<li><code>EVA</code> (Chi et al., 2024): テキスト入力に基づいてビデオシーケンス内のイベントをシミュレートし予測するために、実体化されたワールドモデルを組み込んでいます。</li>
</ul>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-bullseye"></i>まとめ: 共同マルチモーダル生成</p>
<p>共同マルチモーダル生成は、複数のモダリティにわたるコンテンツの同時作成を含み、モデルがそれらの間の一貫性と整列を維持することを要求します。研究が進むにつれて、将来の開発は、モーダル間の一貫性、適応性、シームレスな生成の改善に焦点を当てる可能性が高く、動的で多次元的なコンテンツ作成とインタラクティブなユーザーエクスペリエンスの新たな可能性を開きます。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-brain"></i>5.3 Multimodal Reasoning</h3>
<p class="content-box">
<span class="keyword">マルチモーダル推論</span>は、単純な理解や生成を超えて、モデルが<strong class="highlight">複数のモダリティからの情報を統合する</strong>ことを要求します。これにより、異なる種類のデータ間の関係をより深く理解する必要がある推論を行い、問題を解決し、複雑な質問に答えることができます。
    </p>
<p class="content-box">マルチモーダル推論モデルは、大きく2つの主要カテゴリに分類できます。</p>
<div class="info-grid">
<div class="info-card glass-card">
<h4 class="section-title mini" style="font-size: 16px;"><i class="fas fa-eye"></i>(1) General Visual Reasoning (一般視覚推論)</h4>
<p>モデルが視覚コンテンツを理解し、一般的な知識、論理、常識を適用してタスクを解決する能力を評価します。</p>
</div>
<div class="info-card glass-card">
<h4 class="section-title mini" style="font-size: 16px;"><i class="fas fa-microscope"></i>(2) Domain-specific Reasoning (ドメイン固有推論)</h4>
<p>視覚入力に基づく数学的問題解決など、特定の、しばしばより技術的な推論能力を評価します。</p>
</div>
</div>
<h4 class="subsection-title"><i class="far fa-lightbulb"></i>5.3.1 General Visual Reasoning</h4>
<p class="content-box">
<span class="keyword">一般視覚推論</span>は、マルチモーダル推論モデルにおける最も重要な能力の1つです。モデルが視覚情報を知覚するだけでなく、広範な知識、論理的演繹、常識を用いて、様々なシナリオでそれを理解、分析、推論することを要求します。
    </p>
<p class="content-box">この能力を厳密に評価するために、それぞれが視覚推論の異なる側面を対象とする幅広いベンチマークが開発されてきました。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-question-circle"></i>単純な質問応答タスク (例: VQA) を超えて:</p>
<ul class="unstyled-list">
<li><span class="badge blue">VCR (Visual Commonsense Reasoning)</span> (Zellers et al., 2019): 視覚的常識推論ベンチマーク。</li>
<li><span class="badge blue">PhysBench</span> (Chow et al., 2025): 物理的推論のための専門データセット。</li>
<li><span class="badge blue">VideoPhy</span> (Bansal et al., 2024): 動画における物理的常識の理解。</li>
</ul>
<p>これらはモデルに、視覚的状況を解釈するために日常的な知識を適用することを要求します。</p>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-brain"></i>マルチモーダル一般知能ベンチマーク</p>
<p>より広範なAI能力への野心は、マルチモーダル一般知能ベンチマークに反映されています。これらには、以下のような包括的な評価が含まれます。</p>
<div class="tag-list">
<span class="tag">MMBench (Liu et al., 2024f) (多言語側面をカバー)</span>
<span class="tag">MMMU (Yue et al., 2024) (多様な分野を網羅)</span>
<span class="tag">AGIEval (Zhong et al., 2024b) (人間中心の評価に焦点)</span>
<span class="tag">VideoVista (Li et al., 2024k)</span>
<span class="tag">MMStar (Chen et al., 2024f) (動画中心)</span>
</div>
<p>これらのベンチマークは、他のモダリティやタスクとともに、視覚推論を重要な構成要素として組み込んでいます。</p>
<p>さらに、図や構造化された視覚情報上の視覚推論も重要であり、<code>AI2D</code> (Kembhavi et al., 2016) や <code>InfographicVQA</code> (Mathew et al., 2022) のようなベンチマークは、モデルに空間レイアウトの解釈、関係性の理解、図、チャート、インフォグラフィックからの情報抽出を要求します。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-database"></i>訓練および評価用データセット</p>
<p>これらのベンチマークにおける重要な要素は、モデルの訓練と評価に使用されるデータセットです。いくつかのデータセットが、特定の推論能力を育成するために設計されています。</p>
<ul class="unstyled-list">
<li><code>SWAG</code> (Zellers et al., 2018): 視覚シーンにおける行動のありそうな継続を予測するようにモデルを訓練。</li>
<li><code>LLava-CoT dataset</code> (Xu et al., 2024b): 大規模言語モデルを統合することにより、モデルが視覚的常識タスクについて推論できるようにする。</li>
<li><code>CLEVR</code> (Johnson et al., 2016): 日常的な物体の合成画像に対する複雑な推論を実行することをモデルに要求。</li>
<li><code>Mulberry-260K</code> (Yao et al., 2024a), <code>ShareGPT4oReasoning</code> (Zhang et al., 2024e): それぞれ視覚的常識推論とマルチモーダル対話のためにモデルをさらに訓練。</li>
<li><code>Video-R1-data</code> (Feng et al., 2025b): 動画シーケンス内の動的な視覚コンテンツに関する推論のためにモデルを訓練するのに役立つ。</li>
<li><code>Visual-CoT</code> (Shao et al., 2024): 様々なタスクにわたる視覚理解と推論の両方を必要とするモデルの訓練をサポート。</li>
</ul>
<p>この動的で絶えず進化するベンチマークとデータセットの状況は、マルチモーダル推論モデルを進歩させるために不可欠です。</p>
</div>
<h4 class="subsection-title"><i class="fas fas-flask"></i>5.3.2 Domain-specific Reasoning</h4>
<p class="content-box">
<span class="keyword">ドメイン固有推論ベンチマーク</span>は、特定の分野におけるマルチモーダルモデルの専門的な推論能力を評価する上で重要な役割を果たします。
    </p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-calculator"></i>数学的推論</p>
<p><code>MathVista</code> (Lu et al., 2024) や <code>MATH-Vision</code> (Wang et al., 2024c) のようなデータセットは、視覚的文脈で数学的問題を解決するモデルの能力を評価し、視覚理解と数学的推論の両方を必要とします。同様に、<code>ChartQA</code> (Masry et al., 2022) や <code>ScienceQA</code> (Lu et al., 2022) のようなベンチマークは、特定ドメインでの推論に焦点を当てています。</p>
</div>
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-robot"></i>ロボティクス</p>
<p>いくつかのベンチマークは、推論に強い重点を置いた実体化AIの様々な側面を評価します。</p>
<ul class="unstyled-list">
<li>シミュレーション環境: <code>Habitat</code> (Savva et al., 2019), <code>AI2-THOR</code> (Kolve et al., 2017), <code>iGibson</code> (Li et al., 2021a) は、エージェントに複雑な3D設定でのナビゲーション、相互作用、空間理解に関する推論を要求します。</li>
<li>操作タスク: <code>Isaac Lab</code> (Mittal et al., 2023), <code>ProcTHOR</code> (Deitke et al., 2022) は、多様な環境での操作タスクのための推論に焦点を当てています。</li>
<li>その他: <code>WebArena</code> (Zhou et al., 2024c) はWebコンテンツに関する推論をテストし、<code>CALVIN</code> (Mees et al., 2022) のようなベンチマークは言語誘導推論を評価します。</li>
</ul>
</div>
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-atom"></i>物理的推論</p>
<p><code>PhysBench</code> (Chow et al., 2025), <code>VideoPhy</code> (Bansal et al., 2024), <code>CRAVE</code> (Sun et al., 2025) のようなデータセットは、視覚およびビデオ文脈にわたる物理法則と常識のモデルの理解を評価します。</p>
</div>
<div class="info-card glass-card">
<p class="keyword"><i class="fas fa-globe"></i>ワールドモデル</p>
<p>最後に、<code>GAIA-1</code> (Hu et al., 2023) や <code>RoboGen</code> (Wang et al., 2024l) のようなベンチマークは、モデルが現実世界のダイナミクスと相互作用をどの程度シミュレートし推論できるかを評価することにより、ワールドモデルの開発をサポートします。</p>
</div>
</div>
<p class="content-box">これらのドメイン固有ベンチマークは、専門分野におけるマルチモーダル推論の限界を押し広げ、特定のアプリケーションのためのより有能で知的なマルチモーダル推論モデルの開発を可能にするために不可欠です。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-bullseye"></i>まとめ: マルチモーダル推論</p>
<p>マルチモーダル推論は、複雑なタスクを解決するために、テキスト、画像、ビデオなどの複数のモダリティにわたる情報を統合し推論することをモデルに要求するAIの重要な分野です。それは、視覚コンテンツに論理と常識を適用する<strong class="highlight">一般視覚推論</strong>と、数学、ロボティクス、物理法則などの分野で専門的な推論能力を評価する<strong class="highlight">ドメイン固有推論</strong>に分けられます。これらのタスクは、マルチモーダル推論モデルを継続的に進化させ、人間レベルの推論に近づけるように促します。分野が進歩するにつれて、マルチモーダル推論の未来は、多様なタスクと現実世界のシナリオにわたって一般化できる、より統合されたシステムの作成に焦点を当て、より適応性があり、知的で、汎用性の高いAIソリューションを可能にするでしょう。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-tasks"></i>5.4 Multimodal Planning</h3>
<p class="content-box">
<span class="keyword">マルチモーダル計画ベンチマーク</span>は、エージェントが視覚、テキスト、インタラクティブデータなどの<strong class="highlight">多様な入力を統合・処理</strong>し、複雑な多段階タスクを実行する能力を評価するために不可欠です。これらのベンチマークは、Webナビゲーション、グラフィカルユーザーインターフェース（GUI）、実体化環境、オープンエンドシミュレーションなど、幅広い課題をカバーしています。計画、推論、適応性をテストすることにより、エージェントの能力の包括的な視点を提供します。これらのベンチマークを2つの主要な領域に分類し、それらの独自の貢献と革新性を強調します。
    </p>
<h4 class="subsection-title"><i class="fas fa-desktop"></i>5.4.1 GUI Navigation</h4>
<p class="content-box">
        GUIナビゲーションのベンチマークは、エージェントがデジタルインターフェース全体でタスクを計画および実行する能力を評価し、堅牢な<strong class="highlight">視覚言語グラウンディング</strong>と<strong class="highlight">多段階推論</strong>を必要とします。
    </p>
<div class="feature-card-grid">
<div class="feature-item glass-card">
<p class="keyword">WebArena (Zhou et al., 2024c), Mind2Web (Deng et al., 2023)</p>
<p>ナビゲーションと情報抽出のための現実的なWeb環境を提供し、Mind2Webは一般化可能性をテストするためにクロスウェブサイトタスクをさらに導入しています。</p>
</div>
<div class="feature-item glass-card">
<p class="keyword">VisualWebBench (Liu et al., 2024b)</p>
<p>クロスページ統合と要素ローカライズに焦点を当てた1.5Kタスクで、視覚集約的な計画を進歩させます。</p>
</div>
<div class="feature-item glass-card">
<p class="keyword">Windows Agent Arena (Bonatti et al., 2024)</p>
<p>デスクトップ環境でのクロスアプリケーション計画を評価します。</p>
</div>
<div class="feature-item glass-card">
<p class="keyword">Ferret-UI (You et al., 2024)</p>
<p>多段階指示を実行するためのグラウンディングされたUI理解に焦点を当てています。</p>
</div>
<div class="feature-item glass-card">
<p class="keyword">WebShop (Yao et al., 2022)</p>
<p>シミュレートされたeコマース環境で視覚言語グラウンディングをテストします。</p>
</div>
<div class="feature-item glass-card">
<p class="keyword">OSWorld (Xie et al., 2024a), OmniACT (Kapoor et al., 2024)</p>
<p>実際のデスクトップOS環境を提供し、ファイル操作やデータ処理などのクロスアプリケーションワークフローをサポートします。</p>
</div>
<div class="feature-item glass-card">
<p class="keyword">VisualAgentBench (Liu et al., 2024d)</p>
<p>GUI、実体化、視覚設計タスク全体で大規模マルチモーダルモデルを体系的に評価することにより、このパラダイムを拡張し、視覚的にリッチなデジタル環境での計画と行動のための統一されたベンチマークを確立します。</p>
</div>
<div class="feature-item glass-card">
<p class="keyword">LlamaTouch (Zhang et al., 2024d)</p>
<p>495タスクでモバイルUI自動化をスケーリングし、アプリナビゲーションなどの多段階操作をテストします。</p>
</div>
</div>
<h4 class="subsection-title"><i class="fas fa-vr-cardboard"></i>5.4.2 Embodied and Simulated Environments</h4>
<p class="content-box">
        実体化およびシミュレートされた環境は、エージェントが物理的または仮想世界に適応しなければならない、<strong class="highlight">動的でインタラクティブな設定での計画</strong>を強調します。
    </p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="keyword">MineDojo (Fan et al., 2022)</p>
<p>Minecraftでオープンエンドのベンチマークを提供し、リッチでインタラクティブな環境で多様なタスクにわたるジェネラリストエージェントの訓練と評価を可能にします。その柔軟性は、オブジェクトの相互作用、ナビゲーション、リソース管理のためのマルチモーダル計画をサポートします。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">MuEP (Li et al., 2024g)</p>
<p>シミュレートされた環境での経路計画などのタスクのための視覚言語入力による実体化計画に焦点を当てています。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">GVCCI (Kim et al., 2023)</p>
<p>言語誘導ロボット操作のための視覚グラウンディングを強化するために合成データを生成する生涯学習フレームワークを導入し、人間の監督なしで大幅なパフォーマンス向上を達成します。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">BEHAVIOR-1K (Li et al., 2024c)</p>
<p>1,000の家庭活動のデータセットを提供し、視覚、意味、行動データを統合することにより、ロボットが複雑なタスクを計画できるようにします。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">Habitat 3.0 (Puig et al., 2024)</p>
<p>シミュレートされた家庭での人間とロボットのコラボレーションを進歩させ、ナビゲーションと相互作用のためのマルチモーダル計画をサポートします。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">SAPIEN (Xiang et al., 2020)</p>
<p>部品ベースのオブジェクト操作のための忠実度の高い環境を提供し、ロボット計画の精度を向上させます。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">HomeRobot (Yenamandra et al., 2023), OpenVocabManip benchmark (Yenamandra et al., 2024)</p>
<p>オープンボキャブラリーモバイル操作を開拓し、一般化可能なタスクのために言語、知覚、行動を組み合わせます。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">HoloAssist (Wang et al., 2023b)</p>
<p>自己中心的な人間とロボットの相互作用をキャプチャし、現実世界の共同タスクの計画を容易にします。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">DrivingDojo (Rietsch et al., 2022)</p>
<p>ビデオとマルチエージェントデータを使用して、リアルタイムの運転シナリオでの動的な意思決定をテストします。</p>
</div>
<div class="info-card glass-card">
<p class="keyword">V-MAGE (Zheng et al., 2025d)</p>
<p>位置決め、軌道追跡、視覚記憶などのタスクでマルチモーダル大規模言語モデル（MLLM）を評価するためのゲームベースの評価フレームワークを提示し、計画能力を定量化するための新しいアプローチを提供します。</p>
</div>
</div>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i>課題と今後の展望</p>
<p>マルチモーダル計画ベンチマークは、Webナビゲーションから実体化環境まで、多様なタスクにわたるエージェントの評価において大きな進歩を遂げてきました。しかし、<strong class="highlight">長期的な計画</strong>、<strong class="highlight">ノイズの多い入力の処理</strong>、<strong class="highlight">現実世界への適応性</strong>などの課題が残っています。将来のベンチマークは、オープンワールド環境、リアルタイムの人間からのフィードバック、特にマルチエージェントまたは人間-AIシナリオにおける協調計画に焦点を当てるべきです。これらのギャップに対処することは、予測不可能な現実世界のタスクをより高い柔軟性と一般化で処理できるエージェントの開発を前進させるのに役立ちます。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-clipboard-check"></i>5.5 Evaluation Method</h3>
<p class="content-box">
        現在主流の評価方法には、<span class="keyword">完全/曖昧一致 (Exact/Fuzzy Match)</span>、<span class="keyword">選択肢照合 (Option Matching)</span>、<span class="keyword">LLM/MLLMスコアリング (LLM/MLLM Scoring)</span>、<span class="keyword">エージェント評価 (Agentic Evaluation)</span> があります。
    </p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-equals"></i>完全/曖昧一致 (Exact/Fuzzy Matching)</p>
<p>主に <code>VQAv2</code> (Antol et al., 2015) や <code>OKVQA</code> (Marino et al., 2019) などの一般的なオープンエンドVQAタスクで使用されます。</p>
<div class="pipeline">
<div class="pipeline-step glass-card">
<span class="step-number">1</span> これらの評価データセットは通常、複数の人間が注釈を付けた候補回答を提供します。
            </div>
<div class="pipeline-step glass-card">
<span class="step-number">2</span> ルールによって処理された予測回答は、候補回答と完全一致または曖昧一致で照合されます。
            </div>
<div class="pipeline-step glass-card">
<span class="step-number">3</span> 最終的な評価スコアは、特定のルールに基づいて計算されます。
                <ul class="unstyled-list">
<li>例1 (VQAv2): 1つの候補回答との一致は1/3点のみで、満点の1点を獲得するには3つの候補回答すべてとの一致が必要です。</li>
<li>例2 (DocVQA (Mathew et al., 2021)): 予測結果の精度を測定するためにレーベンシュタイン距離を使用します。</li>
</ul>
</div>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-check-double"></i>選択肢照合 (Options Matching)</p>
<p>回答の多様性により、完全一致および曖昧一致の方法ではすべての候補オプションを網羅できないことがよくあります。評価の公平性と正確性を確保するために、選択肢照合アプローチが導入されました。</p>
<div class="process-step">
<div class="step-number">1</div>
<div class="step-content">システムプロンプトにはいくつかの候補オプションが含まれます。</div>
</div>
<div class="process-step">
<div class="step-number">2</div>
<div class="step-content">モデルは最も適切なものを選択する必要があります。</div>
</div>
<div class="process-step">
<div class="step-number">3</div>
<div class="step-content">さらに、選択プロセス中にモデルが特定のオプションに対する嗜好を示す可能性を減らすために、<code>MMBench</code> (Liu et al., 2024f) のような研究では、評価における確率的な変動を最小限に抑えるために <strong class="highlight">CircularEval</strong> 方法論を採用しています。</div>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-star-half-alt"></i>LLM/MLLMスコアリング (LLM/MLLM Scoring)</p>
<p>選択肢の選択は公平性を保証しますが、オープンエンドの質問や現実世界のシナリオの性質からはかなり逸脱しています。その結果、LLMベースの評価方法がオープンエンドの質問の評価に導入されました (Fu et al., 2024b; Zhang et al., 2023f)。</p>
<div class="bubble-box">
<p>このアプローチでは、特定のプロンプト、質問、標準回答、およびモデルの予測をGPT-4oなどのLLMまたはMLLMに入力してスコアを生成します (Chen et al., 2024a; Xu et al., 2024d; Saad-Falcon et al., 2024)。プロンプトには通常、採点ガイドライン、参照例、およびモデルが公正でバランスの取れたスコアを提供するように誘導するために設計されたその他の情報が含まれます。</p>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-users-cog"></i>エージェント評価 (Agentic Evaluation)</p>
<p>評価プロセス中、単一モデルの能力は本質的に制限されており、多様なマルチモーダル情報を処理する際に欠点が生じる可能性があります。このため、エージェントベースのアプローチは、モデル自体の固有の制限を軽減するためにツールを活用できます。</p>
<ul class="unstyled-list">
<li class="bubble-box">📝 例: <code>CIGEval</code> (Wang et al., 2025f) は、多機能ツールボックスを統合することによりMLLMの視覚理解能力を拡張し、よりきめ細かい評価を可能にします。</li>
<li class="bubble-box">🤝 さらに、マルチエージェントディスカッションは、コンセンサスを促進し、より堅牢なソリューションを生み出すことにより、下流タスクで有効性を示しており (Xu et al., 2023b; Chen et al., 2024d; Xu et al., 2025f)、この利点は評価設定にも及びます。</li>
<li class="bubble-box">🔍 複数のエージェント間の協調的または敵対的な相互作用を活用して出力を評価する方法は、より信頼性が高く解釈可能な評価を示しています (Chan et al., 2024; Li et al., 2024h; Zhao et al., 2024b; Liang et al., 2024)。</li>
</ul>
</div>
</div>
<div class="section-card" id="6_Conclusion">
<h2 class="section-title"><i class="fas fa-flag-checkered" style="margin-right: 10px;"></i>6 Conclusion</h2>
<div class="content-box">
<p><span style="font-size: 1.2em; font-weight: bold; color: var(--color-primary);">📌 この論文の結論を一言で言うと…</span><br/>マルチモーダル推論モデルの進化をたどり、現在の言語中心の推論の限界を指摘し、<span class="keyword">真にマルチモーダルな大規模モデル (N-LMRMs)</span> への展望を示しました。これらは、多様なモーダル情報を統合し、より人間らしい文脈適応性と汎用的な問題解決能力を目指すものです。</p>
</div>
<div class="info-grid">
<div class="info-card">
<div class="framework-box">
<div class="framework-title"><i class="fas fa-lightbulb" style="color: var(--color-accent3);"></i> 論文のまとめ</div>
<p>この論文では、マルチモーダル推論モデルの進化の軌跡を詳細に調査し、以下の点を明らかにしました。</p>
<ul class="unstyled-list">
<li>✏️ <span class="highlight">現在のモデルの状況:</span> 主に言語を中心とした推論パラダイムを採用しており、<span class="keyword">視覚的質問応答 (VQA)</span>、<span class="keyword">視覚的数学問題解決</span>、<span class="keyword">ビデオ理解</span>といったタスクで目覚ましい成果を上げています。</li>
<li>⚠️ <span class="highlight">依然として残る課題:</span> しかし、いくつかの重要な課題が未解決のままです。
                        <ul class="unstyled-list" style="margin-left: 20px;">
<li><i class="fas fa-eye" style="color: var(--color-accent1); margin-right: 5px;"></i><strong>視覚中心の長期推論:</strong> 例えば、3D文脈の理解や複雑な視覚情報探索型の質問への対応など。</li>
<li><i class="fas fa-comments" style="color: var(--color-accent2); margin-right: 5px;"></i><strong>インタラクティブなマルチモーダル推論:</strong> 例えば、動的なクロスモーダル対話や反復的なフィードバックループなど。</li>
</ul>
                        これらの領域は、さらなる深い探求が求められています。
                    </li>
</ul>
</div>
</div>
<div class="info-card">
<div class="glass-card">
<h3 class="subsection-title" style="border-left-color: var(--color-accent1);"><i class="fas fa-cogs" style="color: var(--color-accent1);"></i> 将来の展望：N-LMRMs</h3>
<p>経験的な評価と実験的な洞察に基づき、私たちは言語支配的なアーキテクチャを超える、<span class="keyword">本質的にマルチモーダルな大規模モデル (N-LMRMs)</span> の概念を提案します。このようなモデルは、以下の3つの核となる能力を優先すべきです。</p>
<div class="feature-card-grid" style="grid-template-columns: 1fr;">
<div class="feature-item" style="background-color: rgba(74, 111, 165, 0.05);">
<div class="icon-item"><i class="fas fa-robot" style="font-size: 2em; color: var(--color-primary);"></i></div>
<h4 style="color: var(--color-primary); font-family: 'Yomogi';">1. マルチモーダルエージェント推論</h4>
<p style="font-size: 0.9em; text-align: left;">能動的な環境とのインタラクションを可能にします。例えば、実世界での試行錯誤を通じて学習する<span class="highlight">身体性AIエージェント</span>などです。</p>
</div>
<div class="feature-item" style="background-color: rgba(255, 126, 95, 0.05);">
<div class="icon-item"><i class="fas fa-brain" style="font-size: 2em; color: var(--color-secondary);"></i></div>
<h4 style="color: var(--color-secondary); font-family: 'Yomogi';">2. オムニモーダル理解と生成的推論</h4>
<p style="font-size: 0.9em; text-align: left;">あらゆるモーダルの意味論を統合します（例：視覚、聴覚、テキスト間で抽象的な概念を整合させる）。複雑でオープンな世界の文脈における曖昧さを解決しつつ、モーダル間で一貫性のある文脈認識型の出力を生成します（例：音声指示からの図生成、テキストからのビデオ物語合成）。</p>
</div>
</div>
</div>
</div>
</div>
<div class="note-box" style="background-color: rgba(92, 184, 92, 0.1); border-left-color: var(--color-accent1);">
<div class="note-title" style="color: var(--color-accent1);"><i class="fas fa-bullseye"></i> 目指すところ</div>
<p>これらの側面に取り組むことで、将来のモデルは<span class="keyword">人間のような文脈適応性</span>を達成し、個別のタスクパフォーマンスと<span class="highlight">汎用的で実世界の問題解決</span>との間のギャップを埋めることができる可能性があります。</p>
<div style="text-align: center; margin-top: 15px;">
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-accent1);">Isolated Tasks</span>
<i class="fas fa-arrow-right" style="color: var(--color-accent1); margin: 0 10px; font-size: 1.5em;"></i>
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-accent1); font-weight: bold;">Generalized Problem-Solving</span>
</div>
</div>
<div class="bubble-box" style="border-color: var(--color-accent2); margin-top: 30px;">
<h3 class="subsection-title" style="border-left: none; color: var(--color-accent2);"><i class="fas fa-paper-plane" style="color: var(--color-accent2);"></i> 今後の研究への期待</h3>
<p>このサーベイが、マルチモーダル推論モデルのさらなる発展、特に<span class="keyword">視覚中心の長期推論</span>や<span class="keyword">インタラクティブな推論</span>といった未開拓分野の研究を促進し、より高度なAIシステムの実現に貢献することを期待します。N-LMRMsの概念は、そのための重要な一歩となるでしょう。</p>
</div>
</div>
</div>
</body>
</html>
