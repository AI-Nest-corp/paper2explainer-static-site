<!DOCTYPE html>

<html lang="ja">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning解説</title>
<link href="style.css" rel="stylesheet"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\\\(', '\\\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]'], ['\\\\[', '\\\\]']]
          }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet"/>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N7SLXFTVBP"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'G-N7SLXFTVBP');
</script>

<body>
<div class="container">
<!-- ヘッダー部分 -->
<div class="header">
<div class="title-area">
<h1 class="title">VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</h1>
<p class="subtitle">None</p>
</div>
<div class="meta-info">
<p>論文解説</p>
</div>
</div>
<div class="section-card" id="Abstract">
<h2 class="section-title"><i class="fas fa-flask"></i>Abstract</h2>
<div class="content-box">
<p>このアブストラクトでは、近年の拡散モデルの目覚ましい進展を踏まえつつ、現在の画像生成における主流アプローチが抱える課題を指摘します。そして、これらの課題を解決するために提案された新しい普遍的画像生成フレームワーク「<span class="keyword">VisualCloze</span>」について、その核心的なアイデアと主な特徴を解説します。</p>
</div>
<div class="subsection-title"><i class="fas fa-microscope"></i>背景と既存の課題</div>
<div class="content-box">
<p><span class="highlight">拡散モデル（Diffusion Models）</span><i class="fas fa-atom"></i>の最近の進歩は、様々な画像生成タスクを大きく前進させました。これにより、非常にリアルで高品質な画像を生成することが可能になっています。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-book-open"></i>用語解説: 拡散モデル (Diffusion Models)</div>
<p>拡散モデルとは、データに徐々にノイズを加えていき（拡散過程）、その逆の過程（逆拡散過程）を学習することで、ノイズから新しいデータを生成するモデルの一種です。特に画像生成分野で高い性能を示しています。</p>
</div>
<p>しかし、現在の主流アプローチは、依然として<span class="keyword">タスク特化モデル（task-specific models）</span><i class="fas fa-puzzle-piece"></i>の構築に焦点が当てられています。これらのモデルは特定のタスク（例：顔画像の生成、風景画の生成など）には非常に優れていますが、</p>
<div style="text-align: center; margin: 15px 0;">
<span style="font-family: 'Yomogi', cursive; font-size: 16px; border: 2px dashed var(--color-secondary); padding: 5px 10px; border-radius: 8px;">
<i class="fas fa-cogs" style="color: var(--color-secondary);"></i> タスクAモデル <i class="fas fa-long-arrow-alt-right"></i> タスクA成果 <br/>
<i class="fas fa-paint-brush" style="color: var(--color-secondary);"></i> タスクBモデル <i class="fas fa-long-arrow-alt-right"></i> タスクB成果
            </span>
</div>
<p>広範囲の異なるニーズをサポートする際には<span class="highlight">効率が限られる</span><i class="fas fa-hourglass-half"></i>という問題があります。</p>
<p><span class="keyword">ユニバーサルモデル（universal models）</span><i class="fas fa-globe-americas"></i>は、この限界に対処しようとする試みです。つまり、一つのモデルで多様なタスクを処理することを目指しています。しかし、これらのユニバーサルモデルも、いくつかの重要な課題に直面しています。</p>
<div class="challenge-box" style="margin-top: 20px;">
<div class="challenge-title"><i class="fas fa-exclamation-triangle"></i>ユニバーサルモデルが直面する3つの主要な壁</div>
<ul class="unstyled-list">
<li><i class="fas fa-comments" style="color: var(--color-accent1);"></i> <strong>汎用的なタスク指示 (Generalizable task instruction):</strong> モデルに「何をしてほしいか」を曖昧さなく、かつ様々なタスクに応用できるように伝える方法。言語だけでは限界があることも。</li>
<li><i class="fas fa-sitemap" style="color: var(--color-accent2);"></i> <strong>適切なタスク分布 (Appropriate task distributions):</strong> 多種多様なタスクをモデルが効率よく学習し、それらの間で知識を転移できるようにするためのタスクの組み合わせや提示方法。</li>
<li><i class="fas fa-cogs" style="color: var(--color-accent3);"></i> <strong>統一されたアーキテクチャ設計 (Unified architectural design):</strong> 様々な種類の入力と出力を扱え、かつ効率的に学習・推論できる単一のモデル構造。</li>
</ul>
</div>
</div>
<div class="subsection-title"><i class="fas fa-lightbulb"></i>提案フレームワーク: VisualCloze</div>
<div class="framework-box">
<div class="framework-title"><i class="fas fa-magic"></i>VisualCloze: 普遍的画像生成フレームワーク</div>
<p>これらの課題に取り組むために、本論文では「<span class="keyword">VisualCloze</span>」という新しい普遍的な画像生成フレームワークを提案します。</p>
<p><span class="keyword">VisualCloze</span> は以下の能力をサポートします：</p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));">
<div class="feature-item" style="border-left: 3px solid var(--color-primary);">
<i class="fas fa-tasks fa-2x" style="color:var(--color-primary)"></i>
<p>広範な<br/><span class="highlight">ドメイン内タスク</span></p>
</div>
<div class="feature-item" style="border-left: 3px solid var(--color-secondary);">
<i class="fas fa-brain fa-2x" style="color:var(--color-secondary)"></i>
<p>未知タスクへの<br/><span class="highlight">汎化能力</span></p>
</div>
<div class="feature-item" style="border-left: 3px solid var(--color-accent1);">
<i class="fas fa-project-diagram fa-2x" style="color:var(--color-accent1)"></i>
<p>複数タスクの<br/><span class="highlight">未知の統合</span></p>
</div>
<div class="feature-item" style="border-left: 3px solid var(--color-accent2);">
<i class="fas fa-history fa-2x" style="color:var(--color-accent2)"></i>
<p><span class="highlight">逆生成</span><br/>(結果から条件を推定)</p>
</div>
</div>
</div>
<div class="subsection-title"><i class="fas fa-cogs"></i>VisualClozeの核心的アプローチ</div>
<div class="info-grid">
<div class="info-card glass-card">
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-primary);"><i class="fas fa-eye"></i> アプローチ1: 視覚的文脈内学習 (Visual In-Context Learning)</h4>
<p>既存の手法は、タスク指示に<span class="highlight">言語ベースの指示</span><i class="fas fa-comment-dots"></i>に依存していることが多く、これがタスクの曖昧さ<i class="fas fa-question-circle"></i>や汎化性能の低下<i class="fas fa-thumbs-down"></i>を引き起こす一因でした。</p>
<p><span class="keyword">VisualCloze</span>では、この問題を解決するために<span class="keyword">視覚的文脈内学習（visual in-context learning）</span><i class="fas fa-images"></i><i class="fas fa-chalkboard-teacher"></i>を統合します。これにより、モデルはいくつかの<span class="highlight">視覚的なデモンストレーション（お手本画像）</span>からタスクの内容を直接学習し、識別することができます。</p>
<div style="text-align: center; margin: 10px 0; padding: 10px; border: 1px dashed var(--color-primary); border-radius: 8px; background-color: rgba(74, 111, 165, 0.05);">
<p style="font-family: 'Yomogi', cursive; font-size: 14px; margin-bottom: 5px;">お手本 (Context Examples):</p>
<span style="font-size: 24px; margin: 0 5px;"><i class="fas fa-image"></i>➡️<i class="fas fa-magic"></i>➡️<i class="fas fa-image"></i></span>
<span style="font-size: 24px; margin: 0 5px;"><i class="fas fa-image"></i>➡️<i class="fas fa-magic"></i>➡️<i class="fas fa-image"></i></span>
<p style="font-family: 'Yomogi', cursive; font-size: 14px; margin-top: 10px; margin-bottom: 5px;">新しい入力 (Query):</p>
<span style="font-size: 24px; margin: 0 5px;"><i class="fas fa-image"></i>➡️<i class="fas fa-question"></i></span>
<p style="font-family: 'Yomogi', cursive; font-size: 14px; margin-top: 10px;">VisualClozeによる生成:</p>
<span style="font-size: 24px; margin: 0 5px;"><i class="fas fa-image"></i>➡️<i class="fas fa-lightbulb" style="color: orange;"></i>➡️<i class="fas fa-image"></i>✨</span>
</div>
<div class="definition-box" style="margin-top:15px;">
<div class="definition-title"><i class="fas fa-book"></i>定義: 視覚的文脈内学習 (Visual In-Context Learning)</div>
<p>少数の視覚的な「お手本」（コンテキスト例やデモンストレーション）をモデルに提示することで、実行すべきタスクをモデル自身に理解させ、新しい入力（クエリ）に対しても同様の処理（画像生成や変換）を適応させる学習パラダイムです。これにより、明示的な言語指示なしに、またはそれを補完する形でタスクを遂行できます。</p>
</div>
</div>
<div class="info-card glass-card">
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-secondary);"><i class="fas fa-database"></i> アプローチ2: Graph200K データセット</h4>
<p>従来の視覚タスクの学習では、各タスクのデータが独立していて関連性が薄い<span class="keyword">「タスク分布の固有の疎性（inherent sparsity）」</span><i class="fas fa-grip-lines"></i><i class="fas fa-unlink"></i>が問題でした。これにより、タスク間で共通する<span class="highlight">転移可能な知識</span>の学習が妨げられていました。</p>
<p>この課題に対処するため、<span class="keyword">VisualCloze</span>では「<span class="keyword">Graph200K</span>」<i class="fas fa-project-diagram"></i>という新しいデータセットを導入します。</p>
<div style="text-align: center; margin: 10px 0; padding: 10px; border: 1px dashed var(--color-secondary); border-radius: 8px; background-color: rgba(255, 126, 95, 0.05);">
<i class="fas fa-image fa-2x" style="color: var(--color-dark);"></i>
<div><i class="fas fa-arrow-down" style="color: var(--color-secondary); margin: 5px 0;"></i> <i class="fas fa-arrow-up" style="color: var(--color-secondary); margin: 5px 0;"></i></div>
<span style="font-family: 'Yomogi', cursive; border: 1px solid var(--color-gray); padding: 3px; border-radius: 4px; margin:2px;"><i class="fas fa-cogs"></i> 条件生成</span>
<span style="font-family: 'Yomogi', cursive; border: 1px solid var(--color-gray); padding: 3px; border-radius: 4px; margin:2px;"><i class="fas fa-tools"></i> 画像修復</span>
<span style="font-family: 'Yomogi', cursive; border: 1px solid var(--color-gray); padding: 3px; border-radius: 4px; margin:2px;"><i class="fas fa-edit"></i> 画像編集</span>
<span style="font-family: 'Yomogi', cursive; border: 1px solid var(--color-gray); padding: 3px; border-radius: 4px; margin:2px;"><i class="fas fa-palette"></i> スタイル変換</span>
<p style="font-family: 'Yomogi', cursive; font-size:12px; margin-top:5px;">(相互に関連付けられたタスク群)</p>
</div>
<p>これは、様々な<span class="highlight">相互に関連するタスク</span>を確立する<span class="keyword">グラフ構造のデータセット</span>であり、タスク間の密度を高め、転移可能な知識の学習を強化します。</p>
<div class="definition-box" style="margin-top:15px;">
<div class="definition-title"><i class="fas fa-book"></i>定義: タスク分布の疎性 (Sparsity of Visual Task Distributions)</div>
<p>画像生成における多種多様なタスク（例：特定の物体を描く、写真のスタイルを変える、ノイズを除去する等）の学習用データが、それぞれ独立して存在し、互いの関連性や共通性が乏しい状態を指します。これにより、あるタスクで学習した知識を他のタスクに応用することが難しくなります。</p>
</div>
<div class="definition-box" style="margin-top:15px;">
<div class="definition-title"><i class="fas fa-book"></i>定義: Graph200K</div>
<p>本研究で提案される、グラフ構造を持つ大規模なデータセットです。各画像データに対して、条件生成、画像修復、画像編集、IP保存（特定のアイデンティティや特徴の保持）、スタイル変換といった複数の「メタタスク」の注釈が関連付けられています。この構造により、タスク間の関連性を密にし、モデルがタスク横断的な知識（転移可能な知識）を効率的に学習することを目指しています。</p>
</div>
</div>
<div class="info-card glass-card">
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-accent1);"><i class="fas fa-puzzle-piece"></i> アプローチ3: 統一アーキテクチャと画像修復モデルの活用</h4>
<p>さらに、本研究では、提案する統一された画像生成の定式化（タスクの表現方法）が、既存の<span class="keyword">画像修復（image infilling）</span>タスク<i class="fas fa-fill-drip"></i>と<span class="highlight">一貫した目的を共有している</span><i class="fas fa-check-double"></i>ことを見出しました。</p>
<p>この発見により、VisualClozeは、事前学習済みの画像修復モデルが持つ強力な<span class="keyword">生成的プライヤー（generative priors）</span><i class="fas fa-brain"></i><i class="fas fa-magic"></i>を、<span class="highlight">アーキテクチャを変更することなく</span>そのまま活用できます。</p>
<div style="text-align: center; margin: 10px 0; padding: 10px; border: 1px dashed var(--color-accent1); border-radius: 8px; background-color: rgba(92, 184, 92, 0.05);">
<p style="font-family: 'Yomogi', cursive; font-size: 14px; margin-bottom: 5px;">VisualClozeのタスク:</p>
<span style="font-size: 18px; border: 1px solid #ccc; padding: 5px; display:inline-block; margin:2px;">条件画像1</span>
<span style="font-size: 18px; border: 1px solid #ccc; padding: 5px; display:inline-block; margin:2px;">条件画像2</span>
<span style="font-size: 18px; border: 2px dashed var(--color-accent1); padding: 5px; display:inline-block; margin:2px; background-color: #e8f5e9;"> ? (生成対象) </span>
<div style="font-size: 24px; margin: 5px 0;"><i class="fas fa-equals" style="color: var(--color-accent1);"></i></div>
<p style="font-family: 'Yomogi', cursive; font-size: 14px; margin-bottom: 5px;">画像修復タスク:</p>
<span style="font-size: 18px; border: 1px solid #ccc; padding: 5px; display:inline-block; margin:2px;">既知領域1</span>
<span style="font-size: 18px; border: 1px solid #ccc; padding: 5px; display:inline-block; margin:2px;">既知領域2</span>
<span style="font-size: 18px; border: 2px dashed var(--color-accent1); padding: 5px; display:inline-block; margin:2px; background-color: #e8f5e9;"> □ (修復対象) </span>
</div>
<div class="definition-box" style="margin-top:15px;">
<div class="definition-title"><i class="fas fa-book"></i>定義: 画像修復 (Image Infilling / Image Completion)</div>
<p>画像の一部分が欠損している（マスクされている）場合に、その欠損部分を周囲のコンテキストに基づいて自然に補完するタスクです。VisualClozeでは、この画像修復の枠組みを一般化し、様々な画像生成タスクに応用します。</p>
</div>
<div class="definition-box" style="margin-top:15px;">
<div class="definition-title"><i class="fas fa-book"></i>定義: 生成的プライヤー (Generative Priors)</div>
<p>モデルが学習過程で獲得した、データがどのように生成されるべきかに関する「事前知識」や「傾向」のことです。例えば、画像生成モデルであれば、自然な画像がどのような特徴（テクスチャ、構造、色分布など）を持つべきかという知識を指します。強力な生成的プライヤーを持つモデルは、より高品質で現実的な画像を生成する傾向があります。</p>
</div>
</div>
</div>
<div class="arrow-connector"></div>
<div class="bubble-box" style="border-color: var(--color-accent1); background-color: rgba(232, 245, 233, 0.5);">
<p style="font-family: 'Yomogi', cursive; text-align:center; font-size: 16px;">
<i class="fas fa-lightbulb" style="color: var(--color-accent1);"></i> <strong>結論として、VisualClozeは...</strong> <br/>
<span class="badge blue">視覚的文脈内学習</span> + <span class="badge purple">Graph200Kデータセット</span> + <span class="badge orange">画像修復モデルの活用</span> <br/>
<i class="fas fa-arrow-down" style="color: var(--color-dark); margin: 5px 0;"></i><br/>
            これにより、タスク指示の曖昧さを減らし、未知タスクへの汎化能力を高め、多様な画像生成タスクを統一的に扱うことを可能にするフレームワークです。
        </p>
</div>
</div>
<div class="section-card" id="1._Introduction">
<h2 class="section-title"><i class="fas fa-book-reader"></i> 1. Introduction</h2>
<div class="content-box">
<p>このセクションでは、論文の導入部分として、本研究の背景、目的、そして提案するフレームワークの概要について解説します。画像生成技術の現状の課題を明らかにし、それらを解決するためのアプローチとして<span class="keyword">VisualCloze</span>という新しいフレームワークを提案するに至った経緯を説明します。</p>
</div>
<div class="glass-card">
<h3 class="subsection-title"><i class="fas fa-rocket"></i> このセクションの主な目的と論旨</h3>
<p>本研究の導入部では、まず近年の画像生成技術の目覚ましい発展、特に<span class="keyword">拡散モデル (Diffusion Models)</span>の貢献について触れます。これらの技術は多岐にわたる応用を生み出していますが、既存の手法は<span class="highlight">特定のタスクごとに専用のモデルが必要</span>となるケースが多く、効率性や拡張性の面で限界があります。</p>
<p>そこで、多様な画像生成タスク、さらには<span class="highlight">学習データに含まれない未知のタスク</span>でさえも、<span class="keyword">単一の統一されたフレームワーク</span>で扱える「汎用生成モデル」への関心が高まっています。しかし、このような汎用モデルを実現するには、以下の3つの大きな課題が残されています：</p>
<ol class="fancy-list">
<li><i class="fas fa-tasks"></i> <strong>識別可能で汎用的なタスク指示方法</strong>：モデルに「何をすべきか」を明確かつ柔軟に伝える方法。</li>
<li><i class="fas fa-database"></i> <strong>訓練時の包括的なタスクカバレッジ</strong>：多様なタスクを学習するための、広範で密なデータ。</li>
<li><i class="fas fa-cogs"></i> <strong>統一されたモデルアーキテクチャ</strong>：様々なタスクに対応できる単一のモデル構造。</li>
</ol>
<p>本論文では、これらの課題に取り組むために<span class="keyword">VisualCloze</span>という新しいフレームワークを提案します。VisualClozeは、<span class="highlight">視覚的な文脈内学習 (Visual In-context Learning)</span>、<span class="highlight">グラフ構造化データセット (Graph200K)</span>、そして<span class="highlight">画像修復 (Image Infilling) モデルの活用</span>という3つの主要なアイデアを組み合わせています。</p>
</div>
<div class="note-box">
<h4 class="note-title"><i class="fas fa-microscope"></i> 用語解説：拡散モデル (Diffusion Models)</h4>
<p>拡散モデルとは、データに徐々にノイズを加えていき、その逆の過程（ノイズからデータを復元する過程）を学習することで、新しいデータを生成するモデルの一種です。特に画像生成の分野で非常に高品質な結果を出すことで知られ、近年の画像生成技術の発展を牽引しています [<a class="reference-link" href="#ref-15">15</a>, <a class="reference-link" href="#ref-33">33</a>, <a class="reference-link" href="#ref-88">88</a>]。</p>
<div style="text-align: center; margin: 15px 0;">
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-primary);">ノイズ除去プロセス</span><br/>
<i class="fas fa-image fa-2x" style="color: var(--color-gray);"></i>
<i class="fas fa-long-arrow-alt-right fa-2x" style="color: var(--color-accent1); margin: 0 10px;"></i>
<i class="fas fa-braille fa-2x" style="color: var(--color-gray);"></i>
<i class="fas fa-long-arrow-alt-right fa-2x" style="color: var(--color-accent1); margin: 0 10px;"></i>
<i class="fas fa-paint-brush fa-2x" style="color: var(--color-primary);"></i>
<br/>
<span style="font-family: 'Yomogi', cursive; font-size: 0.9em; color: var(--color-gray);">(元画像) <i class="fas fa-arrow-right"></i> (ノイズ付加) <i class="fas fa-arrow-right"></i> (ノイズ除去学習) <i class="fas fa-arrow-right"></i> (生成画像)</span>
</div>
</div>
<p>拡散モデルの進歩により、画像編集 [<a class="reference-link" href="#ref-69">69</a>]、スタイル変換 [<a class="reference-link" href="#ref-64">64</a>, <a class="reference-link" href="#ref-81">81</a>]、バーチャルトライオン [<a class="reference-link" href="#ref-11">11</a>, <a class="reference-link" href="#ref-12">12</a>]、個人化生成 [<a class="reference-link" href="#ref-38">38</a>, <a class="reference-link" href="#ref-54">54</a>] など、幅広い応用が可能になりました。しかし、これらのタスクは通常、<span class="keyword">タスク特化モデル</span>を必要とし、これが実世界のアプリケーションにおける効率性とスケーラビリティを制限しています。</p>
<p>近年、<span class="keyword">汎用生成モデル (Universal Generative Models)</span> [<a class="reference-link" href="#ref-27">27</a>, <a class="reference-link" href="#ref-39">39</a>, <a class="reference-link" href="#ref-44">44</a>] への関心が高まっており、これらは多様な画像生成タスク、さらには未知のタスクでさえも、単一の統一されたフレームワーク内で処理することを目指しています。大きな進歩があったものの、いくつかの重要な課題が残されています。具体的には、<span class="highlight">(1) 識別可能で汎用的なタスク指示</span>、<span class="highlight">(2) 訓練中の包括的なタスクカバレッジ</span>、そして<span class="highlight">(3) 統一されたモデルアーキテクチャ</span>です。</p>
<div class="challenge-box">
<h4 class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 課題1：識別可能で汎用的なタスク指示</h4>
<p>モデルが望ましいタスクを効果的に処理するためには、<span class="keyword">理想的なタスク指示</span>が不可欠です。既存の手法は主に、実行するタスクを区別するために<span class="highlight">言語指示</span> [<a class="reference-link" href="#ref-27">27</a>, <a class="reference-link" href="#ref-44">44</a>] や<span class="highlight">タスク特化トークン</span> [<a class="reference-link" href="#ref-39">39</a>] に依存しています。</p>
<p>しかし、視覚タスクの複雑さと、視覚と言語モダリティ間の固有のギャップにより、モデルが言語のみのタスク記述を理解することは困難です。これは<span class="keyword">タスク混同 (Task Confusion)</span> [<a class="reference-link" href="#ref-39">39</a>] を引き起こし、未知タスクへの汎化を妨げます [<a class="reference-link" href="#ref-35">35</a>, <a class="reference-link" href="#ref-71">71</a>]。さらに、事前に学習されたタスク特化トークンは、モデルを既知のタスクの処理のみに制約します。</p>
</div>
<div class="note-box">
<h4 class="note-title"><i class="fas fa-lightbulb"></i> 提案：視覚的In-context Learning</h4>
<p>これに対し、大規模言語モデル (LLMs) は、<span class="keyword">In-context Learning (文脈内学習)</span> [<a class="reference-link" href="#ref-5">5</a>] の台頭もあって、統一されたマルチタスクモデリングに成功しています。In-context Learningは、モデルが少数のデモンストレーション（文脈内事例）を用いるだけで様々なタスクに適応できるようにするものです。</p>
<p>本研究では、このIn-context Learningの概念を<span class="highlight">純粋な視覚モダリティ</span>で再現することを目指します。つまり、モデルがタスクデモンストレーションとして提示された<span class="highlight">少数の視覚的実例から直接望ましいタスクを学習する</span>ようにします（下図 Fig. 1 左上部参照）。</p>
<img alt="Figure 1: Visual In-Context Learningの概念図" src="visualcloze_framework_in_context_learning.jpg" style="display: block; margin: 20px auto; width: 60%;"/>
<p class="reference" style="text-align:center;">図1: VisualClozeフレームワークと視覚的In-context Learning。(左上) 汎用画像生成フレームワークが、特定のタスクのクエリに対し、デモンストレーションとして提示された少数の文脈内例を観察することでタスクを学習する様子。各タスクの生成結果は赤枠で示される。</p>
<p>この設定において、In-context Learningは汎用画像生成に対して強力な可能性を示します。我々はこのアプローチに関する4つの主要な発見をまとめました：</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card">
<p class="badge orange"><i class="fas fa-check-circle"></i> 発見1</p>
<p>タスクの曖昧さを低減し、様々な<span class="keyword">ドメイン内タスク</span>をサポートします (Fig. 1)。</p>
</div>
<div class="info-card">
<p class="badge blue"><i class="fas fa-eye"></i> 発見2</p>
<p><span class="keyword">未知のタスク</span>へも汎化します (Fig. 2, Fig. 8)。</p>
</div>
<div class="info-card">
<p class="badge purple"><i class="fas fa-project-diagram"></i> 発見3</p>
<p>タスク統合のための未知の戦略として、複数のサブタスクを単一ステップに統合し、中間結果を生成できます (Fig. 3)。</p>
</div>
<div class="info-card">
<p class="badge green"><i class="fas fa-undo-alt"></i> 発見4</p>
<p><span class="keyword">逆生成 (Reverse Generation)</span>、つまり与えられたターゲットから一連の条件を推論することを可能にします (Fig. 9)。</p>
</div>
</div>
<p>過去の研究 [<a class="reference-link" href="#ref-1">1</a>, <a class="reference-link" href="#ref-3">3</a>, <a class="reference-link" href="#ref-4">4</a>, <a class="reference-link" href="#ref-43">43</a>, <a class="reference-link" href="#ref-66">66</a>, <a class="reference-link" href="#ref-71">71</a>, <a class="reference-link" href="#ref-82">82</a>] も視覚におけるIn-context Learningを探求していますが、それらは主に密な予測 (dense prediction) やスタイル変換 [<a class="reference-link" href="#ref-67">67</a>, <a class="reference-link" href="#ref-87">87</a>] のような特定のドメインに制約されているか、または単一の条件と単一のターゲット画像のみを含む単純化された生成設定 [<a class="reference-link" href="#ref-43">43</a>, <a class="reference-link" href="#ref-60">60</a>] に限られています。</p>
</div>
<div class="definition-box">
<h4 class="definition-title"><i class="fas fa-book-open"></i> 用語解説：In-context Learning (文脈内学習)</h4>
<p>モデルが実行時に少数のタスク例（プロンプトやデモンストレーションと呼ばれる）を与えられることで、そのタスクのパターンを「文脈から」学習し、新しい入力に対して同様の処理を行う能力のことです。特に大規模言語モデル (LLM) でその有効性が示されています。本論文では、これを視覚情報に応用します。</p>
<div style="display: flex; align-items: center; justify-content: space-around; margin: 15px 0; font-family: 'Yomogi', cursive;">
<div>
<span class="badge yellow">例1: 入力A <i class="fas fa-arrow-right"></i> 出力A'</span><br/>
<span class="badge yellow">例2: 入力B <i class="fas fa-arrow-right"></i> 出力B'</span>
</div>
<i class="fas fa-chalkboard-teacher fa-2x" style="color: var(--color-accent2);"></i>
<div>
<span class="badge blue">新規入力C <i class="fas fa-question-circle"></i> ?</span><br/>
<span class="badge green"><i class="fas fa-arrow-right"></i> モデルが推論 <i class="fas fa-arrow-right"></i> 出力C'</span>
</div>
</div>
</div>
<img alt="Figure 2: 未知タスクへの汎化" src="unseen_tasks_generalizing_in_context_learning.jpg" style="display: block; margin: 20px auto; width: 60%;"/>
<p class="reference" style="text-align:center;">図2: 未知タスク <span style="font-family: 'Yomogi', cursive; color: var(--color-secondary);">♦</span> : In-context Learning を介して訓練中に見たことのないタスクに汎化する。より多くの文脈内例がより正確な結果につながる。</p>
<img alt="Figure 3: 未知タスクの統合" src="unseen_tasks_task_unification.jpg" style="display: block; margin: 20px auto; width: 60%;"/>
<p class="reference" style="text-align:center;">図3: 未知タスク <span style="font-family: 'Yomogi', cursive; color: var(--color-secondary);">♣</span> : In-context Learning を活用して、複数の既知タスクを単一ステップの未知タスクに統合する。左: [深度から画像へ] と [再照明] タスクを単一の [様々な照明を持つ深度から画像へ] タスクに統合。右: 複数の密予測タスクを共同予測タスクに統合。視覚的文脈なしの結果は付録参照。</p>
<div class="challenge-box">
<h4 class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 課題2：訓練時の包括的なタスクカバレッジ (タスク分布のスパース性)</h4>
<p>タスク分布の観点から見ると、視覚タスクは自然言語処理 (NLP) のタスクと比較して本質的に<span class="keyword">スパース (疎)</span> です。これは、異なるタスクのためのタスク特化データセット [<a class="reference-link" href="#ref-71">71</a>, <a class="reference-link" href="#ref-85">85</a>] が最小限の重複しか持たないためです [<a class="reference-link" href="#ref-19">19</a>, <a class="reference-link" href="#ref-32">32</a>, <a class="reference-link" href="#ref-79">79</a>]。</p>
<p>このようなスパースなタスク学習は、各タスクの知識を分離させ、モデルがタスク間で<span class="highlight">共有される特徴を学習することを制限</span>します。さらに、タスク間の相関が弱いことが、知識の転移や新しいタスクへの適応性を妨げます。しかし、マルチタスク学習における既存の研究 [<a class="reference-link" href="#ref-10">10</a>, <a class="reference-link" href="#ref-16">16</a>, <a class="reference-link" href="#ref-31">31</a>, <a class="reference-link" href="#ref-53">53</a>] は、関連タスク間で知識を重複させることの利点を検証しています。</p>
</div>
<div class="note-box">
<h4 class="note-title"><i class="fas fa-project-diagram"></i> 提案：Graph200K データセット</h4>
<p>視覚タスクのスパース性を緩和するために、我々は<span class="keyword">Graph200K</span>というグラフ構造のデータセットを導入します。このデータセットでは、各画像が5つの<span class="keyword">メタタスク (Meta-tasks)</span> にまたがるアノテーションと関連付けられています。具体的には、</p>
<ul class="unstyled-list" style="padding-left: 20px;">
<li><i class="fas fa-cogs" style="color: var(--color-accent1);"></i> 条件付き生成 (Conditional Generation) [<a class="reference-link" href="#ref-80">80</a>]</li>
<li><i class="fas fa-shield-alt" style="color: var(--color-accent2);"></i> IP保存 (IP Preservation) [<a class="reference-link" href="#ref-76">76</a>]</li>
<li><i class="fas fa-palette" style="color: var(--color-accent3);"></i> スタイル変換 (Style Transfer) [<a class="reference-link" href="#ref-81">81</a>]</li>
<li><i class="fas fa-edit" style="color: var(--color-secondary);"></i> 画像編集 (Image Editing) [<a class="reference-link" href="#ref-69">69</a>]</li>
<li><i class="fas fa-tools" style="color: var(--color-primary);"></i> 修復 (Restoration) [<a class="reference-link" href="#ref-77">77</a>]</li>
</ul>
<p>異なる条件を組み合わせることで、我々はモデルを互いに<span class="highlight">重複する多様なタスク</span>で訓練します。この非常に重複したコンパクトなタスク空間により、我々のデータセットはタスク密度を大幅に増加させ、モデルが<span class="highlight">共有可能で転移可能な知識</span>をより効果的に学習できるようにします。</p>
</div>
<div class="challenge-box">
<h4 class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 課題3：統一されたモデルアーキテクチャ</h4>
<p>アーキテクチャ設計においては、以下の2点が不可欠です：</p>
<ol>
<li><span class="highlight">柔軟なタスクフォーマットに対応</span>し [<a class="reference-link" href="#ref-27">27</a>, <a class="reference-link" href="#ref-35">35</a>, <a class="reference-link" href="#ref-71">71</a>]、シームレスなIn-context Learningを保証すること。</li>
<li>最先端のモデル [<a class="reference-link" href="#ref-33">33</a>, <a class="reference-link" href="#ref-88">88</a>] と<span class="highlight">互換性を保ち</span>、それらの強力な生成的プライヤー（事前知識）を完全に活用すること。</li>
</ol>
</div>
<div class="note-box">
<h4 class="note-title"><i class="fas fa-puzzle-piece"></i> 提案：画像修復モデルの活用</h4>
<p>本研究では、最先端の<span class="keyword">画像修復 (Image Infilling) モデル</span> [<a class="reference-link" href="#ref-33">33</a>] が、我々のIn-context Learningベースの汎用生成定式化と<span class="highlight">一貫した目的を持つ</span>ことを見出しました。</p>
<p>具体的には、すべての入力画像と出力画像を連結し、タスクの目的を<span class="highlight">出力領域を埋めること</span>とします。この整合性により、追加の変更なしに高度な汎用修復モデル上に我々のモデルを構築することが可能となり、最小限のデータと訓練コストで強力な汎用生成能力を達成します。</p>
<div style="text-align: center; margin: 15px 0;">
<i class="fas fa-images fa-2x" style="color: var(--color-gray);"></i> <span style="font-family: 'Yomogi', cursive; font-size: 1.5em; color: var(--color-accent1); margin: 0 10px;">+</span> <i class="far fa-square fa-2x" style="color: var(--color-gray);"></i>
<i class="fas fa-long-arrow-alt-right fa-2x" style="color: var(--color-accent1); margin: 0 10px;"></i>
<i class="fas fa-highlighter fa-2x" style="color: var(--color-primary);"></i>
<br/>
<span style="font-family: 'Yomogi', cursive; font-size: 0.9em; color: var(--color-gray);">(入力画像群) <span style="font-family: 'Yomogi', cursive;">+</span> (空の出力領域) <span style="font-family: 'Yomogi', cursive;"> <i class="fas fa-arrow-right"></i> </span> (画像修復タスクとして解決)</span>
</div>
</div>
<div class="framework-box">
<h3 class="framework-title"><i class="fas fa-cogs"></i> VisualCloze フレームワークの概要</h3>
<p>本研究では、<span class="keyword">VisualCloze</span>という汎用画像生成フレームワークを提案します。これは、<span class="keyword">FLUX.1-Filldev</span> [<a class="reference-link" href="#ref-33">33</a>] をGraph200Kからサンプリングされた相互関連タスクでファインチューニングすることにより、転移可能な知識を学習し、視覚的In-context Learningをサポートします。</p>
<p>In-contextの例の数が増えるにつれて、パフォーマンスの向上とタスク混同の低減が観察され、モデルは条件付き生成、画像修復、編集、スタイル変換、IP保存、およびそれらの組み合わせを含む広範なドメイン内タスクをサポートできるようになります。</p>
<p>未知のタスクにおいても、モデルはある程度の汎化能力を示します (Fig. 2)。</p>
<h4 class="subsection-title" style="color: var(--color-primary); border-left-color: var(--color-primary);"><i class="fas fa-bullseye"></i> 主な貢献 (Main Contributions)</h4>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));">
<div class="info-card">
<div class="feature-item">
<i class="fas fa-universal-access fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>汎用フレームワークの提案</strong><br/>広範なドメイン内タスクをサポートし、未知のタスクへの汎化を示す、In-context Learningベースの汎用画像生成フレームワークを提案します。</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-database fa-2x" style="color: var(--color-accent2);"></i>
<p><strong>Graph200Kデータセットの設計</strong><br/>コンパクトなタスク空間を構築するグラフ構造のデータセットGraph200Kを設計し、柔軟なオンラインタスクサンプリングを可能にし、モデルがタスク間で共有可能で転移可能な知識を学習することを促進します。</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-link fa-2x" style="color: var(--color-accent3);"></i>
<p><strong>統一された生成定式化</strong><br/>我々の統一された画像生成定式化は、最先端の修復モデルと一貫した目的を共有しており、構造を変更することなく最小限のチューニングで優れたパフォーマンスを実現します。</p>
</div>
</div>
</div>
</div>
<div class="tag-list" style="margin-top: 20px;">
<span class="tag">拡散モデル</span>
<span class="tag">汎用生成モデル</span>
<span class="tag">In-context Learning</span>
<span class="tag">タスク指示</span>
<span class="tag">タスクスパース性</span>
<span class="tag">Graph200K</span>
<span class="tag">画像修復</span>
<span class="tag">FLUX.1-Filldev</span>
</div>
</div>
<div class="section-card" id="2._Related_Work">
<h2 class="section-title"><i class="fas fa-book-open"></i> 2. Related Work</h2>
<p>このセクションでは、本研究「VisualCloze」がどのような背景のもとに提案されたのかを理解するために、関連する既存の研究について解説します。主に、<span class="keyword">画像生成技術</span>の進展と、<span class="keyword">視覚的文脈内学習 (Visual In-context Learning)</span> という学習パラダイムに焦点を当て、それぞれの分野での現状の成果と課題を明らかにします。そして、本研究がこれらの課題にどのように取り組み、どのような新しい貢献を目指しているのかを明確にします。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> このセクションの目的</p>
<ul class="unstyled-list">
<li>✏️ 画像生成分野と視覚的文脈内学習分野の主要な研究動向を概観する。</li>
<li>✏️ 既存研究の限界や課題を指摘する。</li>
<li>✏️ 本研究「VisualCloze」の位置づけと新規性を明らかにする。</li>
</ul>
</div>
<h3 class="subsection-title"><i class="fas fa-paint-brush"></i> 2.1. Image Generation</h3>
<p>近年、<span class="highlight">テキスト記述から画像を生成する技術</span>は目覚ましい進歩を遂げています。この発展は、主に2つのタイプのモデルによって牽引されてきました。</p>
<div class="info-grid">
<div class="info-card">
<p class="icon-item"><i class="fas fa-robot"></i> <strong>自己回帰モデル (Autoregressive Models)</strong></p>
<p>画像をピクセル単位で順番に生成していくモデルです。例えば、Transformerベースのモデルなどがこれに該当し、高品質な画像を生成できますが、生成に時間がかかる傾向があります。 (例: <span class="reference">[41, 58, 78]</span>)</p>
</div>
<div class="info-card">
<p class="icon-item"><i class="fas fa-atom"></i> <strong>拡散モデル (Diffusion Models)</strong></p>
<p>ノイズから徐々に画像を生成していくモデルです。近年非常に高い性能を示しており、多くの研究で採用されています。 (例: <span class="reference">[2, 13, 15, 18, 24, 40, 42, 48, 51]</span>)</p>
</div>
</div>
<p>これらのモデルの中でも、<span class="keyword">Rectified Flow Transformer</span> <span class="reference">[15, 17, 33, 88]</span> は、学習効率と全体的な性能の高さで注目されています。これらの基礎モデルをベースとして、様々な応用技術が登場しています。</p>
<div class="feature-card-grid">
<div class="feature-item">
<p class="icon-item"><i class="fas fa-sliders-h"></i></p>
<p><strong>条件付き生成 (Conditional Generation)</strong> <span class="reference">[80]</span><br/>テキストだけでなく、スケッチやセグメンテーションマップなど、様々な条件に基づいて画像を生成します。</p>
</div>
<div class="feature-item">
<p class="icon-item"><i class="fas fa-palette"></i></p>
<p><strong>スタイル変換 (Style Transfer)</strong> <span class="reference">[64]</span><br/>ある画像のスタイルを別の画像に適用します。</p>
</div>
<div class="feature-item">
<p class="icon-item"><i class="fas fa-user-circle"></i></p>
<p><strong>個人化生成 (Personalized Generation)</strong> <span class="reference">[38]</span><br/>特定の人物や物体の画像を生成します。</p>
</div>
</div>
<div class="bubble-box">
<p>📝 <strong>ユニバーサルモデルへの挑戦</strong></p>
<p>最近では、これらの多様なタスクを単一のモデルで処理しようとする<span class="keyword">ユニバーサルモデル (Universal Models)</span> <span class="reference">[35, 44, 83]</span> の研究が進められています。例えば、</p>
<ul>
<li><span class="badge blue">OmniGen</span> <span class="reference">[71]</span>: 大規模視覚言語モデルを活用し、複数のタスクを単一フレームワークに統合。</li>
<li><span class="badge blue">UniReal</span> <span class="reference">[9]</span>: 画像生成タスクを不連続な動画生成として統一。</li>
</ul>
</div>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 既存ユニバーサルモデルの課題</p>
<p>しかし、これらのユニバーサルモデルはいくつかの課題に直面しています。</p>
<ul class="unstyled-list">
<li><i class="fas fa-language"></i> <span class="keyword">言語指示への過度な依存</span>: タスクの指示を言語に頼りすぎているため、複雑な視覚タスクのニュアンスを捉えきれない場合があります。</li>
<li><i class="fas fa-puzzle-piece"></i> <span class="keyword">視覚タスクの分離と疎性</span>: 各視覚タスクが独立しており、タスク間の関連性が薄いため、知識の転移が難しい。</li>
<li><i class="fas fa-cogs"></i> <span class="keyword">柔軟なタスク形式に対応するアーキテクチャ設計</span>: 多様な入力形式やタスク要求に柔軟に対応できる統一的なモデル構造が求められています。</li>
</ul>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-paper-plane"></i> 本研究のアプローチ</p>
<p>これらの課題に対処するため、本研究では、画像生成タスクを<span class="highlight">画像インフィル（穴埋め）</span>として統一する新しいユニバーサル画像生成フレームワーク「VisualCloze」を提案します。
        具体的には、
        </p>
<div class="process-step">
<div class="step-number">1</div>
<div class="step-content"><strong>視覚的文脈内学習 (Visual In-context Learning)</strong> を活用し、言語指示の曖昧さを軽減します。</div>
</div>
<div class="process-step">
<div class="step-number">2</div>
<div class="step-content">独自に構築した<span class="keyword">Graph200Kデータセット</span>により、密なタスク空間を形成し、タスク間で転移可能な知識の学習を促進します。</div>
</div>
<p>これにより、多様なドメイン内タスクをサポートし、訓練時には学習していない未知のタスクに対しても高い汎化性能を発揮することを目指します。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-microscope"></i> 2.2. Visual In-context Learning</h3>
<p><span class="keyword">文脈内学習 (In-context Learning)</span> <span class="reference">[14]</span> は、大規模言語モデル (LLM)、例えばGPT-3 <span class="reference">[5]</span> の登場とともに注目されるようになったアプローチです。これは、モデルにいくつかのデモンストレーション（例示）を与えることで、複雑なタスクを理解し実行させる手法です。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book"></i> 用語解説：文脈内学習 (In-context Learning)</p>
<p>モデルが少数のタスク例（文脈）を入力として受け取り、その文脈からタスクのパターンやルールを推測し、新しい入力に対して同様のタスクを実行する能力のことです。ファインチューニングなしに新しいタスクに適応できる点が特徴です。</p>
<div style="text-align: center; margin-top: 10px;">
<span class="badge purple">例1</span> <span class="arrow-connector" style="width: 20px; height: 14px; display: inline-block; position: relative; margin: 0 5px;"><svg style="position: absolute; top: -10px; left: 0; width: 20px; height: 30px;" viewbox="0 0 100 50"><path d="M0 25 L80 25 L70 15 M80 25 L70 35" fill="none" stroke="#4a6fa5" stroke-linecap="round" stroke-width="5"></path></svg></span> <span class="badge purple">タスク説明</span> <span class="arrow-connector" style="width: 20px; height: 14px; display: inline-block; position: relative; margin: 0 5px;"><svg style="position: absolute; top: -10px; left: 0; width: 20px; height: 30px;" viewbox="0 0 100 50"><path d="M0 25 L80 25 L70 15 M80 25 L70 35" fill="none" stroke="#4a6fa5" stroke-linecap="round" stroke-width="5"></path></svg></span> <span class="badge purple">モデル出力</span>
</div>
</div>
<p>視覚分野（Vision Modality）における初期の研究 <span class="reference">[21, 22]</span> では、<span class="keyword">画像アナロジー (Image Analogies)</span> という形で、例から自動的に画像フィルタを作成する試みがありました。近年では、以下の技術を活用して、より多くのタスクに対応する<span class="keyword">視覚的文脈内学習 (Visual In-context Learning)</span> が提案されています。</p>
<ul class="tag-list">
<li class="tag">インペインティングモデル (Inpainting Model) <span class="reference">[3, 4, 82]</span></li>
<li class="tag">マスク画像モデリング (Masked Image Modeling) <span class="reference">[43, 66, 67]</span></li>
<li class="tag">視覚言語モデル (Vision-Language Model) <span class="reference">[1, 86]</span></li>
</ul>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 既存の視覚的文脈内学習の限界</p>
<p>しかし、これらの研究は主に以下の点に限定されていました。</p>
<ul class="unstyled-list">
<li><i class="fas fa-border-all"></i> <span class="keyword">密な予測 (Dense Prediction)</span> <span class="reference">[55, 59, 87]</span>: 画像の各ピクセルに対して値を予測するタスク（例：セグメンテーション、深度推定）。</li>
<li><i class="fas fa-brain"></i> <span class="keyword">視覚理解 (Visual Understanding)</span> <span class="reference">[63]</span>: 画像の内容を理解するタスク（例：物体認識、キャプション生成）。</li>
</ul>
<p><span class="badge blue">OmniGen</span> <span class="reference">[71]</span> は文脈内学習を活用して、訓練時に学習したセグメンテーションタスクを未知の概念に対しても汎化させることを試みていますが、これも主に単純な密な予測タスクに焦点が当てられており、未知のドメインと訓練ドメイン間のギャップは依然として限定的です。</p>
<p>画像生成への応用 <span class="reference">[34, 43, 60, 68]</span> も試みられていますが、条件付き生成や密な予測といった比較的単純なタスクに限定されています。</p>
</div>
<div class="glass-card" style="margin-top: 20px;">
<p><i class="fas fa-project-diagram" style="color: var(--color-accent2);"></i> <strong>視覚タスクの疎性の問題</strong></p>
<p>さらに重要な課題として、<span class="keyword">視覚タスクの疎性 (Sparsity of visual tasks)</span> が挙げられます。これは、異なる視覚タスク間で知識を転移させたり、共通の知識を学習したりすることが難しいという問題です。タスクがまばらに存在しているため、モデルはタスク間で重複する知識を獲得しにくく、これが文脈内学習による画像生成能力を制限する一因となっています。</p>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-lightbulb"></i> 本研究の貢献：Graph200Kデータセット</p>
<p>この課題に対し、本研究では<span class="keyword">Graph200K</span>という<span class="highlight">グラフ構造のデータセット</span>を導入します。このデータセットは、相互に関連する多数のタスクをサポートすることで、より<span class="keyword">密なタスク空間 (Dense Task Space)</span> を構築します。これにより、モデルはタスク間で共有可能かつ転移可能な知識を効率的に学習し、適応性を大幅に向上させることが期待されます。</p>
</div>
<img alt="Figure 4. Illustration of the proposed Graph200K dataset." src="graph200k_dataset_tasks.jpg" style="margin-top: 20px; margin-bottom: 10px; border: 1px solid #ddd; padding: 5px; border-radius: 8px;"/>
<p style="text-align: center; font-family: 'Yomogi', cursive; color: var(--color-gray);">図4: 提案するGraph200Kデータセットの図解</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-search-plus"></i> 図4の解説</p>
<p>この図は、本研究で提案されている<span class="keyword">Graph200Kデータセット</span>の構造を示しています。</p>
<ul>
<li>📌 <strong>中心画像 (Image)</strong>: データセットの中心となる元画像です。図では王冠の画像が例として示されています。</li>
<li>📌 <strong>5つのメタタスク (Meta-tasks)</strong>: 各中心画像には、以下の5つの主要なタスクカテゴリ（メタタスク）に関連するアノテーションが付与されます。
                <ol>
<li><span class="badge orange">条件付き生成 (Conditional Generation)</span>: 特定の条件（例：深度マップ、エッジ画像）から画像を生成するタスク。図では、深度情報やセグメンテーションマップなどが示されています。</li>
<li><span class="badge orange">画像修復 (Image Restoration)</span>:劣化した画像（例：ノイズ除去、ブレ除去）を高品質な画像に復元するタスク。図では、ぼやけた画像や線で隠された画像などが示されています。</li>
<li><span class="badge orange">画像編集 (Image Editing)</span>:画像の一部を変更・修正するタスク。図では、蝶や木が追加されたり、オブジェクトが変化したりする例が示されています。</li>
<li><span class="badge orange">IP保存 (IP Preservation)</span>: 特定の物体やキャラクター（Intellectual Property）の特徴を保持したまま画像を生成・編集するタスク。図では、参照画像 (Reference)として王冠の画像が示されており、これを基にスタイルを適用する例などを示唆しています。</li>
<li><span class="badge orange">スタイル変換 (Style Transfer)</span>: ある画像のスタイルを別の画像に適用するタスク。図では、絵画風のスタイルなどが示されています。</li>
</ol>
</li>
<li>📌 <strong>タスクの組み合わせ (Combining diverse tasks)</strong>: 図の下部では、これらのメタタスクやアノテーションを組み合わせることで、より複雑で多様なタスクを構築できることが示されています。例えば、<span class="highlight">「被写体 (subject) + レイアウト (layout) + スタイル (style) = スタイル化 (stylization)」</span>といった具体的な組み合わせ例が挙げられています。これは、王冠（被写体）を特定の構図（レイアウト）で、絵画風（スタイル）の画像として生成するタスクを意味します。</li>
</ul>
<p>このようにGraph200Kデータセットは、多様なアノテーションを関連付けることで、タスク間の関係性を密にし、モデルが汎用的な画像生成能力を獲得するための基盤となります。</p>
</div>
</div>
<div class="section-card" id="3._Dataset">
<h2 class="section-title"><i class="fas fa-database"></i>3. Dataset</h2>
<div class="content-box">
<p>このセクションでは、論文で提案する<span class="keyword">VisualCloze</span>フレームワークの学習に使用するデータセットについて詳しく説明します。特に、既存の画像生成研究における課題、すなわち<span class="highlight">タスクのスパース性（まばらさ）</span>と<span class="highlight">分離性</span>がモデルの<span class="keyword">汎化能力</span>（未知のタスクへの対応能力）を制限しているという問題意識から出発します。この問題を解決するために、タスク間の関連性を強化し、タスク密度を高めることを目的とした新しいデータセット<span class="keyword">Graph200K</span>を構築します。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i>主な目的と論旨</p>
<p>従来の統合画像生成モデルは、学習データに含まれない<span class="highlight">未知のタスクへの汎化能力</span>に限界がありました。これは、学習に使われる個々の視覚タスクがまばらに存在し、互いに孤立しているため、モデルがタスク間で共通する特徴を学習したり、未知のタスクに対応したりするのが難しいことが一因と考えられます。また、タスク間の関連性が弱いと、知識の転移が妨げられ、モデルの適応性が制限されます。</p>
<p>そこで本研究では、<span class="keyword">タスク密度を高める</span>、あるいは<span class="keyword">タスク間の相互関係を強化する</span>ことで、コンパクトなタスク分布を通じてモデルの汎化能力を向上させることを目指します。<span class="highlight">Subject200K</span>[61]データセットを出発点とし、各画像に対して<span class="badge blue">5つのメタタスク</span>にまたがる<span class="badge orange">49種類のアノテーション</span>を付与することで、<span class="keyword">Graph200K</span>データセットを構築します。この豊富なアノテーション空間により、図4に示すように、異なるメタタスク間でアノテーションの任意の部分集合をサンプリングし組み合わせることで、<span class="highlight">広範囲の関連タスクを柔軟に構築</span>できます。</p>
</div>
</div>
<img alt="Graph200K Dataset Tasks Illustration" class="section-image" src="graph200k_dataset_tasks.jpg"/>
<div class="caption-box" style="text-align: center; margin-bottom: 20px;">
<p style="font-family: 'Yomogi', cursive; font-size: 13px; color: var(--color-gray);">
<i class="fas fa-image"></i> <strong>図4: 提案するGraph200Kデータセットの図解</strong><br/>
            各画像は、<span class="badge blue">条件付き生成</span>、<span class="badge green">画像修復</span>、<span class="badge orange">画像編集</span>、<span class="badge purple">IP保存</span>、<span class="badge yellow">スタイル転送</span>という5つのメタタスクのアノテーションが付与されます。<br/>これらのタスクを利用して、図の下部のように、広範囲の複雑なタスクを組み合わせることができます。
        </p>
</div>
<h3 class="subsection-title"><i class="fas fa-project-diagram"></i>3.1. Graph-Structured Multi-Task Dataset</h3>
<div class="content-box">
<p>自然言語処理（NLP）の分野では、タスク同士が大きく重複しているため、強力な<span class="keyword">クロスタスク学習能力</span>（複数のタスクを同時に学習する能力）が促進されます。しかし、視覚タスクは本質的にそれぞれが異なっているため、視覚モデルがインストラクションチューニング（指示に従って学習する手法）を通じて同様の汎化能力を達成することは困難です。</p>
<p>この問題を緩和するために、本論文では<span class="keyword">Graph-Structured Multi-Task Dataset (グラフ構造化マルチタスクデータセット)</span>を導入します。</p>
<div class="glass-card">
<p style="font-family: 'Yomogi', cursive; text-align: center; font-size: 16px; color: var(--color-primary); margin-bottom: 10px;">
<i class="fas fa-network-wired"></i> データセットの構造イメージ (図4 (a)参照)
            </p>
<div style="display: flex; align-items: center; justify-content: space-around; flex-wrap: wrap;">
<div style="text-align: center; margin: 10px; padding: 10px; background-color: rgba(255,255,255,0.7); border-radius: 8px;">
<i class="fas fa-image fa-3x" style="color: var(--color-primary);"></i>
<p style="font-family: 'Kaisei Decol', serif;">中心ノード: 各画像</p>
</div>
<div style="font-size: 30px; color: var(--color-secondary); margin: 0 10px;">↔</div>
<div style="text-align: center; margin: 10px; padding: 10px; background-color: rgba(255,255,255,0.7); border-radius: 8px;">
<i class="fas fa-tasks fa-3x" style="color: var(--color-accent1);"></i>
<p style="font-family: 'Kaisei Decol', serif;">周辺ノード: 多様なタスクアノテーション</p>
<ul class="unstyled-list" style="font-size: 12px; text-align: left; margin-top: 5px;">
<li><i class="fas fa-draw-polygon" style="color: var(--color-accent2);"></i> 様々な空間条件</li>
<li><i class="fas fa-tools" style="color: var(--color-accent2);"></i> 画像の劣化</li>
<li><i class="fas fa-edit" style="color: var(--color-accent2);"></i> 画像編集結果</li>
<li><i class="fas fa-id-badge" style="color: var(--color-accent2);"></i> IP保存用参照画像</li>
<li><i class="fas fa-palette" style="color: var(--color-accent2);"></i> スタイル転送 (多様な参照スタイル)</li>
</ul>
</div>
</div>
<p style="text-align: center; margin-top: 15px; font-size: 13px;">各タスクペアの構築プロセスについては、次のセクションで詳しく説明します。</p>
</div>
<p>図4に示されるように、各タスクアノテーションは画像と<span class="keyword">双方向のエッジ</span>（つながり）を形成します。これにより、グラフは<span class="keyword">強連結</span>であると言えます。これは、グラフ内の任意の2つのノード（例えば、あるアノテーションと別の画像、または2つの異なるアノテーション）間に、双方向の経路が存在することを意味します。</p>
<div class="bubble-box">
<p style="font-family: 'Yomogi', cursive; font-weight: bold; color: var(--color-primary);"><i class="fas fa-lightbulb"></i> 生成タスクの新しい捉え方</p>
<p>このグラフ構造において、画像生成タスクは<span class="highlight">グラフ内のパス（経路）</span>として定式化できます。</p>
<ul>
<li>パス上のノード（終点ノードを除く）：<span class="keyword">条件画像</span>として機能します。これは、インストラクションファインチューニングにおける「質問」に類似しています。</li>
<li>パスの終点ノード：<span class="keyword">ターゲット画像</span>（生成したい画像）であり、「回答」の役割を果たします。</li>
</ul>
<p>例えば、図4の下部に示されているように、「<span class="badge purple">reference</span> <i class="fas fa-arrow-right"></i> <span class="badge orange">editing</span> <i class="fas fa-arrow-right"></i> <span class="badge green">image</span>」というパスは、参照画像を用いた画像編集タスクに対応します。</p>
</div>
<p>具体的には、私たちの<span class="keyword">Graph200K</span>データセットには<span class="badge blue">49種類</span>のノードがあり、最大で<span class="badge orange">134個</span>の<span class="highlight">高度に重複するタスク</span>をサンプリングします。これにより、モデルはタスク間でよりコンパクトで共有された表現を学習することが可能になります。さらに、これは私たちのインストラクションファインチューニングデータの<span class="keyword">多様性と柔軟性</span>を豊かにします。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i>3.2. Dataset Construction</h3>
<div class="content-box">
<p>データ構築の便宜上、<span class="keyword">Subject200K</span>[61]データセットから被写体主導のデータを継承しています。さらに、画像修復データを取得するために、画像に対して<span class="badge blue">32種類</span>の異なる<span class="keyword">劣化 (degradations)</span> をオンライン（学習処理中）で適用します。</p>
<p>このセクションでは、残りの3つの主要タスク（<span class="highlight">条件付き生成</span>、<span class="highlight">スタイル転送</span>、<span class="highlight">画像編集</span>）に関するデータ構築方法をまとめます。</p>
<div class="info-grid">
<div class="info-card">
<p class="framework-title" style="font-family: 'Yomogi', cursive;"><i class="fas fa-magic"></i> Conditional generation (条件付き生成)</p>
<p>ControlNet [80] の手法に倣い、各画像は専門モデルによって生成された<span class="badge orange">12種類</span>の異なる条件とペアリングされます。これらの条件には以下のようなものが含まれます：</p>
<ul class="unstyled-list tag-list" style="margin-top: 10px; margin-bottom:10px;">
<li class="tag"><i class="fas fa-bezier-curve"></i> Cannyエッジ [6]</li>
<li class="tag"><i class="fas fa-signature"></i> HEDエッジ [72]</li>
<li class="tag"><i class="fas fa-ruler-combined"></i> Hough線 [20]</li>
<li class="tag"><i class="fas fa-map-marked-alt"></i> セマンティックセグメンテーションマップ [37]</li>
<li class="tag"><i class="fas fa-layer-group"></i> 深度マップ [74]</li>
<li class="tag"><i class="fas fa-shapes"></i> 形状法線マップ [73]</li>
<li class="tag"><i class="fas fa-male"></i> 人間のキーポイント [7]</li>
</ul>
<p>本研究では、これらの条件をさらに拡張し、以下を組み込んでいます：</p>
<ul class="unstyled-list tag-list" style="margin-top: 10px;">
<li class="tag"><i class="fas fa-object-ungroup"></i> SAM2 [50] マスク</li>
<li class="tag"><i class="fas fa-crop-alt"></i> 前景セグメンテーション</li>
<li class="tag"><i class="fas fa-search-location"></i> オープンワールドのボックスとマスク</li>
</ul>
<div class="note-box" style="padding: 10px; margin-top:10px; background-color: rgba(92, 184, 92, 0.1); border-left: 3px solid var(--color-accent1);">
<p class="note-title" style="color: var(--color-accent1);"><i class="fas fa-info-circle"></i> 詳細</p>
<p style="font-size: 13px;">前景セグメンテーションはRMBG [84]から派生し、<span class="keyword">インペインティング</span>（部分的な画像の修復・補完）や<span class="keyword">前景抽出</span>などの多様なタスクをサポートします。オープンワールドのバウンディングボックスは、Qwen2-VL [65]の<span class="keyword">グラウンディングキャプション能力</span>（テキスト記述に基づいて画像内の領域を特定する能力）を通じて生成され、これらはSAM2 [50]を使用して対応するマスクを生成するために処理されます。</p>
</div>
</div>
<div class="info-card">
<p class="framework-title" style="font-family: 'Yomogi', cursive;"><i class="fas fa-palette"></i> Style transfer (スタイル転送)</p>
<p>参照画像に基づいて、<span class="keyword">セマンティックバリアント</span>（意味内容が変化する）設定と<span class="keyword">セマンティックインバリアント</span>（意味内容を保持する）設定の両方で画像のスタイルを転送します。</p>
<div class="two-column" style="gap: 10px;">
<div class="column" style="background-color: rgba(255, 126, 95, 0.05); padding:10px; border-radius: 5px;">
<p style="font-weight: bold; color: var(--color-secondary);">Semantic-invariant (意味内容保持)</p>
<p style="font-size: 13px;">InstantStyle [64] を採用し、意味内容を保持します。</p>
</div>
<div class="column" style="background-color: rgba(149, 117, 205, 0.05); padding:10px; border-radius: 5px;">
<p style="font-weight: bold; color: var(--color-accent2);">Semantic-variant (意味内容変化)</p>
<p style="font-size: 13px;">FLUX.1-Redux-dev [33] に依存し、スタイル埋め込みと深度を条件として使用します。</p>
</div>
</div>
<p>各画像に対して、ランダムに<span class="badge orange">5つ</span>のスタイル化されたバージョンを生成します。</p>
<div class="note-box" style="padding: 10px; margin-top:10px; background-color: rgba(255, 213, 79, 0.1); border-left: 3px solid var(--color-accent3);">
<p class="note-title" style="color: var(--color-accent3);"><i class="fas fa-exclamation-triangle"></i> 重要ポイント</p>
<p style="font-size: 13px;">これら2つのタスク（意味内容保持と変化）を混合することで、モデルが曖昧さを避け、<span class="highlight">文脈内例（in-context examples）</span>により良く従うように促します。</p>
</div>
</div>
<div class="info-card">
<p class="framework-title" style="font-family: 'Yomogi', cursive;"><i class="fas fa-edit"></i> Image editing (画像編集)</p>
<p>2種類の編集タスクを設計します：<span class="keyword">背景バリアント</span>（背景が変化する）編集と<span class="keyword">背景インバリアント</span>（背景を保持する）編集です。</p>
<div class="pipeline" style="margin-top: 15px;">
<div class="pipeline-step">
<p style="font-weight: bold;">背景インバリアント編集:</p>
<ol class="unstyled-list" style="padding-left: 20px; font-size: 13px;">
<li><span class="step-number" style="display:inline-flex; width:20px; height:20px; font-size:12px;">1</span> 被写体の位置を特定します。</li>
<li><span class="step-number" style="display:inline-flex; width:20px; height:20px; font-size:12px;">2</span> 大規模視覚言語モデル<span class="keyword">Qwen2-VL [65]</span>を活用し、元の被写体を新しいオブジェクトに置き換えるように画像キャプションを修正します。</li>
<li><span class="step-number" style="display:inline-flex; width:20px; height:20px; font-size:12px;">3</span> 被写体がマスクされた画像を、<span class="keyword">FLUX.1-Fill-dev [33]</span>インペインティングモデルで処理し、代替オブジェクトをマスク領域に統合します。</li>
<li><span class="step-number" style="display:inline-flex; width:20px; height:20px; font-size:12px;">4</span> データセットを豊かにするために、上記の操作を<span class="badge orange">5回</span>繰り返します。</li>
</ol>
</div>
<div class="pipeline-step" style="margin-top: 25px;">
<p style="font-weight: bold;">背景バリアント編集:</p>
<p style="font-size: 13px;">違いは最後のステップにあり、<span class="keyword">深度を条件</span>とし、<span class="keyword">修正されたキャプションをテキストプロンプト</span>として<span class="keyword">FLUX.1-Redux-dev [33]</span>を利用します。</p>
</div>
</div>
</div>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-plus-square"></i>3.3. Other Data</h3>
<div class="content-box">
<p>タスクの範囲をさらに拡大し、モデルの<span class="keyword">汎化能力</span>を向上させるために、トレーニング中にいくつかの<span class="highlight">オープンソースデータセット</span>を組み込みます。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-tshirt fa-2x" style="color: var(--color-accent1);"></i>
<p style="font-weight: bold;">VITON-HD [11]</p>
<p style="font-size: 13px;"><span class="badge green">バーチャル試着</span>タスク用</p>
</div>
<div class="feature-item">
<i class="fas fa-paint-brush fa-2x" style="color: var(--color-accent2);"></i>
<p style="font-weight: bold;">PhotoDoodle [28]</p>
<p style="font-size: 13px;"><span class="badge purple">芸術的な画像編集</span>タスク用</p>
</div>
<div class="feature-item">
<i class="fas fa-object-group fa-2x" style="color: var(--color-secondary);"></i>
<p style="font-weight: bold;">OmniEdit [69]</p>
<p style="font-size: 13px;"><span class="badge orange">画像編集タスク</span>拡張用</p>
</div>
</div>
<div class="challenge-box" style="margin-top: 20px;">
<p class="challenge-title"><i class="fas fa-microscope"></i> OmniEdit [69] の詳細</p>
<p>特に、以下の2つのサブタスクをトレーニングに使用します：</p>
<ul style="list-style-type: '✏️'; padding-left: 20px; margin-top:5px;">
<li>オブジェクト追加</li>
<li>オブジェクト削除</li>
</ul>
<p>他の編集タスク（例：<span class="keyword">属性変更</span>、<span class="keyword">環境変更</span>など）は、訓練されたモデルの<span class="highlight">汎化能力を評価するための未見タスク</span>として扱います。</p>
</div>
<p style="margin-top: 20px;">さらに、高品質な<span class="keyword">内部データ</span>の一部も活用します。これには以下のタスクが含まれます：</p>
<div class="tag-list" style="margin-top: 10px;">
<span class="tag" style="background-color: var(--color-primary); color: white;"><i class="fas fa-pencil-ruler"></i> 描画プロセス [62]</span>
<span class="tag" style="background-color: var(--color-primary); color: white;"><i class="fas fa-cubes"></i> マルチビュー生成 [29]</span>
</div>
<p style="margin-top: 10px;">これらの追加データにより、モデルがより多様なシナリオに対応し、ロバストな汎化性能を獲得することを目指します。</p>
</div>
</div>
<div class="section-card" id="4._Method">
<h2 class="section-title"><i class="fas fa-microscope"></i>4. Method ~手法~</h2>
<div class="content-box">
<p>この論文では、<span class="keyword">汎用的な画像生成モデル</span>を構築する上での核心的な課題を特定しています。具体的には、以下の3点が挙げられます。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card">
<div class="feature-item">
<i class="fas fa-tasks fa-2x" style="color: var(--color-accent1);"></i>
<p><strong>明確で一般化可能なタスク定式化の必要性</strong></p>
<p class="reference">様々なタスクを統一的に扱えるような、明確な「問題設定」の方法が求められます。</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-puzzle-piece fa-2x" style="color: var(--color-accent2);"></i>
<p><strong>視覚タスクのスパース性 (疎性)</strong></p>
<p class="reference">視覚タスクのデータは関連性が薄く散在しがちで、これがモデルの学習効率を下げます。（これは前セクションで<span class="highlight">Graph200Kデータセット</span>の構築により対処されました。）</p>
</div>
</div>
<div class="info-card">
<div class="feature-item">
<i class="fas fa-cogs fa-2x" style="color: var(--color-accent3);"></i>
<p><strong>マルチタスク学習のための統一フレームワークの欠如</strong></p>
<p class="reference">複数のタスクを効率よく学習できる、一貫したモデル構造が必要です。</p>
</div>
</div>
</div>
<p>このセクションでは、これらの課題に取り組むための具体的な手法を提案します。</p>
<ul class="unstyled-list">
<li>✏️ <strong style="color: var(--color-secondary);">セクション4.1</strong>では、<span class="keyword">視覚的インコンテキスト学習 (Visual In-context Learning)</span> を、汎用的なタスク定式化のための理想的なパラダイムとして導入します。</li>
<li>✏️ <strong style="color: var(--color-secondary);">セクション4.2</strong>では、<span class="keyword">画像修復 (Image Infilling) モデル</span>を統一的なマルチタスクフレームワークとみなし、最小限のコストで強力な汎化能力を達成する方法を説明します。</li>
</ul>
</div>
<h3 class="subsection-title"><i class="fas fa-lightbulb"></i>4.1. Visual In-context Learning ~視覚的インコンテキスト学習~</h3>
<div class="content-box">
<p>単一の生成モデルで複数の視覚生成タスクを扱うために、従来は<span class="highlight">言語指示 (Language Instructions)</span> を使ってタスクの定義を指定するのが一般的でした。しかし、このアプローチにはいくつかの課題があります。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i>言語指示の限界</p>
<ul>
<li><span class="keyword">視覚と言語のギャップ</span>: 視覚的なニュアンスを言語だけで完全に表現するのは難しい。</li>
<li><span class="keyword">モデルのテキスト理解能力の限界</span>: 画像生成モデルのテキスト解釈能力はまだ発展途上。</li>
<li><span class="keyword">タスク混同 (Task Confusion)</span>: どのタスクを実行すべきかモデルが混乱しやすい。[39]</li>
<li><span class="keyword">未知タスクへの汎化能力の弱さ</span>: 学習データに含まれない新しいタスクへの対応が苦手。</li>
</ul>
</div>
<p>そこで、この研究では<span class="keyword">大規模言語モデル (LLM)</span>における<span class="keyword">少数ショット学習 (Few-shot Learning)</span> [5] の成功に着想を得て、<span class="highlight">視覚的コンテキスト</span>が視覚生成モデルにとってより直感的で理解しやすいタスク指示となり得ると考えました。なぜなら、画像生成モデルは元来、優れた<span class="keyword">視覚理解能力</span>を持っているからです。</p>
<div class="bubble-box">
<p>💡 <strong>アイデアの核心</strong>: モデルに「やってほしいこと」を言葉で説明するのではなく、<span class="highlight">「お手本」となる視覚的な例 (Visual Demonstrations)</span> を見せることでタスクを理解させる、というアプローチです。</p>
</div>
<p>この論文では、汎用的で一般化可能な画像生成システムを構築するために、<span class="keyword">視覚的インコンテキスト学習 (Visual In-context Learning)</span> を再提案します。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i>用語定義: クエリ (Query)</p>
<p>説明を簡単にするため、任意の条件付き生成タスクの画像入出力を<span class="keyword">クエリ</span>として定義します。クエリは以下から構成されます：</p>
<ul>
<li><span class="badge blue">条件画像群</span>: \(L-1\) 個の条件画像 (\(x_1, \dots, x_{L-1}\))。これらがタスクの入力となります。</li>
<li><span class="badge orange">空白ターゲット</span>: モデルが補完・生成すべき空白の領域 (\(\emptyset\))。これがタスクの出力となります。</li>
</ul>
<p>クエリ \(X\) は、これらを連結したものとして表現されます：</p>
<div class="formula">
<p>\(X = \operatorname { c o n c a t } ( \{ x _ { 1 } , \dots , x _ { L - 1 } , \emptyset \} ) \)</p>
</div>
<p class="reference">📌 ここで \(L\) は、1つのタスク例（クエリまたはインコンテキスト例）に含まれる画像の総数（条件画像＋ターゲット画像）です。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-expand-arrows-alt"></i>拡張性について</p>
<p>この手法は、クエリの最後に1枚の画像を生成するだけでなく、より一般的なシナリオ（例えば、任意の位置に任意の数の画像を生成する）にも拡張可能です。これについては、論文のセクション5.1で詳しく説明されています。</p>
</div>
<p>訓練時には、モデルにタスクの「お手本」として、最大 \(C\) 個の<span class="keyword">インコンテキスト例 (In-context Examples)</span> をランダムに提供します。各インコンテキスト例も、クエリと同様に \(L\) 個の画像（条件画像群と対応する正解のターゲット画像）を含んでいます。</p>
<div class="feature-item" style="background-color: #e3f2fd; padding:15px; border-radius:8px; margin-top:10px; margin-bottom:10px;">
<i class="fas fa-cogs fa-2x" style="color: var(--color-primary);"></i>
<p><strong>訓練戦略の目的</strong></p>
<p>このランダムな数のインコンテキスト例を提供する戦略は、モデルが<span class="highlight">様々な数の「お手本」に対応できる汎化能力</span>を身につけることを保証します。つまり、お手本が1つの場合でも、複数の場合でも、あるいは全くない場合でも、モデルが適切に動作することを目指します。</p>
</div>
<p>実験結果から、インコンテキスト例をタスクのデモンストレーションとして提供することには、以下のような効果があることが示されています：</p>
<ul class="unstyled-list">
<li>✅ <span class="highlight">タスク混同の軽減</span>: モデルが実行すべきタスクをより明確に理解できるようになります。</li>
<li>✅ <span class="highlight">ドメイン内タスクのパフォーマンス向上</span>: 学習データに含まれるタスク（既知のタスク）に対する精度が向上します。[39]</li>
<li>✅ <span class="highlight">未知タスクへの汎化能力向上</span>: 学習データに含まれない新しいタスクへの対応能力が向上します。</li>
</ul>
<hr/>
<p>📌 <strong>図5: 位置埋め込みを適用する際の画像の連結方法</strong></p>
<img alt="Figure 5: Concatenating images when applying position embeddings." class="figure-image" src="in_context_learning_depth_sketch_to_image.jpg"/>
<div class="glass-card">
<p><i class="fas fa-image"></i> <strong>図5の解説</strong></p>
<p>この図は、アスペクト比が異なる可能性のある複数の画像をどのように連結し、それらに<span class="keyword">位置埋め込み (Positional Embedding)</span>を適用するかを示しています。位置埋め込みは、画像内の各部分の位置情報をモデルに伝えるために重要です。</p>
<ol>
<li><span class="badge blue">ステップ1: 水平方向の連結</span>
<p>まず、\(C\) 個のインコンテキスト例とクエリに含まれるそれぞれの \(L\) 個の画像（条件画像とターゲット画像）を、各例・クエリごとに<span class="highlight">水平方向（横方向）に連結</span>します。これにより、各タスク例が一つの長い横長の画像ストリップのようになります。</p>
<div style="text-align: center; margin: 10px 0;">
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-primary);">[画像1][画像2]...[画像L]</span> <span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-secondary);"> (← 1つのタスク例)</span>
</div>
</li>
<li><span class="badge orange">ステップ2: 時間軸方向 (Temporal) の連結</span>
<p>次に、ステップ1で作成された複数の水平連結された画像ストリップ（各インコンテキスト例とクエリに対応するもの）を、<span class="highlight">時間軸方向 (temporally)、つまり垂直方向（縦方向）に連結</span>します。この「時間軸方向」という表現は、画像シーケンスを扱う際の慣習に由来するもので、ここでは実質的に行として積み重ねることを意味します。これにより、アスペクト比の不一致という問題をうまく処理できます。</p>
<div style="text-align: center; margin: 10px 0; line-height:1.6;">
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-primary);">[例1: 画像1 ... 画像L]</span><br/>
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-primary);">[例2: 画像1 ... 画像L]</span><br/>
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-gray);">... (C個のインコンテキスト例) ...</span><br/>
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-secondary);">[クエリ: 画像1 ... ∅]</span>
</div>
</li>
</ol>
<p>このようにして、様々なタスク例を単一の大きなグリッド状の入力としてモデルに与えることができます。図中の「temporal concatenation」は行方向の積み重ね、「spatial concatenation」は列方向の連結を指しています。</p>
<p>例えば、図では上から順にインコンテキスト例が並び、一番下が現在のタスク（クエリ）です。各行（各タスク例）は、複数の画像（Visual Prompt = 条件画像、Target = 生成対象）が横に連結されています。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-sitemap"></i>4.2. Unified Multi-task Framework ~統一マルチタスクフレームワーク~</h3>
<div class="content-box">
<p>従来の視覚的インコンテキスト学習手法の多く[43, 60]は、単一の画像条件と単一のコンテキスト（お手本）を持つシナリオに主に焦点を当てていました。しかし、この研究では、<span class="keyword">さまざまな数の条件やコンテキストを扱える統一的なフレームワーク</span>を構築し、多様なタスクに柔軟に適応することを目指します。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-ruler-combined"></i>初期仮定: 画像サイズ</p>
<p>説明を簡単にするため、最初はモデルが処理するすべての画像のサイズが同じ (<span class="highlight">\(W \times H\)</span>) であると仮定します。アスペクト比が異なる場合の対応については、このセクションの最後に説明します。</p>
</div>
<p>この仮定のもと、\(C\) 個のインコンテキスト例と1つのクエリがあり、それぞれが \(L\) 個の画像を含む場合を考えます。これらの画像はすべて、サイズが <span class="highlight">\((L \times W, (C+1) \times H)\)</span> の完全な<span class="keyword">グリッドレイアウト画像</span>に連結することができます。</p>
<div style="text-align: center; margin: 20px 0;">
<p style="font-family: 'Yomogi', cursive; font-size: 1.2em;">🖼️ グリッドレイアウトのイメージ 🖼️</p>
<div style="border: 2px dashed var(--color-primary); padding: 10px; border-radius: 8px; background-color: #f0f4f8;">
<p>タテ: <span style="color: var(--color-secondary); font-weight: bold;">(C+1)行</span> (インコンテキスト例C個 + クエリ1個)</p>
<p>ヨコ: <span style="color: var(--color-accent1); font-weight: bold;">L列</span> (各タスク例内のL個の画像)</p>
<pre style="font-family: 'Zen Kurenaido', sans-serif; text-align: left; padding: 5px;">
  [例1: 画像1] [例1: 画像2] ... [例1: 画像L]
  [例2: 画像1] [例2: 画像2] ... [例2: 画像L]
  ... (C個のインコンテキスト例) ...
  [クエリ: 条件1] [クエリ: 条件2] ... [クエリ: ターゲット∅]
                </pre>
</div>
<p style="margin-top: 10px;">全体のサイズ: <span style="color: var(--color-primary); font-weight: bold;">幅 \(L \times W\)</span> × <span style="color: var(--color-primary); font-weight: bold;">高さ \((C+1) \times H\)</span></p>
</div>
<p>このグリッドレイアウトにおいて、モデルは周囲のコンテキスト（既知の画像）に基づいてターゲットとなるグリッド（通常はクエリの最後の空白部分）を<span class="keyword">修復 (infilling)</span> することでタスクを完了します。これは、まるで<span class="highlight">視覚的な穴埋め問題 (Visual Cloze Puzzles)</span> を解くようなものです。そのため、この論文では、複数の解像度を扱える汎用的な<span class="keyword">画像修復アーキテクチャ</span>に基づいて、統一フレームワーク<span class="keyword">VisualCloze</span>を構築します。</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-brain"></i>VisualClozeモデルの定式化</p>
<p>一般的な拡散ベースの修復モデルの設計と一貫して、VisualClozeモデルは次のように定式化できます：</p>
<div class="formula">
<p>\( { \hat { X } } = f ( X \mid T , M ) \)</p>
<p class="reference">式 (1)</p>
</div>
<p>ここで、各要素の意味は以下の通りです：</p>
<ul>
<li><span class="badge blue">\(X\)</span>: 連結されたグリッド画像。最後のグリッド（ターゲット部分）は空白になっています。</li>
<li><span class="badge orange">\(T\)</span>: 言語指示 (Language Instruction)。タスクの種類や生成内容に関するテキスト情報。</li>
<li><span class="badge green">\(M\)</span>: マスク条件 (Mask Condition)。どの部分を修復（生成）すべきかを示すバイナリ行列。</li>
<li><span class="badge purple">\(\hat{X}\)</span>: 修復された結果の画像。</li>
<li><span class="badge gray">\(f\)</span>: 画像修復モデル。</li>
</ul>
</div>
<p><span class="keyword">マスク \(M\)</span> は、サイズが \((H \times (C+1), W \times L)\) のバイナリ行列（0か1の値を持つ行列）です。</p>
<div class="formula">
<p>\(M ( i , j ) = \left\{ \begin{array} { l l } { 1 } &amp; { \mathrm { i f } \ i \in [ H \times C , H \times ( C + 1 ) ) } \\ &amp; { \mathrm { a n d } \ j \in [ W \times ( L - 1 ) , W \times L ) , } \\ { 0 } &amp; { \mathrm { o t h e r w i s e } , } \end{array} \right.\)</p>
<p class="reference">式 (2) (論文中のインデックス調整: C-1 → C, C → C+1)</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-pencil-ruler"></i>論文中の数式(2)のインデックスについて補足</p>
<p>論文中の数式(2)では、行インデックスが \(i \in [H \times (C-1), H \times C)\) となっていますが、これは \(C\) 個のインコンテキスト例がある場合、最後の行（クエリ行）のインデックスが \(C\) 番目（0から数えると \(C-1\) 番目）であることを意味します。しかし、本文の説明ではインコンテキスト例が \(C\) 個あり、クエリが1つあるため、全体で \((C+1)\) 行となります。ターゲットは最後の行 (\(C+1\) 行目、0-indexedで \(C\) 行目) の最後の列 (\(L\) 列目、0-indexedで \(L-1\) 列目) にあるため、数式(2)の \(C-1\) は \(C\) に、 \(C\) は \((C+1)\) に読み替えるのが適切です。上記のHTMLではこの解釈で記述しています。</p>
<p>\(M(i,j)=1\) は、そのピクセルがマスクされ（隠され）、修復モデルによって生成されることを示します。数式(2)は、グリッド画像の<span class="highlight">最後の行（クエリ行）の最後の列（ターゲット画像の位置）</span>の領域をマスクすることを意味します。</p>
</div>
<div class="bubble-box">
<p>🧠 <strong>訓練時の工夫: リバース生成の促進</strong></p>
<p>訓練中、VisualClozeはターゲット画像だけでなく、<span class="highlight">最初の \(L-1\) 個のグリッド（条件画像部分）のいずれか一つを確率0.5でランダムにマスク</span>します。これは、モデルが「結果から原因を推測する」ような<span class="keyword">リバース生成 (Reverse Generation)</span>能力（セクション5.1で詳述）を獲得するのを助けます。</p>
<p>例えば、「猫の絵」と「スケッチ」から「リアルな猫の画像」を生成するタスクの場合、</p>
<ul>
<li>通常: [猫の絵] [スケッチ] [<u>空白</u>] → [リアルな猫] を生成</li>
<li>リバース生成の学習: [猫の絵] [<u>空白</u>] [リアルな猫] → [スケッチ] を生成、または [<u>空白</u>] [スケッチ] [リアルな猫] → [猫の絵] を生成</li>
</ul>
</div>
<img alt="Reverse Generation Example" class="figure-image" src="in_context_learning_reverse_generation.jpg"/>
<p class="reference" style="text-align:center; font-style: italic;">図: リバース生成の例 (論文Fig.9より引用・再構成)</p>
<p>推論時には、生成された大きなグリッド画像 \(\hat{X}\) からターゲット画像の部分を切り出すことで、目的の画像を簡単に得ることができます。</p>
<div class="glass-card">
<p class="subsection-title" style="color: var(--color-accent1); border-color: var(--color-accent1);"><i class="fas fa-link"></i>Aligned Optimization Objective ~整合した最適化目標~</p>
<p>この設計の重要な利点は、VisualClozeの定式化が、一般的な画像修復モデルと<span class="highlight">非常に整合性の高い最適化目標</span>を共有していることです。しかも、アーキテクチャの変更や明示的な入力条件の追加は不要です。</p>
<p>この整合性により、以下が可能になります：</p>
<ul>
<li>✅ 先進的な既存の画像修復モデルを、構築した新しいデータセット（Graph200K）で直接ファインチューニングできる。</li>
<li>✅ 基盤モデル（Foundation Model）が持つ事前知識を最大限に活用できる。</li>
</ul>
<p>対照的に、既存のタスク特化型モデルでは、しばしば追加の学習可能なモジュールを導入したり[38, 69]、追加の条件入力に適応させたり[61]する必要があり、これはモデル本来の能力を損なう可能性があります。</p>
</div>
<div class="glass-card">
<p class="subsection-title" style="color: var(--color-accent2); border-color: var(--color-accent2);"><i class="fas fa-language"></i>Language Instructions ~言語指示~</p>
<p>VisualClozeにおいても、言語指示の設計は依然として必要です。その役割は以下の通りです：</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card">
<p><strong>1. レイアウト指示 (Layout Instruction)</strong> 📐</p>
<p>グリッド画像の \((C+1) \times L\) レイアウトを記述します（論文では \((C+1) \times W\) とありますが、列数はLが適切）。つまり、何行何列の画像グリッドなのかを伝えます。</p>
</div>
<div class="info-card">
<p><strong>2. タスク指示 (Task Instruction)</strong> 🎯</p>
<p>実行すべきタスクの種類を指定します。例えば、「画像を編集する」「スタイルを変換する」など。</p>
</div>
<div class="info-card">
<p><strong>3. コンテンツ指示 (Content Instruction)</strong> 📝</p>
<p>生成すべきターゲット画像の内容を記述します。例えば、「赤いリンゴのある静物画」など。これは、特にインコンテキスト例が利用できない場合にタスクの意図を特定するために重要です。</p>
</div>
</div>
<p>これらの指示の具体的な詳細については、論文の付録Aで説明されています。</p>
<p>式(1)における3つの構成要素 \(X\) (連結画像), \(T\) (言語指示), \(M\) (マスク) を再構成することにより、VisualClozeは、一般的な画像修復パラダイムを用いた<span class="highlight">画像生成のための統一マルチタスクフレームワーク</span>を実現し、<span class="keyword">インコンテキスト学習</span>をサポートします。</p>
</div>
<div class="glass-card">
<p class="subsection-title" style="color: var(--color-accent3); border-color: var(--color-accent3);"><i class="fas fa-map-marker-alt"></i>Positional Embedding ~位置埋め込み~</p>
<p>前述の通り、すべての画像はグリッドレイアウト画像に連結され、この大きな画像に対して<span class="keyword">位置埋め込み (Positional Embedding)</span>（例: RoPE [57]）を適用できます。これは、画像内の各パッチやピクセルの相対的または絶対的な位置情報をモデルに伝えるための重要な技術です。</p>
<p>しかし、アスペクト比が異なるインコンテキスト例からグリッド画像を構成する場合、潜在的な制約が生じます。単純に連結すると、歪みが生じたり、位置情報がうまく伝わらなかったりする可能性があります。</p>
<p>この問題に対処するため、VisualClozeは <span class="highlight">FLUX.1-Fill-dev</span> モデルに実装されている <span class="keyword">3D-RoPE (3次元ロータリー位置埋め込み)</span> を活用します。具体的には、図5で示されているように、クエリとインコンテキスト例を<span class="highlight">時間軸方向 (Temporal Dimension) に連結</span>します。これは、各タスク例（インコンテキスト例やクエリ、それぞれが水平方向に連結された画像列）を、あたかもビデオフレームのように積み重ねるイメージです。</p>
<div class="bubble-box">
<p>💡 <strong>3D-RoPEによる解決</strong></p>
<p>この3D-RoPEを用いることで、アスペクト比の異なる画像を効果的に処理し、性能を低下させることなくこの課題を克服します。時間軸方向の連結は、各画像ストリップ（1つのタスク例）を1つの「時間ステップ」として扱い、それらの間の順序関係と、各ストリップ内の空間的な位置関係の両方を捉えることができます。</p>
</div>
<p>これは図5で説明した連結方法と一致しており、インコンテキスト例とクエリの各行（L個の画像が水平連結されたもの）を縦に積み重ねることで、アスペクト比の不一致に対応します。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-tools"></i>4.3. Implementation Details ~実装詳細~</h3>
<div class="content-box">
<p>このセクションでは、VisualClozeモデルの具体的な実装の詳細について説明します。</p>
<div class="pipeline">
<div class="pipeline-step">
<p><span class="badge blue">基盤モデル (Foundation Model)</span></p>
<p><span class="keyword">FLUX.1-Fill-dev</span> [33] を使用します。これは、オープンソースの画像修復モデルの中でも卓越した性能を持つため選ばれました。</p>
</div>
<div class="pipeline-step">
<p><span class="badge orange">ファインチューニング手法</span></p>
<p>モデル全体をファインチューニングするのではなく、<span class="keyword">LoRA (Low-Rank Adaptation)</span> [25] を用いてファインチューニングを行います。これにより、以下の利点があります：</p>
<ul>
<li><i class="fas fa-coins" style="color: var(--color-accent3);"></i> 訓練コストの削減</li>
<li><i class="fas fa-shield-alt" style="color: var(--color-accent1);"></i> 基盤モデルの能力の維持</li>
<li><i class="fas fa-plug" style="color: var(--color-accent2);"></i> 生成されたLoRAは、コミュニティの他のLoRAと融合可能で、より広範な応用が可能</li>
</ul>
<p>LoRAのランク (rank) は <span class="highlight">256</span> に設定しました。</p>
</div>
<div class="pipeline-step">
<p><span class="badge green">訓練設定</span></p>
<ul>
<li><span class="keyword">イテレーション数</span>: 20,000回</li>
<li><span class="keyword">累積バッチサイズ</span>: 64</li>
<li><span class="keyword">使用GPU</span>: 8 × A100 GPU</li>
<li><span class="keyword">オプティマイザ</span>: AdamW</li>
<li><span class="keyword">学習率</span>: \(1 \times 10^{-4}\) (1e-4)</li>
<li><span class="keyword">ノイズ戦略</span>: FLUX.1-Fill-dev に倣い、動的時間シフト (dynamic time shifting) を伴う <span class="highlight">lognormノイズ戦略</span> を採用。</li>
</ul>
</div>
<div class="pipeline-step">
<p><span class="badge purple">データパラメータ (訓練時)</span></p>
<ul>
<li><span class="keyword">インコンテキスト例の数 (\(C\))</span>: 最大2つ (Sec. 4.2で定義)。つまり、\(C=0, 1, 2\) のいずれか。</li>
<li><span class="keyword">1タスクあたりの画像数 (\(L\))</span>: Graph200Kデータセット内で2から4の間で変動。つまり、条件画像の数は1～3枚。</li>
</ul>
<div class="note-box">
<p class="note-title"><i class="fas fa-info-circle"></i>推論時のインコンテキスト例</p>
<p>訓練時は最大2つですが、推論時にはより多くのインコンテキスト例に一般化（対応）可能です。</p>
</div>
</div>
<div class="pipeline-step">
<p><span class="badge yellow" style="color: var(--color-dark);">画像処理</span></p>
<ul>
<li><span class="keyword">リサイズ</span>: 計算効率とのバランスを取るため、各画像はグリッドレイアウトに連結される前に、面積が <span class="highlight">\(384 \times 384\)</span> または <span class="highlight">\(512 \times 512\)</span> ピクセルになるようにリサイズされます。</li>
<li><span class="keyword">高解像度出力</span>: 実用的なアプリケーションでは、単純な<span class="highlight">後処理アップスケーリング技術</span> [45] を用いることで高解像度の出力を得ることができます。</li>
</ul>
</div>
</div>
<div class="summary-box bubble-box" style="border-color: var(--color-secondary); margin-top: 20px;">
<p class="summary-title" style="font-weight: bold; color: var(--color-secondary); font-family: 'Yomogi', cursive; font-size: 1.2em;">⚙️ 実装のポイントまとめ ⚙️</p>
<ul class="unstyled-list">
<li><span class="badge blue">基盤</span>: FLUX.1-Fill-dev</li>
<li><span class="badge orange">学習</span>: LoRA (rank 256), 2万イテレーション, バッチ64, AdamW, LR 1e-4</li>
<li><span class="badge green">データ</span>: インコンテキスト例 最大2, タスク画像数 2-4</li>
<li><span class="badge purple">画像サイズ</span>: 384² or 512²相当にリサイズ後連結</li>
</ul>
</div>
</div>
</div>
<div class="section-card" id="5._Experiments">
<h2 class="section-title"><i class="fas fa-flask"></i> 5. Experiments (実験)</h2>
<div class="content-box">
<p>このセクションでは、提案する画像生成フレームワーク <span class="keyword">VisualCloze</span> の中核技術である<span class="highlight">視覚的インコンテキスト学習 (Visual In-context Learning)</span> の有効性を、様々な実験を通して詳細に検証します。特に、モデルが訓練データには含まれていなかった<span class="keyword">未知のタスク (unseen tasks)</span> に対してどれだけ柔軟に対応できるか（汎化性能）に焦点を当てています。</p>
<p>主な目的は以下の通りです：</p>
<ul class="fa-ul">
<li><span class="fa-li"><i class="fas fa-check-circle" style="color:var(--color-accent1);"></i></span>VisualClozeが、多様な既知の画像生成タスクを高精度に実行できることを示す。</li>
<li><span class="fa-li"><i class="fas fa-check-circle" style="color:var(--color-accent1);"></i></span>VisualClozeが、未知のタスクに対しても適応し、画像を生成できることを実証する。</li>
<li><span class="fa-li"><i class="fas fa-check-circle" style="color:var(--color-accent1);"></i></span>複数のタスクを統合したり、通常とは逆方向の生成タスク（例：結果画像から条件画像を推測）も可能であることを示す。</li>
</ul>
<p>これらの検証を通じて、VisualClozeがより汎用的で強力な画像生成フレームワークであることを明らかにします。</p>
</div>
<h3 class="section-title"><i class="fas fa-microscope"></i> 5.1. Qualitative Analysis of In-context Learning (インコンテキスト学習の定性的分析)</h3>
<div class="content-box">
<p>このサブセクションでは、<span class="keyword">インコンテキスト学習</span>が様々なタスク、特に訓練時に遭遇しなかった<span class="highlight">未知のタスク (unseen tasks)</span> において、どれほど効果的であるかを一連の実験を通じて具体的に示します。広範な実験結果に基づき、インコンテキスト学習の重要な役割を浮き彫りにする<span class="badge yellow">5つの主要な発見 (Findings)</span>をまとめました。これらの発見は、インコンテキスト学習が単に既存タスクの性能を向上させるだけでなく、モデルの汎用性や応用範囲を大きく広げる可能性を秘めていることを示唆しています。</p>
</div>
<h4 class="section-title"><i class="fas fa-puzzle-piece"></i> In-Context Learning Findings 1: <span class="highlight">既知タスクにおけるタスク曖昧性の軽減</span></h4>
<div class="content-box">
<div class="bubble-box">
<p><span class="badge orange">発見1</span> インコンテキスト学習は、訓練済みのタスク（<span class="keyword">seen tasks</span>）においてモデルがタスクの意図を誤解してしまう「<span class="keyword">タスクの曖昧さ (task confusion)</span>」を軽減する効果があります。</p>
</div>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-question-circle"></i> タスクの曖昧さとは？</div>
<p>モデルが、特に<span class="keyword">密な予測タスク (dense prediction tasks)</span>、例えば各ピクセルに対して値を予測するようなタスク（セグメンテーション、深度推定など）において、実行すべきタスクの目的を正確に理解できず、混乱してしまう現象を指します。これにより、期待しない出力やノイズの多い結果が生じることがあります。</p>
</div>
<p>インコンテキスト学習は、タスク固有の<span class="highlight">デモンストレーション (お手本例)</span> をモデルに提示することで、この問題を効果的に解消します。具体例を見てみましょう。</p>
<img alt="Figure 6: In-context learning mitigates task ambiguity in seen tasks." src="in_context_learning_mitigating_task_ambiguity.jpg"/>
<p class="reference">図6: インコンテキスト学習による既知タスクにおけるタスク曖昧性の軽減効果 (異なる初期ノイズを用いた3つの結果を表示)</p>
<div class="info-grid">
<div class="info-card">
<h5>図6の解説 <i class="fas fa-search-plus"></i></h5>
<p>この図は、いくつかの既知タスクにおいて、インコンテキスト学習の有無で出力がどう変わるかを示しています。「Without In-context Learning」が手がかりなし、「+ In-context Learning」が手がかりありの場合です。</p>
<ul>
<li><strong>(a) ポーズ推定 (Image to Pose)</strong> と <strong>(c) エッジ検出 (Image to Edge)</strong>: <br/>インコンテキスト例がない場合、生成結果にノイズが多かったり、不鮮明だったりします。しかし、インコンテキスト例（お手本）を提示すると、<span class="highlight">性能と安定性が向上</span>し、よりクリーンで正確な結果が得られています。</li>
<li><strong>(b) 深度推定 (Image to Depth)</strong>: <br/>インコンテキスト例がない場合、特に遠方の領域で不正確な深度推定が見られます。インコンテキスト例を用いることで、<span class="highlight">推定精度が改善</span>されています。</li>
<li><strong>(d) 条件付き生成 (Normal to Image)</strong>: <br/>このタスクでは、インコンテキスト例がなくても比較的良好な結果を安定して生成できています。しかし、論文中の表1 (Tab. 1) で示される定量的な比較結果を見ると、インコンテキスト学習を用いることで<span class="highlight">さらにタスク完了の精度が向上する</span>ことが確認されています。</li>
</ul>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-lightbulb"></i> ポイント</div>
<p>インコンテキスト学習は、モデルに「今やるべきタスクはこれだよ」と具体例で教えることで、モデルの迷いをなくし、より高品質で安定した結果を引き出すことができる、というわけです。</p>
</div>
</div>
<h4 class="section-title"><i class="fas fa-lightbulb"></i> In-Context Learning Findings 2: <span class="highlight">未知タスクへの汎化と精度向上</span></h4>
<div class="content-box">
<div class="bubble-box">
<p><span class="badge orange">発見2</span> インコンテキスト学習は、タスクの曖昧さを軽減するだけでなく、モデルが訓練データに含まれていない<span class="keyword">未知のタスク (unseen tasks)</span> へも対応できる<span class="keyword">汎化能力</span>をサポートします。さらに、提供するインコンテキスト例の数を増やすことで、より正確な画像生成が可能になる傾向があります。</p>
</div>
<p>これは、モデルが未知の課題に直面した際に、少数の例題からタスクのパターンを学習し、応用する能力を持つことを意味します。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-cogs"></i> 未知タスクへの汎化とは？</div>
<p>モデルが訓練データで学習していない種類のタスクや、見たことのないパターンの入力に対しても、適切に対応し、期待される出力を生成する能力のことです。インコンテキスト学習は、この汎化能力を大きく引き出すことができます。</p>
</div>
<p>論文の図2 (Fig. 2) では、側面から撮影された顔画像から正面顔を生成するタスクや、特定の編集指示を画像に適用するタスク（[8]のEdit transferなど）を、インコンテキスト学習によって訓練未経験ながら成功させている例が示されています。</p>
<p>さらに、以下のような未知のタスクの例も紹介されています:</p>
<img alt="Figure 7: Unseen Tasks - Environment and Attribute Modification" src="unseen_tasks_attribute_environment_modification.jpg"/>
<p class="reference">図7: 未知のタスクの例。左: 環境変更 (冬景色へ)、右: 属性変換 (サングラスの色を緑へ)。</p>
<div class="info-grid">
<div class="info-card">
<h5>図7の解説 <i class="fas fa-image"></i></h5>
<p>この図は、モデルが訓練時に学習した画像編集タスクが「オブジェクトの追加・削除」のみであったにも関わらず、インコンテキスト学習を通じて他の種類の編集タスクにも汎化できることを示しています。</p>
<ul>
<li><strong>左 (環境変更)</strong>: 元の画像に対し、「設定を冬のシーンに変える」という指示をインコンテキスト例と共に与えることで、背景が冬景色に変更されています。</li>
<li><strong>右 (属性変換)</strong>: 元の画像に対し、「サングラスの色を緑に変える」という指示とインコンテキスト例を与えることで、サングラスの色が変更されています。</li>
</ul>
<p class="highlight">これらのタスクは訓練データには含まれていなかったものです。</p>
</div>
</div>
<img alt="Figure 8: Unseen Tasks - Multi-subject driven generation" src="multisubject_driven_generation.jpg"/>
<p class="reference">図8: 未知のタスクの例。複数被写体のアイデンティティを保持した画像生成。(拡大して見るとより詳細が分かります)</p>
<div class="info-grid">
<div class="info-card">
<h5>図8の解説 <i class="fas fa-users"></i></h5>
<p>この図では、モデルが単一被写体の画像生成タスクのみで訓練されたにもかかわらず、インコンテキスト学習を用いることで、<span class="highlight">複数の被写体のアイデンティティ (顔の特徴など) を保持した画像を生成</span>できています。各行の左側にある複数の人物画像を参考に、右側のターゲット画像を生成していると考えられます。</p>
<p>これは、インコンテキスト学習が個々の被写体の特徴を捉え、それらを組み合わせるという複雑なタスクにも対応できる可能性を示しています。</p>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fas-paper-plane"></i> まとめ</div>
<p>これらの結果は、インコンテキスト学習が非常に効果的な<span class="keyword">誘導メカニズム (guidance mechanism)</span>であり、モデルを再訓練することなく新しいタスクに適応させることを可能にすることを示しています。まさに、少数の例から学ぶ人間の能力に似ていますね。</p>
</div>
</div>
<h4 class="section-title"><i class="fas fa-project-diagram"></i> In-Context Learning Findings 3: <span class="highlight">タスク統合 (未知の戦略)</span></h4>
<div class="content-box">
<div class="bubble-box">
<p><span class="badge orange">発見3</span> インコンテキスト学習は、<span class="keyword">タスク統合 (task unification)</span> という、訓練時には見られなかった戦略を可能にします。これは、複数のサブタスクを単一のステップに統合し、中間結果を含めて生成する能力です。</p>
</div>
<p>これは、複数の異なるタスクを一度の指示で実行できることを意味し、より効率的で複雑な画像生成への道を開きます。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-layer-group"></i> タスク統合とは？</div>
<p>従来は個別に実行する必要があった複数のタスク（例えば、深度推定してから画像生成、エッジを検出してから画像生成など）を、インコンテキスト学習の手がかりによって、一度の処理でまとめて実行してしまうことです。これも一種の<span class="highlight">未知のタスク (unseen task)</span> と見なせます。</p>
</div>
<p>論文の図3 (Fig. 3) では、2つの例が示されています:</p>
<img alt="Figure 3: Unseen Tasks - Task Unification" src="unseen_tasks_task_unification.jpg"/>
<p class="reference">図3: 未知のタスクの例。インコンテキスト学習を利用して、複数の既知タスクを単一ステップの未知タスクに統合。左: [深度から画像生成]と[再照明]タスクを単一の[様々な照明の深度から画像生成]タスクに統合。右: 複数の密な予測タスクを共同予測タスクに統合。 (視覚的コンテキストなしの結果は付録参照)</p>
<div class="info-grid">
<div class="info-card">
<h5>図3の解説 <i class="fas fa-sitemap"></i></h5>
<ul>
<li><strong>左側</strong>: <span class="keyword">条件付き生成 (Conditional Generation)</span> と <span class="keyword">再照明 (Relighting)</span> という2つのタスクを統合しています。インコンテキスト例を通じて、「深度情報から画像を生成し、かつ照明条件を変える」という複合タスクを一度に実行しています。具体的には、深度マップ（Visual Prompt）を入力として、異なる照明の画像を複数（Target）生成しています。</li>
<li><strong>右側</strong>: <span class="keyword">深度推定 (Depth Estimation)</span>、<span class="keyword">表面法線推定 (Surface Normal Estimation)</span>、<span class="keyword">エッジ検出 (Edge Detection)</span> を同時に実行しています。入力画像（Visual Prompt）から、深度、法線、エッジの3つの異なる情報を一度に（Targetとして）出力しています。</li>
</ul>
</div>
</div>
<p>同様に、図11では、条件付き生成において複数の条件を組み合わせることで、より細かい制御を達成する方法が示されています。</p>
<img alt="Figure 11: Unseen Tasks - Multiple conditions generation" src="unseen_tasks_multiple_conditions_generation.jpg"/>
<p class="reference">図11: 未知のタスクの例。複数タスクの未知の組み合わせ。条件付き生成では、複数の条件を統合してより精密な制御を実現。その他の例は図3参照。</p>
<div class="info-grid">
<div class="info-card">
<h5>図11の解説 <i class="fas fa-sliders-h"></i></h5>
<p>この図では、例えば人物の画像を生成する際に、キーポイント（骨格構造を示す色付き線）だけでは位置や体のポーズに関する大まかな情報しか得られません。このような場合に、<span class="keyword">輪郭条件 (contour conditions)</span> を追加で使用することで、他の視覚的要素の属性をより精密に制御できることを示しています。図では、キーポイント(IMAGE1)とキャニーエッジ(IMAGE2)を入力として、ターゲット画像(IMAGE3)を生成しています。</p>
<p>タスクプロンプト: 「各行は、[IMAGE1] 骨格構造のための色付き線を持つ人間のポーズと [IMAGE2] 鋭い白いエッジと暗い部分を持つキャニーマップを、論理的なアプローチを通じて [IMAGE3] 視覚的に印象的で鮮明な画像に変換する方法を示しています。」</p>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-compress-arrows-alt"></i> ポイント</div>
<p>インコンテキスト学習は、単にタスクをこなすだけでなく、複数のタスクを組み合わせたり、より複雑な指示を理解したりする能力をモデルに与えることができるのです。</p>
</div>
</div>
<h4 class="section-title"><i class="fas fa-balance-scale"></i> In-Context Learning Findings 4: <span class="highlight">インコンテキスト例の質と効果の差異</span></h4>
<div class="content-box">
<div class="bubble-box">
<p><span class="badge orange">発見4</span> 異なるインコンテキスト学習の例は、生成結果に様々な効果をもたらします。タスクの意図をより良く伝えることができる例を用いることで、より高品質で安定した生成結果が得られます。</p>
</div>
<p>これは、プロンプトエンジニアリングと同様に、インコンテキスト学習においても「お手本」の選び方が重要であることを示唆しています。先行研究 ([46], [52]) でもプロンプト選択の重要性が指摘されていますが、本研究でも同様の傾向が確認されました。</p>
<div class="challenge-box">
<div class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 重要ポイント</div>
<p>インコンテキスト例が、タスクの意図に関して<span class="highlight">正確で強力なガイダンス</span>を提供することが極めて重要です。</p>
</div>
<p>例えば、図10に示されるように、正面顔を生成するタスクにおいて、お手本として提示する横顔の向きが重要になります。</p>
<img alt="Figure 10: Illustration of the impact of different in-context examples." src="in_context_learning_frontal_face_generation.jpg"/>
<p class="reference">図10: 異なるインコンテキスト例がインコンテキスト学習に与える影響の図解。左の2番目の例では、左右の顔が正面に偏りすぎているため、タスク意図の核心を示せていません。</p>
<div class="info-grid">
<div class="info-card">
<h5>図10の解説 <i class="fas fa-user-friends"></i></h5>
<p>この図は、正面顔生成タスクにおけるインコンテキスト例の選択の重要性を示しています。</p>
<ul>
<li><strong>左側の例 (成功率33%)</strong>: 2つのインコンテキスト例 (Two In-context Examples) として提示されている横顔が、かなり正面に近い向きになっています (図中の説明文では "too biased towards the front" と表現)。このような例では、モデルが「横顔から正面顔を生成する」というタスクの核心 ("core goal of the task intention") を捉えにくく、結果として正面顔の生成成功率が大幅に低下しています（3回の試行中、成功は1回のみ）。</li>
<li><strong>右側の例 (成功率100%)</strong>: 提示されている横顔が、より明確に「横向き」であることが分かります。これにより、モデルはタスクの意図を正確に理解し、安定して正面顔を生成できています（3回の試行全て成功）。</li>
</ul>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-chalkboard-teacher"></i> 教訓</div>
<p>「どのようなお手本を見せるか」が、モデルの学習効率やタスク達成度に大きく影響するということです。不適切な例は、かえってモデルを混乱させてしまう可能性もあります。</p>
</div>
</div>
<h4 class="section-title"><i class="fas fa-exchange-alt"></i> In-Context Learning Findings 5: <span class="highlight">双方向生成 (訓練未経験の逆プロセスも)</span></h4>
<div class="content-box">
<div class="bubble-box">
<p><span class="badge orange">発見5</span> インコンテキスト学習は、<span class="keyword">双方向の生成 (bilateral generation)</span> を誘導することができます。これには、訓練時には経験していない<span class="highlight">逆方向のプロセス</span>も含まれます。</p>
</div>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-retweet"></i> 双方向生成と逆プロセスとは？</div>
<p><strong>双方向生成</strong>: 通常の画像生成（条件からターゲットを生成）だけでなく、その逆のプロセス（ターゲットから条件を推測）も可能であることを指します。</p>
<p><strong>逆プロセス</strong>: 例えば、通常は「スケッチ画像からリアルな画像を生成する」というタスクに対し、「リアルな画像から元のスケッチ画像を推測する」といった、生成方向が逆のタスクのことです。</p>
</div>
<p>モデルは、与えられた一連の条件からターゲット画像を生成するだけでなく、ターゲット画像からその元となった条件を推測する能力も示します。訓練時には、Sec. 4.2で述べられているように、条件画像のうちの1つをランダムにターゲットとして扱うことで、ある程度の逆生成能力を学習していますが、推論時にはさらに困難で訓練未経験の状況、つまり<span class="highlight">ターゲット画像のみから全ての条件画像を推測する</span>というタスクにも汎化することができます。</p>
<p>具体的な例を見てみましょう。</p>
<img alt="Figure 9: Unseen Tasks - Reverse generation" src="in_context_learning_reverse_generation.jpg"/>
<p class="reference">図9: 未知のタスクの例。インコンテキスト学習を通じて、ターゲットから条件への逆生成を実行可能。例: (a) スタイル化画像からレイアウトとスタイルを分解、(b) エッジマップから画像、深度、表面法線を同時に推測（図3左の逆タスク）。</p>
<div class="info-grid">
<div class="info-card">
<h5>図9の解説 <i class="fas fa-undo-alt"></i></h5>
<ul>
<li><strong>(a) 左側 (スタイル化画像からの分解)</strong>:<br/>
                    タスクプロンプト: 「各行は、[IMAGE1] 鮮明なエッジを持つグレー階調の深度マップ、[IMAGE2] 鮮やかなディテールを持つ芸術的にレンダリングされたコンテンツを用いて、[IMAGE3] ユニークなアートスタイルの高精細な画像を生成する方法を示しています。」<br/>
                    図では、スタイル化された画像（ターゲット、図では右端の画像）が与えられた場合に、元の画像（IMAGE1に相当）とスタイル参照画像（IMAGE2に相当）の両方を<span class="highlight">逆算して生成</span>できています。これは、モデルがコンテンツ表現とスタイル表現を分離する能力を持っていることを示しています。</li>
<li><strong>(b) 右側 (エッジマップからの多情報推測)</strong>:<br/>
                    タスクプロンプト: 「各行は、[IMAGE1] 詳細な画像、[IMAGE2] 明確なオブジェクト境界を持つグレースケールの深度マップ、[IMAGE3] バンプマッピング効果のためのRGB法線マップ、[IMAGE4] 論理的なアプローチによるソフトエッジの輪郭検出マップに変換する方法を示しています。」 (図では、このプロンプトの逆のタスクを実行している点に注意)<br/>
                    エッジ画像（ターゲット、図では右端の画像）から、対応する実画像、深度推定、表面法線推定を生成しています。これは、図3の左側に示されたタスク（深度、法線、エッジから画像を生成）の<span class="highlight">完全な逆タスク</span>に相当します。</li>
</ul>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-infinity"></i> 意義</div>
<p>このような逆タスクを実行できる能力は、VisualClozeが異なる種類の画像表現間の複雑な関係性を理解する上での<span class="keyword">柔軟性と頑健性</span>を強調しています。単にAからBを生成するだけでなく、BからAを推測できるというのは、より深い理解がある証拠と言えるでしょう。</p>
</div>
</div>
<h3 class="section-title"><i class="fas fa-chart-bar"></i> 5.2. Main Results (主要な結果)</h3>
<div class="content-box">
<p>このセクションでは、提案手法 (VisualCloze) の性能を、既存の<span class="keyword">汎用生成モデル (universal generative models)</span> や<span class="keyword">特化型モデル (specialized models)</span> と比較し、定量的に評価します。</p>
<div class="framework-box">
<div class="framework-title"><i class="fas fa-balance-scale-left"></i> 比較対象モデル</div>
<ul class="fa-ul">
<li><span class="fa-li"><i class="fas fa-globe"></i></span><strong>汎用モデル:</strong>
<ul>
<li>OmniGen [71]</li>
<li>OneDiffusion [35]</li>
</ul>
</li>
<li><span class="fa-li"><i class="fas fa-star"></i></span><strong>特化型モデル:</strong>
<ul>
<li>ControlNet [80]</li>
<li>OminiControl [61]</li>
</ul>
</li>
</ul>
</div>
<p>評価指標の詳細は付録Cで説明されています。さらに、比較のために<span class="keyword">FLUX.1-dev [33]</span> をFLUX.1-Fill-devと同じ設定でファインチューニングしたモデルも用意し、これらをそれぞれ <span class="highlight">$\mathrm { Ours } _ { \mathrm { dev } }$</span> および <span class="highlight">$\mathrm { Ours } _ { \mathrm { fill } }$</span> と呼びます。$\mathrm { Ours } _ { \mathrm { dev } }$ の詳細は付録Bに記載されています。</p>
<div class="glass-card">
<h4><i class="fas fa-images"></i> 条件付き生成と画像修復タスク</h4>
<p>これらのタスクでは、OminiControl [61] の評価アプローチに従い、以下の3つの基準でモデルを評価します。</p>
<ul class="tag-list">
<li class="tag">制御性 (Controllability)</li>
<li class="tag">視覚的品質 (Visual Quality)</li>
<li class="tag">テキスト整合性 (Text Consistency)</li>
</ul>
<p>下の表1 (Tab. 1) に結果を示します。</p>
<img alt="Table 1: Quantitative comparison on conditioning generation and image restoration." src="table1.png"/>
<p class="reference">表1: 条件付き生成と画像修復に関する定量的比較。各タスクに特化したモデルは灰色でマークされています。それ以外のモデルの中で、最良の手法は太字で、次善の手法は下線で示されています。</p>
<div class="info-grid">
<div class="info-card">
<h5>表1の読み方と結果 <i class="fas fa-table"></i></h5>
<p>表1は、Cannyエッジからの画像生成 (Canny-to-Image)、深度マップからの画像生成 (Depth-to-Image)、画像鮮明化 (Deblurring)、ノイズ除去 (Denoising)、超解像 (Super Resolution) といったタスクにおける性能を示しています。</p>
<p>指標の例:</p>
<ul>
<li><strong>FID $\downarrow$</strong>: Frechet Inception Distance (低いほど良い) - 生成画像の品質と多様性</li>
<li><strong>SSIM $\uparrow$</strong>: Structural Similarity Index (高いほど良い) - 構造的類似性</li>
<li><strong>RMSE $\downarrow$</strong>: Root Mean Square Error (低いほど良い) - 元画像との誤差</li>
<li><strong>F1 Score $\uparrow$</strong>: (高いほど良い) - 制御性 (Canny)</li>
<li><strong>CLIP Score $\uparrow$</strong>: (高いほど良い) - テキストとの整合性</li>
</ul>
<p class="highlight">VisualCloze ($\mathrm { Ours } _ { \mathrm { fill } }$) は、既存の汎用手法と同等の制御性を持ちながら、視覚的品質とテキスト整合性において優れた性能を示しています。特化型モデルと比較しても、最良の結果と同等であり、特に<span class="keyword">深度からの画像生成 (depth-to-image) タスクでは特化型モデルを上回っています</span>。</p>
</div>
</div>
</div>
<div class="glass-card">
<h4><i class="fas fa-palette"></i> スタイル変換タスク</h4>
<p>スタイル変換タスクでは、CLIP [49] モデルを使用して、<span class="keyword">テキスト整合性 (Text Consistency)</span> と<span class="keyword">スタイル整合性 (Style Alignment)</span> を測定します。結果は表3 (Tab. 3) に示されています。</p>
<img alt="Table 3: Quantitative comparison for style transfer." src="table3.png"/>
<p class="reference">表3: スタイル変換の定量的比較。CLIPスコアでテキスト整合性とスタイル整合性を報告。特化型モデルは灰色で示されています。その他の中で、最も性能の高いものが太字で、2番目に優れたものが下線で示されています。</p>
<div class="info-grid">
<div class="info-card">
<h5>表3の読み方と結果 <i class="fas fa-table"></i></h5>
<p>この表は、テキストプロンプトに基づいて画像のスタイルを変換するタスクの性能を示しています。</p>
<p>指標:</p>
<ul>
<li><strong>CLIP-T (Text) $\uparrow$</strong>: 生成画像とテキストプロンプトの整合性 (高いほど良い)</li>
<li><strong>CLIP-S (Style) $\uparrow$</strong>: 生成画像と参照スタイル画像の整合性 (高いほど良い)</li>
</ul>
<p class="highlight">VisualCloze ($\mathrm { Ours } _ { \mathrm { fill } }$) は、OmniGen [71] と比較して、テキスト整合性で<span class="badge green">2%</span>、スタイル整合性で<span class="badge green">3%</span> 向上しています。特化型モデルであるInstantStyle-Plus [81] と比較しても、テキスト整合性で<span class="badge green">2%</span> の改善を達成し、スタイル整合性もわずかな低下に留まっています。</p>
</div>
</div>
</div>
<div class="glass-card">
<h4><i class="fas fa-user-astronaut"></i> 被写体駆動型画像生成タスク</h4>
<p>被写体駆動型画像生成タスク (Subject-driven Image Generation) では、DINOv2 [47]、CLIP-I [49]、CLIP-T [49] スコアを用いて<span class="keyword">意味的整合性 (Semantic Alignment)</span> を評価します。結果は表2 (Tab. 2) に示されています。</p>
<img alt="Table 2: Quantitative comparison for subject-driven image generation." src="table2.png"/>
<p class="reference">表2: 被写体駆動型画像生成の定量的比較。テキスト整合性とスタイル整合性に関するクリップスコアを報告。専門家は灰色で陰影付けされています。残りのメソッドの中で、最良のものは太字で強調表示され、2番目に優れたものは下線が引かれています。</p>
<div class="info-grid">
<div class="info-card">
<h5>表2の読み方と結果 <i class="fas fa-table"></i></h5>
<p>この表は、特定の被写体（例：特定の人物や物）の特徴を保持したまま新しい画像を生成するタスクの性能を示しています。</p>
<p>指標:</p>
<ul>
<li><strong>DINOv2 $\uparrow$</strong>: 参照被写体と生成画像の整合性 (高いほど良い)</li>
<li><strong>CLIP-I (Image) $\uparrow$</strong>: 参照被写体画像と生成画像の整合性 (高いほど良い)</li>
<li><strong>CLIP-T (Text) $\uparrow$</strong>: 生成画像とテキストプロンプトの整合性 (高いほど良い)</li>
</ul>
<p class="highlight">VisualCloze ($\mathrm { Ours } _ { \mathrm { fill } }$) は、これらの全ての指標で一貫して改善を示しています。例えば、特化型モデルOminiControl [61] と比較して、これら3つのスコアでそれぞれ<span class="badge green">7.15%</span>、<span class="badge green">1.66%</span>、<span class="badge green">1.48%</span> の改善を達成しています。</p>
</div>
</div>
</div>
<div class="framework-box">
<div class="framework-title"><i class="fas fa-star-half-alt"></i> インフィリングモデルの利点 ($\mathrm { Ours } _ { \mathrm { fill } }$ vs $\mathrm { Ours } _ { \mathrm { dev } }$)</div>
<p>提案手法 ($\mathrm { Ours } _ { \mathrm { fill } }$) は、<span class="keyword">FLUX.1-Fill-dev [33]</span> という画像インフィリング（穴埋め）モデルをベースに構築されています。このモデルは、我々の提案する統一的画像生成フレームワークの目的と非常に親和性が高いです。</p>
<p>この有効性を検証するため、FLUX.1-Fill-devの代わりに、一般的なテキストから画像を生成するモデルである<span class="keyword">FLUX.1-dev [33]</span> を同じ設定でファインチューニングしたモデル ($\mathrm { Ours } _ { \mathrm { dev } }$) とも比較します。</p>
<p>$\mathrm { Ours } _ { \mathrm { fill } }$ はアーキテクチャの変更が不要であるのに対し、$\mathrm { Ours } _ { \mathrm { dev } }$ は汎用画像生成に対応するためにモデルの適応が必要でした（詳細は付録B）。そのシンプルさにもかかわらず、$\mathrm { Ours } _ { \mathrm { fill } }$ は複数のタスクで優れた性能を達成しています。</p>
<div class="two-column">
<div class="column">
<p><strong>表1 (条件生成・修復) の比較:</strong></p>
<ul class="fa-ul">
<li><span class="fa-li"><i class="fas fa-arrow-up" style="color:var(--color-accent1);"></i></span>Cannyからの画像生成では、$\mathrm { Ours } _ { \mathrm { dev } }$ がF1スコアで$\mathrm { Ours } _ { \mathrm { fill } }$ を上回りました。</li>
<li><span class="fa-li"><i class="fas fa-arrow-down" style="color:var(--color-secondary);"></i></span>しかし、他のタスクでは $\mathrm { Ours } _ { \mathrm { fill } }$ が大きな優位性を示しました。
                            <ul>
<li>深度からの画像生成: $\mathrm { Ours } _ { \mathrm { fill } }$ はRMSEを25.06から<span class="highlight">10.31</span>に大幅削減。</li>
<li>画像鮮明化: $\mathrm { Ours } _ { \mathrm { fill } }$ はRMSEを低減しつつ高いSSIMを維持し、高品質を達成。</li>
</ul>
</li>
</ul>
</div>
<div class="column">
<p><strong>表2 (被写体駆動型生成) の比較:</strong></p>
<ul class="fa-ul">
<li><span class="fa-li"><i class="fas fa-check" style="color:var(--color-accent1);"></i></span>$\mathrm { Ours } _ { \mathrm { fill } }$ は一貫して $\mathrm { Ours } _ { \mathrm { dev } }$ を上回りました。</li>
</ul>
<p><strong>表3 (スタイル変換) の比較:</strong></p>
<ul class="fa-ul">
<li><span class="fa-li"><i class="fas fa-equals" style="color:var(--color-gray);"></i></span>$\mathrm { Ours } _ { \mathrm { fill } }$ は $\mathrm { Ours } _ { \mathrm { dev } }$ と同等の性能を示しました。</li>
</ul>
</div>
</div>
<img alt="Figure 12: Comparison between Ours_dev and Ours_fill" src="flux1_dev_vs_flux1_fill_dev_comparison.jpg"/>
<p class="reference">図12: Flux.1-dev ($\mathrm { Ours } _ { \mathrm { dev } }$) と Flux.1-Fill-dev ($\mathrm { Ours } _ { \mathrm { fill } }$) の比較。</p>
<div class="info-card">
<h5>図12の解説 <i class="fas fa-images"></i></h5>
<p>この図は、$\mathrm { Ours } _ { \mathrm { dev } }$ と $\mathrm { Ours } _ { \mathrm { fill } }$ の視覚的な比較を示しています。$\mathrm { Ours } _ { \mathrm { fill } }$ (右側) が $\mathrm { Ours } _ { \mathrm { dev } }$ (中央) よりも明らかに優れた結果を生成していることが分かります。</p>
<p>特に、<span class="highlight">深度からの画像生成 (Depth to Image、上段)</span> において、$\mathrm { Ours } _ { \mathrm { dev } }$ が生成した画像には頻繁に<span class="keyword">斜めの縞模様アーティファクト</span>が現れ、視覚的な忠実度を著しく低下させています。一方、$\mathrm { Ours } _ { \mathrm { fill } }$ はそのようなアーティファクトがなく、より自然な画像を生成しています。</p>
<p>画像鮮明化 (Deblurring、下段) においても、$\mathrm { Ours } _ { \mathrm { fill } }$ はよりシャープで詳細な結果を生成しています。</p>
<p>性能、視覚的品質、アーキテクチャ効率の利点を考慮すると、<span class="highlight">$\mathrm { Ours } _ { \mathrm { fill } }$ が優れたモデル</span>であると言えます。</p>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-analytics"></i> インコンテキスト学習の定量的比較 (既知タスク)</div>
<p>ここでは、インコンテキスト学習が既知タスクに与える影響をさらに分析します。表1は、インコンテキスト学習が様々な画像生成タスクに与える影響を示しています。</p>
<div class="info-grid">
<div class="info-card">
<p><strong>Cannyエッジ条件の場合 (Canny-to-Image):</strong></p>
<p>インコンテキスト例なしの場合、FIDは30.60でした。2つのインコンテキスト例を用いると、FIDは31.15に<span class="highlight">改善</span>します。(注: FIDは低い方が良い指標ですが、論文では "improves to 31.15" と記載。文脈や他の指標との兼ね合いで改善と解釈されている可能性があります。あるいは、特定の条件下でのわずかな悪化かもしれません。原文の表現を尊重します。)</p>
</div>
<div class="info-card">
<p><strong>深度条件の場合 (Depth-to-Image):</strong></p>
<p>インコンテキスト例の数が増えるにつれて、RMSEは10.31から<span class="highlight">9.68</span>に減少し、構造的な一貫性が向上したことを示しています。</p>
</div>
<div class="info-card">
<p><strong>画像鮮明化タスクの場合 (Deblurring):</strong></p>
<p>同様に、RMSEは26.53から<span class="highlight">25.57</span>に減少し、元コンテンツへの忠実度が向上したことを反映しています。</p>
</div>
</div>
<p>これらの結果は、インコンテキスト学習が効果的な<span class="keyword">誘導メカニズム</span>であり、モデルがタスクの意図により良く整合することを可能にすることを示しています。</p>
</div>
</div>
</div>
<div class="section-card" id="6._Limitations">
<h2 class="section-title"><i class="fas fa-exclamation-triangle"></i> 6. Limitations</h2>
<p style="text-align: center; font-family: 'Yomogi', cursive; font-size: 16px; margin-bottom: 25px; padding: 10px; background-color: rgba(255, 248, 225, 0.7); border-radius: 8px; border-left: 5px solid var(--color-accent3);">
        このセクションでは、提案されているVisualClozeモデルが抱えるいくつかの<span class="keyword">限界点</span>や<span class="keyword">課題</span>について詳しく見ていきます。<br/>
        どんなに優れたモデルでも、完璧ではありません。その弱点を理解することは、今後の改善に繋がる重要なステップです。 📝
    </p>
<div class="arrow-connector" style="margin: 15px 0;"></div>
<div class="content-box">
<p style="margin-bottom: 20px;">
            この論文で提案されたVisualClozeモデルは、多くの面で強力な性能を示していますが、いくつかの限界も存在します。主な課題は、<span class="highlight">特定の状況下での安定性</span>に関連しています。
        </p>
<div class="info-grid" style="grid-template-columns: 1fr; gap: 25px;">
<div class="info-card glass-card">
<h3 class="subsection-title" style="color: var(--color-secondary); border-left-color: var(--color-secondary);"><i class="fas fa-cogs"></i> 課題1: 学習済みタスク (In-domain tasks) における部分的な不安定性</h3>
<p>
                    モデルは、学習データに含まれるタスク (in-domain tasks) のほとんどにおいては<span class="keyword">高い安定性</span>を発揮します。しかし、一部の特定のタスク、例えば<span class="keyword" style="border-bottom-color: var(--color-secondary);">オブジェクト除去 (object removal)</span> 🗑️ といったタスクでは、まだ<span class="highlight">不安定な挙動</span>を示すことがあります。
                </p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(220px, 1fr)); margin-top:15px;">
<div class="feature-item" style="background-color: rgba(92, 184, 92, 0.1); border: 1px dashed var(--color-accent1);">
<i class="fas fa-check-circle fa-2x" style="color: var(--color-accent1); margin-bottom: 10px;"></i>
<strong>多くの学習済みタスク</strong>
<p style="font-size: 13px; color: var(--color-gray);">高い安定性 👍</p>
</div>
<div class="feature-item" style="background-color: rgba(255, 126, 95, 0.1); border: 1px dashed var(--color-secondary);">
<i class="fas fa-exclamation-circle fa-2x" style="color: var(--color-secondary); margin-bottom: 10px;"></i>
<strong>特定のタスク (例: オブジェクト除去)</strong>
<p style="font-size: 13px; color: var(--color-gray);">不安定な場合あり 📉</p>
</div>
</div>
<div class="note-box" style="margin-top: 15px; background-color: rgba(255, 126, 95, 0.05); border-left-color: var(--color-secondary);">
<p class="note-title" style="color: var(--color-secondary);"><i class="fas fa-microscope"></i> この意味するところは？</p>
<p>この結果は、モデルのパフォーマンスが<span class="highlight">タスクの特性 (task characteristics)</span> に敏感であることを示唆しています。つまり、タスクの種類や性質によって、モデルの得意不得意が現れるということです。</p>
</div>
</div>
<div class="info-card glass-card">
<h3 class="subsection-title" style="color: var(--color-secondary); border-left-color: var(--color-secondary);"><i class="fas fa-question-circle"></i> 課題2: 未学習タスク (Unseen tasks) における安定性不足</h3>
<p>
                    さらに、モデルが学習データで見たことのない<span class="keyword">未学習タスク (unseen tasks)</span> 🗺️ に対する安定性は、まだ<span class="highlight">十分とは言えません</span>。新しい、未知のタスクに直面したときの汎用性やロバストさには、まだ改善の余地があると考えられます。
                </p>
<div style="text-align: center; margin: 20px 0; padding: 10px; background-color: rgba(255,224,130,0.2); border-radius: 8px;">
<span style="font-family: 'Yomogi', cursive; font-size: 16px;">
<i class="fas fa-road" style="color: var(--color-accent3);"></i> 未学習タスクへの道は、まだ先が長い… <i class="fas fa-hourglass-half" style="color: var(--color-accent3);"></i>
</span>
</div>
</div>
</div>
<div class="arrow-connector" style="margin: 30px 0;"></div>
<div class="framework-box" style="border-color: var(--color-accent2); background-color: rgba(149, 117, 205, 0.05);">
<h3 class="subsection-title" style="color: var(--color-accent2); border-left-color: var(--color-accent2); margin-top:0;"><i class="fas fa-search-dollar"></i> 不安定性を引き起こす可能性のある要因</h3>
<p>
                このような安定性の問題は、単一の原因によるものではなく、複数の要因が絡み合っていると考えられます。論文では、主に以下の3つの要因が指摘されています。
            </p>
<ul class="unstyled-list" style="margin-top: 15px;">
<li class="process-step" style="margin-bottom: 20px;">
<div class="step-number" style="background-color: var(--color-accent2); min-width: 30px; height: 30px; font-size:16px;">1</div>
<div class="step-content">
<strong class="keyword" style="font-size: 16px;">タスク自体の難易度 (Difficulty of the task)</strong>
<p style="font-size: 14px;">
                            実行しようとしているタスクが本質的に非常に難しい場合、安定した高品質な結果を得ることは一般的に困難です。
                        </p>
<div style="text-align: center; margin-top:8px;">
<i class="fas fa-weight-hanging fa-lg" style="color: var(--color-accent2);"></i> <span style="font-family: 'Kaisei Decol', serif;">高難度タスク ⛰️</span>
</div>
</div>
</li>
<li class="process-step" style="margin-bottom: 20px;">
<div class="step-number" style="background-color: var(--color-accent2); min-width: 30px; height: 30px; font-size:16px;">2</div>
<div class="step-content">
<strong class="keyword" style="font-size: 16px;">既学習タスクとの差異 (Difference with seen tasks)</strong>
<p style="font-size: 14px;">
                            未学習タスクが、モデルが既に学習したタスクと大きくかけ離れている場合、モデルはその新しいタスクにうまく汎化できず、結果が不安定になる可能性があります。
                        </p>
<div style="display: flex; align-items: center; justify-content: space-around; margin-top:10px; font-family: 'Yomogi', cursive; font-size:13px; text-align:center;">
<div style="padding: 5px; background-color: #e6f7ff; border-radius: 5px; box-shadow: 0 1px 2px rgba(0,0,0,0.1);"><i class="fas fa-brain" style="color: var(--color-primary);"></i> 学習済み<br/>タスク群</div>
<i class="fas fa-arrows-alt-h fa-2x" style="color: var(--color-gray); margin: 0 10px;"></i>
<div style="padding: 5px; background-color: #fff0f0; border-radius: 5px; box-shadow: 0 1px 2px rgba(0,0,0,0.1);"><i class="fas fa-lightbulb" style="color: var(--color-secondary);"></i> 未学習タスク<br/>(差異大 → <i class="fas fa-unlink" style="color: var(--color-secondary);"></i> 不安定)</div>
</div>
</div>
</li>
<li class="process-step">
<div class="step-number" style="background-color: var(--color-accent2); min-width: 30px; height: 30px; font-size:16px;">3</div>
<div class="step-content">
<strong class="keyword" style="font-size: 16px;">曖昧な文脈内学習の例 (Ambiguous in-context examples)</strong>
<p style="font-size: 14px;">
                            モデルにタスクの実行方法を教えるための<span class="highlight">文脈内学習の例 (in-context examples)</span> が曖昧であったり、タスクの意図を正確に伝えきれていなかったりする場合も、不安定な結果を生む原因となり得ます。
                        </p>
<div class="bubble-box" style="margin-top: 15px; border-color: var(--color-accent2); background-color: #f3e8ff;">
<p style="font-family: 'Yomogi', cursive; font-size: 14px; line-height: 1.5;">
<i class="fas fa-comment-dots" style="color: var(--color-accent2);"></i> <strong style="color: var(--color-accent2);">補足: Sec. 5.1 での議論</strong> <i class="fas fa-link" style="color: var(--color-accent2); font-size: 0.8em;"></i><br/>
                                論文のセクション5.1では、この「文脈内学習の例」の重要性について触れられています。<br/>
                                例えば、Fig. 10で示されているように、「正面を向いた顔画像を生成する」タスクで、お手本として提示される横顔の画像が実際には正面に近すぎると、モデルはタスクの真の目的（横顔から正面顔への変換）を理解しにくくなり、結果として正しい正面顔を生成する成功率が大幅に低下する、といった現象が報告されています。<br/>
<span style="display:block; text-align:center; margin-top:5px;">
<i class="far fa-images" style="color: var(--color-accent2);"></i> 良い例 <i class="fas fa-long-arrow-alt-right" style="color: var(--color-accent1);"></i> 良い結果 <br/>
<i class="far fa-question-circle" style="color: var(--color-accent2);"></i> 曖昧な例 <i class="fas fa-long-arrow-alt-right" style="color: var(--color-secondary);"></i> 不安定な結果
                                </span>
</p>
</div>
</div>
</li>
</ul>
</div>
<p style="text-align: center; font-style: italic; margin-top: 30px; color: var(--color-gray); font-family: 'Kaisei Decol';">
            これらの限界点を認識し、克服していくことが、VisualClozeのような汎用画像生成フレームワークのさらなる発展に繋がります。
        </p>
</div>
</div>
<div class="section-card" id="7._Conclusion">
<h2 class="section-title"><i class="fas fa-flag-checkered"></i> 7. Conclusion</h2>
<p style="text-align: center; font-style: italic; margin-bottom: 25px;">この論文の締めくくりとして、提案手法 <span class="keyword">VisualCloze</span> が画像生成の分野にどのような貢献をしたのか、その核心を振り返りましょう。✏️</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-rocket"></i> VisualCloze: 普遍的画像生成への新たなアプローチ</p>
<p>本研究では、<strong class="keyword">VisualCloze</strong>という、新しい<span class="highlight">普遍的画像生成フレームワーク</span>を提案しました。このフレームワークは、既存の画像生成手法が直面しているいくつかの重要な課題に取り組むことを目的として設計されています。</p>
<div class="challenge-box" style="margin-top:15px; margin-bottom:15px;">
<p class="challenge-title"><i class="fas fa-bullseye"></i> 主な挑戦と課題</p>
<p>既存手法には、主に以下の3つの大きな課題がありました：</p>
<ul class="unstyled-list" style="list-style-type: '🎯'; padding-left: 25px; margin-top:10px;">
<li style="margin-bottom: 5px;">より<span class="highlight">一般化可能な指示設計</span>（モデルにタスクをどう伝えるか）</li>
<li style="margin-bottom: 5px;">学習効率を高めるための<span class="highlight">適切なタスク分布</span>（どんなタスクをどう学ばせるか）</li>
<li style="margin-bottom: 5px;">多様なタスクを扱える<span class="highlight">統一されたアーキテクチャ設計</span>（どんなモデル構造にするか）</li>
</ul>
</div>
</div>
<div class="arrow-connector"></div>
<p style="margin-top: 20px; text-align:center; font-weight:bold;">VisualClozeは、これらの課題に対し、以下の3つの革新的なアプローチで応えます！💡</p>
<div class="info-grid" style="grid-template-columns: 1fr; gap: 25px; margin-top:20px;">
<div class="info-card" style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(5px);">
<h3 class="subsection-title" style="color: var(--color-accent1); border-left-color: var(--color-accent1);"><i class="fas fa-eye" style="color: var(--color-accent1);"></i> アプローチ1: 視覚的文脈学習 (Visual In-Context Learning) の再提案</h3>
<div class="content-box">
<p>従来の多くの手法がタスクの意図を伝えるために言語ベースの指示（テキストによる命令）に大きく依存していました。しかし、VisualClozeでは<strong class="keyword">視覚的文脈学習 (Visual In-Context Learning)</strong> というアプローチを改めて提案しています。これは、モデルがいくつかの<span class="highlight">視覚的なデモンストレーション（お手本となる画像例）</span>を観察することで、実行すべきタスクを自ら学習することを可能にする手法です。</p>
<div class="bubble-box" style="border-color: var(--color-accent1); margin-top:15px; margin-bottom:15px;">
<p style="text-align: center; font-weight: bold; color: var(--color-accent1);"><i class="fas fa-chalkboard-teacher"></i> 見て学ぶ！「百聞は一見にしかず」</p>
<p>例えば、「この写真の猫を犬に変えて」とテキストで指示する代わりに、いくつかの「猫の画像 → 犬の画像」という変換ペアをモデルに見せます。これにより、モデルは「ああ、こういう変換をすればいいんだな」とタスク内容を直感的に理解します。</p>
</div>
<p><i class="fas fa-thumbs-up" style="color: var(--color-accent1);"></i> このアプローチの主な利点：</p>
<ul class="unstyled-list" style="list-style-type: '✨'; padding-left: 20px;">
<li style="margin-bottom:5px;"><span class="highlight">未知のタスク（学習データに含まれていなかったタスク）への汎化能力が向上</span>します。モデルが見たことのない新しい種類の変換でも、類似のデモンストレーションから類推して対応できるようになります。</li>
<li style="margin-bottom:5px;"><span class="highlight">タスクの曖昧さが減少</span>します。言葉だけでは伝わりにくい複雑なタスクも、具体的な例を見ることで明確に理解できます。</li>
</ul>
</div>
</div>
<div class="info-card" style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(5px);">
<h3 class="subsection-title" style="color: var(--color-accent2); border-left-color: var(--color-accent2);"><i class="fas fa-project-diagram" style="color: var(--color-accent2);"></i> アプローチ2: Graph200Kデータセットの構築</h3>
<div class="content-box">
<p>視覚タスクのデータは、その種類ごとにバラバラに存在していることが多く（<strong class="keyword">視覚タスク分布のスパース性</strong>）、これがタスク間で共通する知識（転移可能な知識）の学習を妨げる一因でした。VisualClozeでは、この問題を克服するために<strong class="keyword">Graph200K</strong>という新しいデータセットを構築しました。</p>
<div class="bubble-box" style="border-color: var(--color-accent2); margin-top:15px; margin-bottom:15px;">
<p style="text-align: center; font-weight: bold; color: var(--color-accent2);"><i class="fas fa-link"></i> タスク同士をつなぐ「知識のネットワーク」</p>
<p>Graph200Kは、様々なタスクがお互いに関連付けられた<span class="highlight">グラフ構造</span>を持つデータセットです。これにより、タスク間の関連性が明確になり、モデルはより密なタスク空間で学習できます。</p>
</div>
<p><i class="fas fa-cogs" style="color: var(--color-accent2);"></i> このデータセットの主な利点：</p>
<ul class="unstyled-list" style="list-style-type: '🔗'; padding-left: 20px;">
<li style="margin-bottom:5px;">モデルがタスク間で共通する<span class="highlight">転移可能な表現</span>を学習しやすくなります。例えば、「背景を変える」タスクと「オブジェクトの色を変える」タスクで共通して使える画像編集の基礎知識のようなものです。</li>
<li style="margin-bottom:5px;">モデルの<span class="highlight">適応能力が向上</span>し、新しいタスクにも対応しやすくなります。</li>
</ul>
</div>
</div>
<div class="info-card" style="background: rgba(255, 255, 255, 0.8); backdrop-filter: blur(5px);">
<h3 class="subsection-title" style="color: var(--color-secondary); border-left-color: var(--color-secondary);"><i class="fas fa-puzzle-piece" style="color: var(--color-secondary);"></i> アプローチ3: 画像修復モデルとの目的の一致</h3>
<div class="content-box">
<p>VisualClozeでは、<span class="highlight">画像修復（Image Infilling: 画像の一部が欠けている部分を補完するタスク）</span>と、我々が提案する<strong class="keyword">普遍的画像生成の定式化</strong>の間に、<span class="highlight">目的が本質的に一致している</span>ことを見出しました。</p>
<div class="bubble-box" style="border-color: var(--color-secondary); margin-top:15px; margin-bottom:15px;">
<p style="text-align: center; font-weight: bold; color: var(--color-secondary);"><i class="fas fa-magic"></i> 空白を埋める魔法</p>
<p>例えば、「猫の写真を犬の写真に変換する」タスクは、入力画像（猫）と出力画像（犬、ただし最初は空白）を並べ、その空白部分を「適切に埋める」問題として捉えることができます。これは画像修復モデルが得意とするところです。</p>
</div>
<p><i class="fas fas fa-tools" style="color: var(--color-secondary);"></i> この発見の主な利点：</p>
<ul class="unstyled-list" style="list-style-type: '🛠️'; padding-left: 20px;">
<li style="margin-bottom:5px;">既存の高性能な汎用画像修復モデルを、<span class="highlight">アーキテクチャを変更することなく</span>、そのまま普遍的画像生成タスクに適応させることができます。</li>
<li style="margin-bottom:5px;">これにより、モデルの強力な生成能力を最大限に活用できます。</li>
</ul>
</div>
</div>
</div>
<div class="arrow-connector"></div>
<div class="glass-card" style="margin-top: 25px;">
<h3 class="subsection-title" style="font-size: 20px;"><i class="fas fa-chart-line"></i> 実験結果と将来性</h3>
<p>実験結果は、VisualClozeが視覚的文脈学習を用いることで、<span class="highlight">学習データに含まれる多様なタスク（in-domain tasks）</span>を効果的にサポートできることを示しています。さらに重要なのは、<span class="highlight">学習データに含まれていない未知のタスク（unseen tasks）に対しても強力な汎化能力</span>を発揮することです。</p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); margin-top:15px;">
<div class="feature-item">
<div class="icon-item"><i class="fas fa-check-circle" style="color:var(--color-accent1); font-size:30px;"></i></div>
<p style="font-size: 13px;"><strong>多様な既知タスクへの対応</strong></p>
</div>
<div class="feature-item">
<div class="icon-item"><i class="fas fa-lightbulb" style="color:var(--color-accent3); font-size:30px;"></i></div>
<p style="font-size: 13px;"><strong>未知タスクへの高い汎化性</strong></p>
</div>
</div>
<p style="margin-top: 15px; text-align:center;">🔍 VisualClozeは、より柔軟で高性能な画像生成モデルの実現に向けた重要な一歩と言えるでしょう。</p>
</div>
<div style="text-align: center; margin-top: 30px;">
<span class="badge blue">普遍的生成</span>
<span class="badge purple">視覚的文脈学習</span>
<span class="badge orange">Graph200K</span>
<span class="badge green">画像修復</span>
<span class="badge yellow">汎化能力</span>
</div>
</div>
<div class="section-card" id="Appendix_A._Instruction_Format">
<h2 class="section-title">📜 Appendix A. Instruction Format</h2>
<p style="font-family: 'Yomogi', cursive; font-size: 16px; text-align: center; margin-bottom: 25px; color: var(--color-gray);">
        VisualClozeの魔法の呪文！ 📝✨ モデルに「何をしてほしいか」を伝えるための指示フォーマットを徹底解剖します！
    </p>
<div class="definition-box" style="margin-bottom: 25px;">
<p class="definition-title"><i class="fas fa-bullseye"></i> このセクションの目的と論旨</p>
<p>このセクションでは、VisualClozeという<span class="keyword">統一フレームワーク (unified framework)</span>で用いられる「指示 (instruction)」の具体的な形式について解説します。画像生成モデルに様々なタスクを実行させるためには、明確で理解しやすい指示を与えることが不可欠です。ここでは、その指示がどのような要素で構成されているのか、そしてそれがどのように機能するのかを学びます。</p>
<p>主なポイントは、指示が以下の3つの部分から構成されるという点です：</p>
<ul class="unstyled-list" style="padding-left: 20px;">
<li>1. <strong>レイアウト指示</strong>: 画像がどのように配置されるか。</li>
<li>2. <strong>タスク指示</strong>: どんな種類のタスクを実行するか。</li>
<li>3. <strong>コンテンツ指示</strong>: 生成する画像の具体的な内容は何か。</li>
</ul>
<p>これらの指示を組み合わせることで、VisualClozeは多様な画像生成タスクに対応できるようになります。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i> 指示の3つの構成要素 🔧</h3>
<p>VisualClozeフレームワークにおける指示は、モデルがタスクを正確に理解し実行するための重要な情報源です。これらの指示は、人間がモデルに対して「何をしてほしいか」を伝えるためのコミュニケーション手段と考えることができます。具体的には、以下の3つのパートに分かれています。</p>
<div class="info-grid" style="margin-top: 20px; margin-bottom: 30px;">
<div class="info-card">
<h4 class="subsection-title" style="font-size: 16px; color: var(--color-accent1);"><i class="fas fa-th-large"></i> 1. レイアウト指示 (Layout Instruction) 🖼️</h4>
<p>これは、入力される画像群（インコンテキスト例や条件画像など）と、生成されるべきターゲット画像が、全体としてどのような<span class="keyword">グリッド画像 (grid image)</span>の配置になっているかを記述します。</p>
<ul style="list-style-type: '👉'; padding-left: 20px;">
<li><strong>役割</strong>: モデルに画像の空間的な配置情報を伝えます。</li>
<li><strong>例</strong>: 「3行4列のグリッドに12枚の画像を配置」といった形式で指定されます。</li>
<li><strong>重要性</strong>: 特に<span class="keyword">インコンテキスト学習 (in-context learning)</span>において、どの画像がデモンストレーション例で、どれがクエリ（処理対象）なのかをモデルが正しく認識するために不可欠です。</li>
</ul>
</div>
<div class="info-card">
<h4 class="subsection-title" style="font-size: 16px; color: var(--color-accent2);"><i class="fas fa-tasks"></i> 2. タスク指示 (Task Instruction) 🎯</h4>
<p>この部分は、モデルに実行してほしい具体的な<span class="keyword">タスクの種類 (task type)</span>を指定します。これにより、モデルは何をすべきかを理解します。</p>
<ul style="list-style-type: '👉'; padding-left: 20px;">
<li><strong>役割</strong>: 実行すべき画像生成タスクを明確に定義します。</li>
<li><strong>例</strong>: 「画像編集」「スタイル転送」「被写体の変更」など。</li>
<li><strong>柔軟性</strong>: この指示により、同じモデルアーキテクチャで多様なタスクに対応できます。</li>
</ul>
</div>
<div class="info-card">
<h4 class="subsection-title" style="font-size: 16px; color: var(--color-secondary);"><i class="fas fa-file-alt"></i> 3. コンテンツ指示 (Content Instruction) ✍️</h4>
<p>これは、モデルが生成すべき<span class="keyword">ターゲット画像 (target image)</span>の具体的な内容や特徴を記述します。これにより、望ましい出力結果を得るための詳細なガイダンスが与えられます。</p>
<ul style="list-style-type: '👉'; padding-left: 20px;">
<li><strong>役割</strong>: 生成画像の具体的な見た目、オブジェクト、シーン、雰囲気などを指定します。</li>
<li><strong>例</strong>: 「公園で遊ぶ赤いボールを持った犬」「夕焼け空の下の未来都市」など。</li>
<li><strong>省略可能性</strong>: 後述するように、タスクによってはこの指示が省略されることもあります。</li>
</ul>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-images"></i> 図13による具体例 🔍</h3>
<p>論文中の図13は、これらの指示が実際のタスクでどのように記述され、機能するのかを視覚的に示しています。この図を見ることで、抽象的な指示の概念がより具体的に理解できるでしょう。</p>
<img alt="図13: 指示フォーマットの例。左側に連結された画像、右側に言語指示が示されている。上段はコンセプト融合、下段は参照画像を用いた画像編集の例。" src="concept_fusion_style_subject_layout.jpg"/>
<div class="note-box" style="margin-top:20px; margin-bottom: 20px; background-color: rgba(255, 213, 79, 0.2); border-left-color: var(--color-accent3);">
<p class="note-title" style="color: var(--color-accent3);"><i class="fas fa-search-plus"></i> 図13の見方ポイント</p>
<p>図13は2つの例（上段と下段）から構成されています。それぞれの例で、左側 (a) には<span class="keyword">連結された画像 (Concatenated images)</span>が、右側 (b) にはそれに対応する<span class="keyword">言語指示 (Language instructions)</span>が示されています。</p>
<ul style="list-style-type: '🔸'; padding-left: 20px;">
<li><strong>連結された画像 (a)</strong>: インコンテキスト学習のためのデモンストレーション例、タスク実行に必要な条件画像、そしてモデルが生成すべきターゲット画像（またはそのプレースホルダー）がグリッド状に配置されています。</li>
<li><strong>言語指示 (b)</strong>: 上述したレイアウト指示、タスク指示、コンテンツ指示がテキスト形式で記述されています。</li>
</ul>
</div>
<div class="bubble-box" style="border-color: var(--color-accent1); margin-bottom: 20px;">
<h4 style="color: var(--color-accent1); border-bottom: 2px dashed var(--color-accent1); padding-bottom: 5px;"><i class="fas fa-palette"></i> 例1: スタイル・被写体・レイアウトのコンセプト融合 (図13 上段)</h4>
<p>この例では、複数の要素（スタイル、被写体、レイアウト）を組み合わせて新しい画像を生成する<span class="keyword">コンセプト融合 (concept fusion)</span>タスクの指示フォーマットが示されています。</p>
<div class="framework-box" style="border-color: var(--color-accent1);">
<p class="framework-title" style="color: var(--color-accent1);">📝 指示内容の詳細:</p>
<ul style="list-style-type: none; padding-left: 10px;">
<li><strong style="color: var(--color-primary);">🖼️ レイアウト指示:</strong> <span class="highlight">"12 images are organized into a grid of 3 rows and 4 columns, evenly spaced."</span> (12枚の画像が3行4列のグリッドに均等配置されている。)
                    <ul style="list-style-type: '➡️'; padding-left: 20px; margin-top: 5px;">
<li>これは、入力画像群がどのように配置されているかをモデルに伝えます。</li>
</ul>
</li>
<li style="margin-top: 10px;"><strong style="color: var(--color-primary);">🎯 タスク指示:</strong> <span class="highlight">"Each row describes a process that begins with [IMAGE1] white edge lines on black from canny detection, [IMAGE2] Photo with a strong artistic theme, [IMAGE3] a reference image showcasing the dominant object and results in [IMAGE4] High-quality visual with distinct artistic touch."</span> (各行は、[IMAGE1]キャニー検出による黒地に白のエッジ線、[IMAGE2]強い芸術的テーマを持つ写真、[IMAGE3]主要なオブジェクトを示す参照画像から始まり、[IMAGE4]明確な芸術的タッチを持つ高品質なビジュアルを結果とするプロセスを記述。)
                    <ul style="list-style-type: '➡️'; padding-left: 20px; margin-top: 5px;">
<li>この指示は、複数の入力画像（エッジ、アート写真、参照画像）から、特定の芸術的タッチを持つ高品質な画像を生成するというタスク内容を定義しています。</li>
</ul>
</li>
<li style="margin-top: 10px;"><strong style="color: var(--color-primary);">✍️ コンテンツ指示:</strong> <span class="highlight">"∅"</span> (空、指示なし)
                    <ul style="list-style-type: '➡️'; padding-left: 20px; margin-top: 5px;">
<li>この「∅」記号は、この特定のタスクでは明示的なテキストによるコンテンツ指示が<span class="keyword">提供されていない</span>ことを意味します。</li>
<li>コンセプト融合タスクでは、入力される複数の画像（スタイル源、被写体源、レイアウト源）自体が強い視覚的情報を持っているため、それらを組み合わせることでターゲット画像の内容が規定される、と考えられます。つまり、モデルは与えられた視覚的文脈から生成すべき内容を推測します。</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="bubble-box" style="border-color: var(--color-accent2); margin-bottom: 20px;">
<h4 style="color: var(--color-accent2); border-bottom: 2px dashed var(--color-accent2); padding-bottom: 5px;"><i class="fas fa-edit"></i> 例2: 参照画像を用いた画像編集 (図13 下段)</h4>
<p>この例は、特定の参照画像に基づいて画像編集を行うタスクの指示フォーマットを示しています。</p>
<div class="framework-box" style="border-color: var(--color-accent2);">
<p class="framework-title" style="color: var(--color-accent2);">📝 指示内容の詳細:</p>
<ul style="list-style-type: none; padding-left: 10px;">
<li><strong style="color: var(--color-primary);">🖼️ レイアウト指示:</strong> <span class="highlight">"A 3x3 grid containing 9 images, aligned in a clean and structured layout."</span> (9枚の画像を含む3x3のグリッドで、すっきりと構造化されたレイアウトに配置されている。)
                    <ul style="list-style-type: '➡️'; padding-left: 20px; margin-top: 5px;">
<li>ここでは9枚の画像が3x3のグリッドに配置されることが示されています。</li>
</ul>
</li>
<li style="margin-top: 10px;"><strong style="color: var(--color-primary);">🎯 タスク指示:</strong> <span class="highlight">"Every row provides a step-by-step guide to evolve [IMAGE1] a reference image with the main subject included, [IMAGE2] an image with flawless clarity into [IMAGE3] a high-quality image."</span> (各行は、[IMAGE1]主要な被写体を含む参照画像、[IMAGE2]完璧な明瞭さを持つ画像を進化させて[IMAGE3]高品質な画像にするためのステップバイステップガイドを提供。)
                    <ul style="list-style-type: '➡️'; padding-left: 20px; margin-top: 5px;">
<li>この指示は、参照画像 (IMAGE1) と高品質化のためのガイド画像 (IMAGE2) を用いて、最終的な高品質画像 (IMAGE3) を生成するという、段階的な画像編集タスクを定義しています。</li>
</ul>
</li>
<li style="margin-top: 10px;"><strong style="color: var(--color-primary);">✍️ コンテンツ指示:</strong> <span class="highlight">"The bottom-right corner image presents: A glossy gel nail polish bottle. At the edge of a bustling city park, this item rests on vibrant green grass, captured with a subtle bokeh effect as joggers and pets move in the background."</span> (右下の画像は、光沢のあるジェルネイルポリッシュのボトルです。賑やかな都市公園の端で、このアイテムは鮮やかな緑の芝生の上に置かれ、ジョガーやペットが背景を移動する中、微妙なボケ効果で捉えられています。)
                    <ul style="list-style-type: '➡️'; padding-left: 20px; margin-top: 5px;">
<li>こちらは非常に具体的なコンテンツ指示です。生成すべきターゲット画像（この場合は右下の画像）の内容、オブジェクト、背景、雰囲気まで詳細に記述されています。</li>
<li>これにより、モデルは明確な目標を持って画像生成を行うことができます。</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="glass-card" style="margin-top: 30px;">
<h3 class="subsection-title" style="border-left: none; padding-left:0; margin-top:0;"><i class="fas fa-info-circle"></i> 重要な注意点: コンテンツ指示の省略について 📌</h3>
<p>論文の最後に述べられているように、<span class="keyword">コンテンツ指示 (content instruction)</span> は、一部のタスクでは省略されることがあります。</p>
<div class="note-box" style="background-color: rgba(92, 184, 92, 0.1); border-left-color: var(--color-accent1);">
<p class="note-title" style="color: var(--color-accent1);"><i class="fas fa-lightbulb"></i> なぜ省略できるのか？</p>
<p>これは、タスクの性質上、入力条件（デモンストレーション例や条件画像）が既に十分な<span class="keyword">視覚的手がかり (strong visual cues)</span>を提供している場合に起こります。</p>
<ul style="list-style-type: '💡'; padding-left: 20px;">
<li><strong>例: スタイル転送 (style transfer)</strong>
<ul style="list-style-type: '🔹'; padding-left: 20px;">
<li>スタイル転送タスクでは、通常「コンテンツ画像」と「スタイル画像」が入力として与えられます。</li>
<li>これらの入力画像自体が、生成すべき画像の「内容」と「スタイル」を明確に示しています。</li>
<li>したがって、追加のテキストによる詳細なコンテンツ記述は不要、あるいは冗長になる場合があります。モデルは入力された視覚情報から何をすべきかを判断できます。</li>
</ul>
</li>
<li><strong>図13上段の例</strong>: 先ほどのコンセプト融合の例でも、コンテンツ指示は「∅」(なし)でした。これも、複数の入力画像が提供する視覚情報が豊富であるため、モデルがそれらを元に生成内容を推測できるからです。</li>
</ul>
</div>
<p style="margin-top: 15px;">この柔軟性により、VisualClozeはさまざまなタスクに対して効率的に指示を構成することができます。視覚情報がリッチな場合はそれを最大限活用し、そうでない場合はテキストによる補足情報で補う、という使い分けが可能です。</p>
</div>
<div class="challenge-box" style="margin-top: 30px;">
<h3 class="subsection-title" style="border-left: none; padding-left:0; margin-top:0; color:var(--color-secondary);"><i class="fas fa-brain"></i> まとめと考察 🧠</h3>
<p>このAppendix Aで解説された指示フォーマットは、VisualClozeフレームワークが多様な画像生成タスクを統一的に扱うための基盤となります。</p>
<ul class="unstyled-list" style="padding-left: 20px;">
<li class="process-step">
<div class="step-number" style="background-color: var(--color-primary);">1</div>
<div class="step-content"><strong>構造化された情報提供:</strong> レイアウト、タスク、コンテンツという3部構成により、モデルに必要な情報を過不足なく、かつ構造的に伝えることができます。</div>
</li>
<li class="process-step">
<div class="step-number" style="background-color: var(--color-accent1);">2</div>
<div class="step-content"><strong>インコンテキスト学習との連携:</strong> 特にレイアウト指示は、インコンテキスト学習のデモンストレーション例とクエリの関係性を明確にする上で重要です。</div>
</li>
<li class="process-step">
<div class="step-number" style="background-color: var(--color-accent2);">3</div>
<div class="step-content"><strong>タスクの多様性への対応:</strong> タスク指示を変えることで、同じモデルが異なる種類の生成タスクを実行できます。</div>
</li>
<li class="process-step">
<div class="step-number" style="background-color: var(--color-secondary);">4</div>
<div class="step-content"><strong>柔軟な指示設計:</strong> コンテンツ指示の省略可能性は、タスクの特性に応じて指示の冗長性を減らし、効率を高める工夫と言えます。</div>
</li>
</ul>
<p style="margin-top: 15px;">大学院生の皆さんは、このような指示フォーマットの設計が、大規模言語モデル (LLM) やマルチモーダルモデルにおいて、いかにユーザーの意図を正確に伝え、モデルの能力を引き出すために重要であるかを理解できるでしょう。VisualClozeは、視覚情報と言語情報を組み合わせた洗練された指示システムにより、その汎用性と高性能を実現しているのです。</p>
</div>
</div>
<div class="section-card" id="Appendix_B._Fine-tuning_FLUX.1-dev_Model">
<h2 class="section-title"><i class="fas fa-microchip"></i> Appendix B. Fine-tuning FLUX.1-dev Model</h2>
<div class="content-box">
<p>このセクションでは、論文の主要な画像インフィリングモデル <span class="keyword">FLUX.1-Fill-dev</span> だけでなく、より一般的なテキストから画像を生成する拡散モデルである <span class="keyword">FLUX.1-dev</span> [33] にVisualClozeの手法を適用し、ファインチューニングする際の詳細について解説します。</p>
<p><i class="fas fa-bullseye"></i> 主な目的は、FLUX.1-devをVisualClozeの普遍的画像生成フレームワークに適合させるために施された、特有のカスタマイズとアーキテクチャ上の変更点を明らかにすることです。特に、<span class="highlight">クリーンな条件画像</span>と<span class="highlight">ノイズを含むターゲット画像</span>を同時に処理する際の課題と、<span class="keyword">adaLN-Zero</span>ブロック[48]におけるパラメータ調整の重要性に焦点を当て、これらの工夫が生成画像の視覚的な忠実度をどのように担保するかを説明します。</p>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i> FLUX.1-devモデルへの適用と背景</h3>
<div class="info-grid">
<div class="info-card">
<h4 class="info-card-title"><i class="fas fa-brain"></i> FLUX.1-devモデルとは？</h4>
<p><span class="keyword">FLUX.1-dev</span> [33] は、テキスト記述に基づいて画像を生成する能力を持つ、一般的な拡散モデルの一つです。特定のタスクに特化しているわけではなく、幅広い画像生成に応用可能です。</p>
</div>
<div class="info-card">
<h4 class="info-card-title"><i class="fas fa-exchange-alt"></i> FLUX.1-Fill-devとの違いと課題</h4>
<p>VisualClozeフレームワークで主に利用される <span class="keyword">FLUX.1-Fill-dev</span> は、画像の特定領域を埋める（インフィリング）タスクに特化しており、その目的は「欠損領域を周囲の文脈に合わせて生成する」という点で、VisualClozeの「ターゲット領域を条件画像に基づいて生成する」という普遍的画像生成の目的と自然に一致します。</p>
<p>しかし、<span class="keyword">FLUX.1-dev</span> はそのようなインフィリングを直接の目的として設計されていません。そのため、VisualClozeの枠組みで用いるには、以下の課題に対応するための<span class="highlight">カスタマイズされた変更</span>が必要になります：</p>
<ul>
<li><i class="fas fa-image"></i> <strong>入力形式の違い：</strong> VisualClozeでは、クリーン（ノイズなし）な条件画像と、生成対象である（初期状態ではノイズが付与されている）ターゲット画像を同時に扱います。FLUX.1-devがこれを適切に処理できるようにする必要があります。</li>
</ul>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-tools"></i> 実施されたカスタマイズの詳細</h3>
<p>FLUX.1-devをVisualClozeフレームワークに適合させるため、主に以下の2つの点で変更が加えられました。</p>
<div class="pipeline">
<div class="pipeline-step">
<h4><span class="badge blue">変更点 1</span> <i class="fas fa-images"></i> 条件画像の処理戦略：クリーンな潜在表現の維持</h4>
<p>VisualClozeでは、複数の条件画像とターゲット画像をグリッド状に配置して処理します。FLUX.1-devを用いる際、このグリッド内の<span class="highlight">条件画像に対応する領域</span>の扱方が重要になります。</p>
<div class="feature-item" style="background-color: rgba(230, 247, 255, 0.5); padding: 15px; border-radius: 8px; margin: 15px 0;">
<p><i class="fas fa-shield-alt" style="color: var(--color-primary); font-size: 20px;"></i> <strong>戦略：</strong> 条件画像に対応する領域は、画像生成の全サンプリングプロセスを通じて、常に<span class="keyword">クリーンな（ノイズのない）潜在埋め込み</span>として保持されます。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-exclamation-circle"></i> なぜこの変更が必要か？</p>
<p>これは、<span class="keyword">FLUX.1-Fill-dev</span> がノイズの付加された潜在埋め込みを入力として受け取るのとは対照的です。FLUX.1-devをVisualClozeの文脈（条件はクリーン、ターゲットはノイズから生成）で使うために、画像サンプリング処理に変更を加える必要がありました。</p>
</div>
</div>
<div class="pipeline-step">
<h4><span class="badge orange">変更点 2</span> <i class="fas fa-sliders-h"></i> adaLN-Zeroブロックの調整：パラメータの分離計算</h4>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i> adaLN-Zero ブロックとは？</p>
<p><span class="keyword">adaLN-Zero</span> (Adaptive Layer Normalization Zero) [48] は、Transformerベースのモデルでよく用いられる適応的レイヤー正規化手法の一種です。入力特徴量に応じて正規化のスケール（\(\gamma\)）とシフト（\(\beta\)）を動的に調整し、特に初期状態ではネットワークの出力を恒等写像に近づけることで学習を安定させる効果があります。ここでの「mean」と「shift」は、この正規化層内部の適応的パラメータを指していると考えられます。</p>
</div>
<p>FLUX.1-devをVisualClozeで用いる上で、この <span class="keyword">adaLN-Zero</span> ブロックの動作を、条件領域とターゲット領域とで区別することが極めて重要であると筆者らは述べています。</p>
<div class="feature-item" style="background-color: rgba(255, 245, 230, 0.5); padding: 15px; border-radius: 8px; margin: 15px 0;">
<p><i class="fas fa-cogs" style="color: var(--color-secondary); font-size: 20px;"></i> <strong>戦略：</strong> <span class="keyword">adaLN-Zero</span> ブロックに与えるタイムステップ情報を制御することで、<span class="highlight">クリーンな条件領域</span>と<span class="highlight">ノイズの多いターゲット領域</span>に対して、それぞれ異なる<span class="keyword">平均 (mean)</span> と <span class="keyword">シフト (shift)</span> パラメータを計算させます。</p>
</div>
<div class="info-grid">
<div class="info-card glass-card">
<h5 style="text-align: center;"><i class="fas fa-image"></i> クリーン条件領域の処理</h5>
<p>条件画像に対応する領域のadaLN-Zeroブロックには、タイムステップとして \(T = 0\) を入力します。これは「ノイズがない状態」を意味します。</p>
<div class="formula">
<p>\( \text{adaLN-Zero}_{\text{condition_params}} = \text{adaLN-Zero_block}(\text{features, } T=0) \)</p>
</div>
</div>
<div class="info-card glass-card">
<h5 style="text-align: center;"><i class="fas fa-braille"></i> ノイズターゲット領域の処理</h5>
<p>生成対象であるターゲット画像領域のadaLN-Zeroブロックには、現在のサンプリングステップ \(t\) でのタイムステップ \(T = t\) を入力します。</p>
<div class="formula">
<p>\( \text{adaLN-Zero}_{\text{target_params}} = \text{adaLN-Zero_block}(\text{features, } T=t) \)</p>
</div>
</div>
</div>
<div class="note-box" style="margin-top: 20px;">
<p class="note-title"><i class="fas fa-hourglass-start"></i> タイムステップ \(t\) について</p>
<p><span class="keyword">タイムステップ \(t\)</span> は、拡散モデルにおける画像生成プロセスの進行度を示します。各サンプリングステップで更新され、本論文の記述では<span class="highlight">0から1へと徐々に増加</span>します（\(t=0\) が初期ノイズ状態、\(t=1\) が生成完了状態、あるいはその逆の定義もありますが、ここでは0から1への増加と明記されています）。</p>
</div>
<div class="bubble-box">
<p><i class="fas fa-link" style="color: var(--color-accent1);"></i> <strong>この戦略の根拠と利点：</strong></p>
<p>このパラメータ分離戦略は、<span class="keyword">FLUX.1-dev</span> の<span class="highlight">事前学習ドメインにおける振る舞いと一致</span>します。事前学習時、FLUX.1-devは入力されるノイズレベル（タイムステップ \(t\) に対応）に応じて異なるmeanとshiftパラメータを学習しています。したがって、条件領域（ノイズレベル0相当）とターゲット領域（ノイズレベル \(t\)）でこれらのパラメータを使い分けることで、モデルが本来持っている能力を最大限に引き出し、より忠実な画像生成が可能になります。</p>
</div>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> 戦略の効果：視覚的忠実度の確保 (図14より)</h3>
<p>上記のadaLN-Zeroブロックにおけるmeanとshiftパラメータの分離戦略が、実際に生成される画像の品質にどのような影響を与えるかを示したのが、論文中の図14です。</p>
<img alt="図14: FLUX.1-devにおけるmeanとshiftパラメータの分離戦略の効果比較。 (a) 分離した場合、(b) 統一した場合。" src="flux1_dev_mean_shift_comparison.jpg" style="display: block; margin: 20px auto; max-width: 80%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
<div class="info-card" style="margin-top: 20px;">
<h4 class="info-card-title"><i class="fas fa-search-plus"></i> 図14の読解ポイント</h4>
<p>この図は、2つの異なる条件での画像生成結果を比較しています。</p>
<ul class="unstyled-list">
<li style="margin-bottom:15px;">
<p><span class="badge green" style="font-size:1em;">(a) separate mean and shift</span> (分離戦略 - 本論文の手法)</p>
<p>左側の列が「条件画像」、右側の列がそれに基づいて生成された「ターゲット画像」です。<br/>
                この設定では、adaLN-Zeroブロックのmeanとshiftパラメータが、条件領域（\(T=0\)を入力）とターゲット領域（\(T=t\)を入力）で<span class="highlight">個別に計算</span>されています。<br/>
<strong>結果：</strong> 条件画像（深度マップやセマンティックマスク）の特徴を維持しつつ、ターゲット画像は<span class="keyword">鮮明で高品質</span>に生成されています。例えば、上の段の女性の画像のディテールや、下の段の建物の構造がはっきりしています。</p>
</li>
<li>
<p><span class="badge red" style="font-size:1em;">(b) unified mean and shift</span> (統一戦略 - 比較対象)</p>
<p>こちらでは、条件領域とターゲット領域でmeanとshiftパラメータを<span class="highlight">区別せずに統一的に計算</span>しています。<br/>
<strong>結果：</strong> ターゲット画像が(a)と比較して<span class="keyword">ぼやけていたり、不自然なノイズやアーティファクト</span>が見受けられます。特に上の段の画像の背景や、下の段の画像の全体的なシャープネスが劣っています。</p>
</li>
</ul>
<div class="note-box" style="background-color: rgba(200, 230, 255, 0.5); border-left-color: var(--color-primary);">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-thumbs-up"></i> 結論：分離戦略の有効性</p>
<p>図14の結果は、FLUX.1-devモデルをVisualClozeのような条件付き生成タスク（特にクリーンな条件とノイジーなターゲットを扱う場合）に適用する際、<span class="keyword">adaLN-Zeroブロックのパラメータ処理を条件領域とターゲット領域で適切に分離することが、生成画像の視覚的な忠実度を保証する上で不可欠</span>であることを明確に示しています。この工夫により、モデルは各領域の特性（ノイズレベルの違いなど）に応じた最適な処理を行うことができ、結果として高品質な画像生成が実現されます。</p>
</div>
</div>
<div class="reference" style="margin-top: 20px; text-align: right;">
<p>参考文献:</p>
<p>[33] Black Forest Labs. Flux. (FLUX.1-dev, FLUX.1-Fill-devの出典)</p>
<p>[48] William Peebles and Saining Xie. Scalable diffusion models with transformers. (adaLN-Zeroの出典)</p>
</div>
</div>
<div class="section-card" id="Appendix_C._Evaluation_Metrics">
<h2 class="section-title"><i class="fas fa-tachometer-alt"></i> Appendix C. Evaluation Metrics</h2>
<div style="text-align: center; margin-bottom: 25px;">
<div style="display: inline-block; font-family: 'Yomogi', cursive; background-color: rgba(230, 247, 255, 0.8); backdrop-filter: blur(5px); border: 2px dashed var(--color-primary); border-radius: 12px; padding: 15px 25px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
<i class="fas fa-bullhorn" style="color: var(--color-secondary); font-size: 20px; margin-right: 10px;"></i>
<span style="font-size: 17px; color: var(--color-dark);">このセクションの目的を一言でいうと...</span><br/>
<strong style="font-size: 19px; color: var(--color-primary); display: block; margin-top: 8px;">「作った画像生成モデル、どれくらいイケてるの？🤔」を客観的に測るためのモノサシ（評価指標）を解説します！</strong>
</div>
</div>
<p>このAppendix Cでは、本論文で提案されている画像生成フレームワーク (VisualCloze) の性能を様々な側面から評価するために用いられる<span class="keyword">評価指標 (Evaluation Metrics)</span> について詳しく説明します。生成される画像の品質や、モデルが指示にどれだけ忠実に従っているかなどを数値化することで、モデルの能力を客観的に比較・分析することが可能になります。特に、以下の3つの主要なタスクカテゴリに焦点を当てて評価指標が定義されています：</p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="feature-item" style="background-color: rgba(74, 111, 165, 0.05); border: 1px solid var(--color-primary);">
<i class="fas fa-cogs fa-2x" style="color: var(--color-primary); margin-bottom: 10px;"></i>
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-primary); font-size: 16px;">1. 条件付き生成</h4>
<p style="font-size: 13px;">特定の条件（エッジ、深度など）に基づいて画像を生成する能力。</p>
</div>
<div class="feature-item" style="background-color: rgba(255, 126, 95, 0.05); border: 1px solid var(--color-secondary);">
<i class="fas fa-user-astronaut fa-2x" style="color: var(--color-secondary); margin-bottom: 10px;"></i>
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-secondary); font-size: 16px;">2. 被写体駆動生成</h4>
<p style="font-size: 13px;">特定の被写体の特徴を保持して画像を生成する能力。</p>
</div>
<div class="feature-item" style="background-color: rgba(92, 184, 92, 0.05); border: 1px solid var(--color-accent1);">
<i class="fas fa-paint-brush fa-2x" style="color: var(--color-accent1); margin-bottom: 10px;"></i>
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-accent1); font-size: 16px;">3. スタイル変換</h4>
<p style="font-size: 13px;">コンテンツは維持しつつ、指定されたスタイルで画像を変換する能力。</p>
</div>
</div>
<h3 class="subsection-title" style="font-size: 22px; color: var(--color-primary); border-bottom: 2px solid var(--color-primary); padding-bottom: 5px; margin-top: 30px;"><i class="fas fa-cogs"></i> C.1. Conditioning Generation (条件付き生成)</h3>
<p>このセクションでは、<span class="keyword">条件付き画像生成 (Conditioning Generation)</span> と <span class="keyword">画像修復 (Image Restoration)</span> タスクにおける生成画像の品質を評価する方法について説明します。評価は主に3つの観点から行われます：<span class="highlight">制御性</span>、<span class="highlight">品質</span>、そして<span class="highlight">テキスト整合性</span>です。</p>
<div class="info-grid">
<div class="info-card glass-card">
<h4 class="subsection-title" style="font-size: 18px; color: var(--color-secondary);"><i class="fas fa-gamepad"></i> Controllability (制御性)</h4>
<p>📌 <strong>定義:</strong> モデルが入力された条件にどれだけ忠実に画像を生成できたか、を測る指標です。</p>
<p>具体的には、生成された画像から抽出した条件と、最初に入力した条件との間の「ズレ」を計測します。</p>
<ul class="unstyled-list">
<li>✏️ <strong>Cannyエッジから画像生成 (Canny-to-image) タスク:</strong>
<div class="bubble-box" style="border-color: var(--color-accent1); margin-top: 10px; margin-bottom: 10px;">
<p style="margin:0;"><strong>指標:</strong> <span class="keyword">F1 Score</span></p>
<p style="margin:5px 0;"><strong>説明:</strong> 生成画像からCannyエッジを抽出し、入力のCannyエッジと比較します。F1 Scoreは、適合率 (Precision) と再現率 (Recall) の調和平均で、エッジの一致度が高いほど良いスコアになります。</p>
<div style="text-align: center; margin-top:10px;">
<span class="badge green">Precision</span> <span class="badge green">Recall</span>  ➡️  <span class="badge yellow">F1 Score</span>
</div>
</div>
</li>
<li>📏 <strong>深度情報から画像生成 (Depth-to-image) タスク:</strong>
<div class="bubble-box" style="border-color: var(--color-accent2); margin-top: 10px; margin-bottom: 10px;">
<p style="margin:0;"><strong>指標:</strong> <span class="keyword">RMSE (Root Mean Square Error)</span></p>
<p style="margin:5px 0;"><strong>説明:</strong> 生成画像から深度マップを推定し、入力の深度マップと比較します。RMSEは、ピクセルごとの誤差の二乗平均平方根で、値が小さいほど元画像とのズレが少ないことを意味します。</p>
<div style="font-family: 'Kaisei Decol', serif; text-align: center; margin-top:10px; font-size: 16px;">
<span style="color: var(--color-accent2);">誤差が小さいほどGOOD! 👍</span>
</div>
</div>
</li>
<li>✨ <strong>画像鮮明化 (Deblurring) タスク:</strong>
<div class="bubble-box" style="border-color: var(--color-accent3); margin-top: 10px; margin-bottom: 10px;">
<p style="margin:0;"><strong>指標:</strong> <span class="keyword">RMSE (Root Mean Square Error)</span></p>
<p style="margin:5px 0;"><strong>説明:</strong> 修復された鮮明な画像と、元のボケていない画像を比較します。ここでもRMSEが使われ、値が小さいほど修復精度が高いことを示します。</p>
</div>
</li>
</ul>
</div>
<div class="info-card glass-card">
<h4 class="subsection-title" style="font-size: 18px; color: var(--color-accent1);"><i class="fas fa-image"></i> Generation Quality (生成品質)</h4>
<p>📌 <strong>定義:</strong> 生成された画像が、視覚的にどれだけ高品質で自然か、を測る指標です。</p>
<p>以下の複数の指標を組み合わせて評価します。</p>
<ul class="unstyled-list">
<li>📊 <strong>FID (Fréchet Inception Distance) [23]:</strong>
<div class="note-box" style="border-left-color: var(--color-primary); margin-top:10px; margin-bottom:10px;">
<p style="margin:0;"><strong>説明:</strong> 生成された画像の<span class="keyword">特徴量分布</span>と、本物の画像の<span class="keyword">特徴量分布</span>がどれだけ似ているかを測ります。Inception Networkという学習済みモデルを使って画像の特徴量を抽出し、その統計的な距離 (Fréchet Distance) を計算します。値が小さいほど、生成画像が本物の画像に近いとされます。</p>
<div style="text-align:center; margin-top:10px;">
<i class="fas fa-chart-bar" style="color: var(--color-primary); font-size: 18px;"></i> 生成画像群 vs <i class="fas fa-images" style="color: var(--color-accent1); font-size: 18px;"></i> 本物画像群  ➡️  <span class="badge blue">FID値 (低いほど良い)</span>
</div>
</div>
</li>
<li>👁️‍🗨️ <strong>SSIM (Structural Similarity Index Measure):</strong>
<div class="note-box" style="border-left-color: var(--color-secondary); margin-top:10px; margin-bottom:10px;">
<p style="margin:0;"><strong>説明:</strong> 人間の知覚的な画質を評価する指標です。2つの画像の<span class="keyword">輝度 (Luminance)</span>、<span class="keyword">コントラスト (Contrast)</span>、<span class="keyword">構造 (Structural patterns)</span> を比較します。具体的には、画像の局所的なパッチの統計情報を計算し、それらを組み合わせて-1から1の範囲のスコアを出力します。値が1に近いほど、構造的な類似性が高く、高品質であると評価されます。</p>
<div style="text-align:center; margin-top:10px;">
<i class="fas fa-adjust" style="color: var(--color-secondary);"></i> 輝度・コントラスト・構造  ➡️  <span class="badge orange">SSIM値 (-1～1、高いほど良い)</span>
</div>
</div>
</li>
<li>🧠 <strong>MANIQA [75] と MUSIQ [30]:</strong>
<div class="note-box" style="border-left-color: var(--color-accent2); margin-top:10px; margin-bottom:10px;">
<p style="margin:0;"><strong>説明:</strong> これらは<span class="keyword">ニューラルネットワーク</span>を活用して画像の品質スコアを予測する<span class="keyword">参照なし画質評価 (No-Reference Image Quality Assessment)</span> 手法です。つまり、比較対象の「正解画像」がなくても、生成された画像単体でその品質を評価できます。MANIQAは論文中で2回言及されていますが、おそらくMulti-dimension Attention Network for No-reference Image Quality Assessmentの略です。</p>
<div style="text-align:center; margin-top:10px;">
<i class="fas fa-brain" style="color: var(--color-accent2);"></i> DNNで品質予測  ➡️  <span class="badge purple">品質スコア (手法による)</span>
</div>
</div>
</li>
</ul>
</div>
<div class="info-card glass-card" style="grid-column: span 2;">
<h4 class="subsection-title" style="font-size: 18px; color: var(--color-accent3);"><i class="fas fa-align-left"></i> Text Consistency (テキスト整合性)</h4>
<p>📌 <strong>定義:</strong> 生成された画像が、入力された<span class="keyword">テキストプロンプト (指示文)</span> の内容と意味的にどれだけ一致しているか、を測る指標です。</p>
<p>モデルが指示を正しく理解し、それに従っているかを評価します。</p>
<div class="bubble-box" style="border-color: var(--color-accent3); margin-top: 10px;">
<p style="margin:0;"><strong>手法:</strong> 強力な<span class="keyword">マルチモーダル能力</span>を持つ<span class="keyword">CLIP (Contrastive Language-Image Pre-training) [49]</span> モデルを活用します。</p>
<p style="margin:5px 0;"><strong>説明:</strong> CLIPは、画像とその説明テキストを同じ特徴空間に埋め込むことができるモデルです。生成された画像と入力テキストプロンプトをそれぞれCLIPで特徴ベクトルに変換し、それらの<span class="keyword">コサイン類似度</span>を計算することで、意味的な整合性を評価します。類似度が高いほど、画像とテキストが意味的に近いと判断されます。</p>
<div class="pipeline" style="margin-top:15px;">
<div class="pipeline-step" style="background-color: rgba(255, 213, 79, 0.1); border-color: var(--color-accent3);">
<span class="step-number" style="background-color: var(--color-accent3); color: var(--color-dark);">1</span>
<div class="step-content">生成画像 🖼️  + テキストプロンプト 📝</div>
</div>
<div class="pipeline-step" style="background-color: rgba(255, 213, 79, 0.1); border-color: var(--color-accent3);">
<span class="step-number" style="background-color: var(--color-accent3); color: var(--color-dark);">2</span>
<div class="step-content">CLIPモデルで各々を特徴ベクトル化 <i class="fas fa-vector-square"></i></div>
</div>
<div class="pipeline-step" style="background-color: rgba(255, 213, 79, 0.1); border-color: var(--color-accent3);">
<span class="step-number" style="background-color: var(--color-accent3); color: var(--color-dark);">3</span>
<div class="step-content">コサイン類似度を計算 <i class="fas fa-calculator"></i>  ➡️  <span class="badge yellow">整合性スコア (高いほど良い)</span></div>
</div>
</div>
</div>
</div>
</div>
<h3 class="subsection-title" style="font-size: 22px; color: var(--color-primary); border-bottom: 2px solid var(--color-primary); padding-bottom: 5px; margin-top: 30px;"><i class="fas fa-user-astronaut"></i> C.2. Subject Driven Generation (被写体駆動生成)</h3>
<p>このセクションでは、<span class="keyword">被写体駆動型画像生成 (Subject Driven Generation)</span> の性能評価について説明します。これは、特定の被写体（例：特定の人物、物体）の特徴を保持したまま新しい画像を生成するタスクです。評価は、先行研究であるDreamBooth [54] や BLIP-Diffusion [36] の手法を踏襲しています。</p>
<div class="framework-box" style="border-color: var(--color-secondary);">
<div class="framework-title" style="color: var(--color-secondary); border-color: var(--color-secondary);"><i class="fas fa-ruler-combined"></i> 使用する主な評価指標</div>
<div class="info-grid">
<div class="info-card" style="background-color: rgba(255, 126, 95, 0.05);">
<h4 class="subsection-title" style="font-size: 16px; color: var(--color-dark); border-left: 3px solid var(--color-secondary); padding-left: 10px;"><i class="fas fa-id-card" style="color: var(--color-secondary);"></i> DINOv2 [47] スコア</h4>
<p><strong>目的:</strong> <span class="highlight">参照被写体</span>と<span class="highlight">生成画像</span>の間の<span class="keyword">視覚的特徴の一致度</span>を測る。</p>
<p><strong>手法:</strong> DINOv2は自己教師あり学習で訓練された視覚特徴抽出モデルです。参照被写体の画像と生成された画像からDINOv2を用いて特徴ベクトルを抽出し、それらの<span class="keyword">コサイン類似度</span>を計算します。値が高いほど、被写体の特徴がよく保持されていることを示します。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-image"></i> 参照被写体 <span style="font-size:18px; color: var(--color-secondary); margin: 0 5px;">↔️</span> <i class="fas fa-image"></i> 生成画像 ➡️ <span class="badge orange">DINOv2スコア (高いほど良い)</span>
</div>
</div>
<div class="info-card" style="background-color: rgba(255, 126, 95, 0.05);">
<h4 class="subsection-title" style="font-size: 16px; color: var(--color-dark); border-left: 3px solid var(--color-secondary); padding-left: 10px;"><i class="fas fa-images" style="color: var(--color-secondary);"></i> CLIP-I [49] スコア</h4>
<p><strong>目的:</strong> <span class="highlight">参照被写体</span>と<span class="highlight">生成画像</span>の間の<span class="keyword">意味的・視覚的な整合性</span>を測る。</p>
<p><strong>手法:</strong> こちらもCLIPモデル [49] を用います。参照被写体の画像と生成された画像をCLIPで特徴ベクトル化し、それらの間のCLIPスコア（通常はコサイン類似度）を計算します。値が高いほど、生成画像が参照被写体のアイデンティティをよく捉えていることを示します。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-user-circle"></i> 参照被写体 <span style="font-size:18px; color: var(--color-secondary); margin: 0 5px;">↔️</span> <i class="fas fa-image"></i> 生成画像 ➡️ <span class="badge orange">CLIP-Iスコア (高いほど良い)</span>
</div>
</div>
<div class="info-card" style="background-color: rgba(255, 126, 95, 0.05);">
<h4 class="subsection-title" style="font-size: 16px; color: var(--color-dark); border-left: 3px solid var(--color-secondary); padding-left: 10px;"><i class="fas fa-file-alt" style="color: var(--color-secondary);"></i> CLIP-T [49] スコア</h4>
<p><strong>目的:</strong> <span class="highlight">生成画像</span>と対応する<span class="highlight">テキストプロンプト</span>の間の<span class="keyword">意味的整合性</span>を測る。</p>
<p><strong>手法:</strong> 条件付き生成のテキスト整合性評価と同様に、生成画像とテキストプロンプトをCLIPで特徴ベクトル化し、コサイン類似度を計算します。値が高いほど、生成画像がテキストの指示内容をよく反映していることを示します。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-image"></i> 生成画像 <span style="font-size:18px; color: var(--color-secondary); margin: 0 5px;">↔️</span> <i class="fas fa-font"></i> テキストプロンプト ➡️ <span class="badge orange">CLIP-Tスコア (高いほど良い)</span>
</div>
</div>
</div>
</div>
<div class="note-box" style="margin-top: 20px; border-left-color: var(--color-secondary);">
<p class="note-title" style="color: var(--color-secondary);"><i class="fas fa-exclamation-triangle"></i> ポイント</p>
<p>これらの指標 (DINOv2, CLIP-I, CLIP-T) を総合的に見ることで、モデルが「指定された被写体の特徴をどれだけ忠実に再現し（DINOv2, CLIP-I）、かつテキストによる指示内容をどれだけ満たしているか（CLIP-T）」を評価できます。</p>
</div>
<h3 class="subsection-title" style="font-size: 22px; color: var(--color-primary); border-bottom: 2px solid var(--color-primary); padding-bottom: 5px; margin-top: 30px;"><i class="fas fa-paint-brush"></i> C.3. Style Transfer (スタイル変換)</h3>
<p>このセクションでは、<span class="keyword">スタイル変換 (Style Transfer)</span> の性能評価について説明します。これは、ある画像のコンテンツ（内容）を保ちつつ、別の参照画像のスタイル（画風、色彩など）を適用するタスクです。評価は、StyleDrop [56] の研究に倣い、<span class="highlight">テキスト整合性</span>と<span class="highlight">スタイル整合性</span>の2つの観点から行われます。</p>
<div class="two-column">
<div class="column">
<div class="info-card glass-card">
<h4 class="subsection-title" style="font-size: 18px; color: var(--color-accent1);"><i class="fas fa-text-height"></i> Text Consistency (テキスト整合性)</h4>
<p>📌 <strong>定義:</strong> 生成された画像が、入力された<span class="keyword">テキストプロンプト</span>の内容と意味的にどれだけ一致しているか。</p>
<div class="bubble-box" style="border-color: var(--color-accent1); margin-top:10px;">
<p style="margin:0;"><strong>手法:</strong> ここでも<span class="keyword">CLIP [49]</span> モデルが活躍します。</p>
<p style="margin:5px 0;"><strong>説明:</strong> 生成された画像とテキストプロンプトの埋め込みベクトル (CLIPによって抽出された特徴量) 間の<span class="keyword">コサイン類似度</span>を測定します。この値が高いほど、生成画像はテキストプロンプトで指示された内容（例えば「猫が赤いボールで遊んでいる」など）をよく反映していると評価されます。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-image"></i> 生成画像 <span style="font-size:18px; color: var(--color-accent1); margin: 0 5px;">↔️</span> <i class="fas fa-font"></i> テキストプロンプト ➡️ <span class="badge green">コサイン類似度 (高いほど良い)</span>
</div>
</div>
</div>
</div>
<div class="column">
<div class="info-card glass-card">
<h4 class="subsection-title" style="font-size: 18px; color: var(--color-accent2);"><i class="fas fa-palette"></i> Style Alignment (スタイル整合性)</h4>
<p>📌 <strong>定義:</strong> 生成された画像が、参照した<span class="keyword">スタイル画像</span>のスタイルとどれだけ一致しているか。</p>
<div class="bubble-box" style="border-color: var(--color-accent2); margin-top:10px;">
<p style="margin:0;"><strong>手法:</strong> こちらも<span class="keyword">CLIP [49]</span> モデルを用います。</p>
<p style="margin:5px 0;"><strong>説明:</strong> 生成された画像とスタイル参照画像の埋め込みベクトル間の<span class="keyword">コサイン類似度</span>を測定します。この値が高いほど、生成画像は参照画像のスタイル（例えば「ゴッホ風」「水彩画風」など）をよく再現していると評価されます。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-image"></i> 生成画像 <span style="font-size:18px; color: var(--color-accent2); margin: 0 5px;">↔️</span> <i class="fas fa-image"></i> スタイル参照画像 ➡️ <span class="badge purple">コサイン類似度 (高いほど良い)</span>
</div>
</div>
</div>
</div>
</div>
<div class="challenge-box" style="margin-top:25px; border-left-color: var(--color-secondary);">
<p class="challenge-title" style="color: var(--color-secondary);"><i class="fas fa-exclamation-circle"></i> 重要な注意点！</p>
<p>テキスト整合性とスタイル整合性の2つの指標は、<span class="keyword" style="border-bottom-color: var(--color-secondary);">同時に考慮する</span>必要があります。なぜなら、もしモデルが<span class="highlight">崩壊 (collapse)</span> してしまった場合（例えば、テキスト指示を完全に無視して、スタイル参照画像をそのままコピーして出力するような場合）、スタイル整合性は1.0という非常に高い値になってしまいます。しかし、これは望ましい結果ではありません。したがって、テキスト指示に従いつつ、スタイルも適切に反映できているかを両方の指標でバランス良く評価することが重要です。</p>
<div style="text-align:center; margin-top:15px; font-family: 'Yomogi', cursive;">
<span style="font-size: 16px; color: var(--color-dark);">目指すは...</span><br/>
<strong style="font-size: 18px; color: var(--color-secondary); display: block; margin-top: 5px;">「テキスト内容もバッチリ👌」 <span style="font-size: 22px;">かつ</span> 「スタイルもイケてる🎨」</strong>
</div>
</div>
<div class="note-box" style="margin-top: 25px;">
<p class="note-title"><i class="fas fa-info-circle"></i> 補足: 論文中の図について</p>
<p>このセクションでは直接図を参照していませんが、Appendix A には図13、Appendix B には図14 が掲載されています。これらの図は、それぞれ言語指示の例や、FLUX.1-devモデルのファインチューニングにおける工夫を示しており、本評価指標の文脈を理解する上で間接的に関連します。</p>
<p>例えば、図13は言語指示のフォーマット例を示しており、これが「テキスト整合性」評価の際の「テキストプロンプト」に該当します。</p>
<img alt="Figure 13. Language instruction examples" class="figure-image" src="concept_fusion_style_subject_layout.jpg"/>
<p style="text-align:center; font-size:12px; color: var(--color-gray);">図13. 連結画像のレイアウト、タスク意図、ターゲット画像の内容に関するプロンプトを含む言語指示の例。</p>
<p>図14は、特定のモデル (FLUX.1-dev) をファインチューニングする際の技術的な詳細を示しています。この改善が、最終的な生成画像の品質、ひいては本セクションで述べられている各種評価指標のスコアに影響を与える可能性があります。</p>
<img alt="Figure 14. Effects of separate mean and shift" class="figure-image" src="flux1_dev_mean_shift_comparison.jpg"/>
<p style="text-align:center; font-size:12px; color: var(--color-gray);">図14. FLUX.1-devのファインチューニングにおける平均とシフトの分離効果。</p>
</div>
</div>
</div>
</body>
</html>
