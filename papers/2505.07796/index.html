<!DOCTYPE html>

<html lang="ja">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Learning Dynamics in Continual Pre-Training for Large Language Models解説</title>
<link href="style.css" rel="stylesheet"/>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
        window.MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)'], ['\\\\(', '\\\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]'], ['\\\\[', '\\\\]']]
          }
        };
    </script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet"/>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N7SLXFTVBP"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'G-N7SLXFTVBP');
</script>
</head>
<body>
<div class="container">
<!-- ヘッダー部分 -->
<div class="header">
<div class="title-area">
<h1 class="title">Learning Dynamics in Continual Pre-Training for Large Language Models</h1>
<p class="subtitle">None</p>
</div>
<div class="meta-info">
<p>論文解説</p>
</div>
</div>
<div class="section-card" id="Abstract">
<h2 class="section-title"><i class="fas fa-microscope"></i> Abstract</h2>
<div class="bubble-box">
<p style="font-size: 16px; text-align: center; font-family: 'Yomogi', cursive;">
<i class="fas fa-bullhorn"></i> この論文の核心を一言で言うと… <br/>
            大規模言語モデルの<strong>継続的事前学習（CPT）</strong>における「学習のクセ」を解き明かし、それを予測・制御するための新しい法則<strong class="keyword">「CPTスケーリング則」</strong>を提案します！これにより、モデルの性能をより賢く、効率的に調整できるようになります。
        </p>
</div>
<p><i class="fas fa-book-open"></i> このアブストラクトでは、論文の主要な目的、手法、発見、そしてその意義について概説します。情報系の大学院1年生の皆さんが、この分野の雰囲気を掴めるように、専門用語も丁寧に解説していきますね。</p>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-puzzle-piece"></i> CPTとは？そして本研究の挑戦</h3>
<p><strong class="keyword">継続的事前学習（Continual Pre-Training, CPT）</strong>は、強力な<strong class="keyword">基盤モデル（Foundation Models）</strong>を、特定の<strong class="keyword">下流タスク（Downstream Tasks）</strong>、例えば医療、法律、金融といった専門分野向けに効率よく適応させるための人気の技術です。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="definition-title" style="font-family: 'Yomogi', cursive;"><i class="fas fa-cogs"></i> 基盤モデル (Foundation Models)</p>
<p>非常に大規模なデータセットで訓練された、汎用的なAIモデルのこと。これ自体でも様々なタスクをこなせますが、特定のタスクに特化させることでさらに性能が向上します。例：GPT-3, BERTなど。</p>
<p style="text-align: center;"><i class="fas fa-landmark fa-2x" style="color: var(--color-primary);"></i></p>
</div>
<div class="info-card glass-card">
<p class="definition-title" style="font-family: 'Yomogi', cursive;"><i class="fas fa-tasks"></i> 下流タスク (Downstream Tasks)</p>
<p>基盤モデルを特定の応用目的（例：質問応答、文章生成、翻訳、専門分野の知識活用など）に合わせてファインチューニング（微調整）した後のタスクのことです。</p>
<p style="text-align: center;"><i class="fas fa-bullseye fa-2x" style="color: var(--color-secondary);"></i></p>
</div>
</div>
<p>この研究では、<strong class="keyword">大規模言語モデル（Large Language Models, LLM）</strong>のCPTプロセス全体を通して、<strong class="keyword">学習ダイナミクス（Learning Dynamics）</strong>、つまりモデルがどのように学習していくのか、その振る舞いを詳細に探ります。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-brain"></i> 大規模言語モデル (LLM)</p>
<p>大量のテキストデータで訓練された自然言語処理モデル。人間のように自然な文章を生成したり、理解したりする能力を持ちます。例：ChatGPTの基盤技術など。</p>
</div>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-chart-line"></i> 学習ダイナミクス (Learning Dynamics)</p>
<p>モデルが訓練データから学習する過程での、性能や内部状態の時間的な変化や振る舞いのこと。学習が順調に進んでいるか、停滞しているか、振動しているかなどを分析します。</p>
</div>
</div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-binoculars"></i> 研究の焦点と重要な観察</h3>
<p>特に注目するのは、一般的な知識を扱う<strong class="keyword">一般ドメイン（General Domain）</strong>の性能と、特定の専門分野を扱う<strong class="keyword">下流ドメイン（Downstream Domain）</strong>の性能が、CPTの各訓練ステップでどのように変化していくかです。ドメイン性能は、<strong class="keyword">検証損失（Validation Losses）</strong>という指標で測定します。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-clipboard-check"></i> 検証損失 (Validation Loss) とは？</p>
<p>モデルの性能を評価するための指標の一つ。訓練データとは別の「検証データ」を使って計算され、モデルが未知のデータに対してどれだけ上手く予測できるか（汎化性能）を示します。損失が低いほど、モデルの性能が良いとされます。</p>
</div>
<p>私たちの観察によると、CPTにおける損失曲線の変化は、実は<strong>「ある既存の損失曲線から、別の（目には見えない）隠れた損失曲線へと移行していくプロセス」</strong>として捉えることができます。この複雑な動きは、以下の2つの主要な効果を切り離して考えることで説明できることが分かりました。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-exchange-alt fa-2x" style="color: var(--color-accent1);"></i>
<h4><i class="fas fa-draw-polygon"></i> 分布シフト (Distribution Shift)</h4>
<p>元の一般データと新しい専門データの性質（分布）が異なることによる影響。</p>
</div>
<div class="feature-item">
<i class="fas fa-sliders-h fa-2x" style="color: var(--color-accent2);"></i>
<h4><i class="fas fa-sort-amount-down"></i> 学習率アニーリング (Learning Rate Annealing)</h4>
<p>学習の進行に合わせて学習率（モデルが一度に学習する量）を徐々に小さくしていく手法による影響。</p>
</div>
</div>
<p style="text-align:center; margin-top:10px;">
<i class="fas fa-long-arrow-alt-right fa-2x" style="color: var(--color-primary); transform: rotate(90deg);"></i>
</p>
<p style="text-align:center; font-family: 'Yomogi', cursive; font-size: 16px;">これらの要素が、CPT中のモデルの振る舞いを理解する鍵となります！</p>
</div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-magic"></i> 提案：CPTスケーリング則</h3>
<p>本研究では、上記の<strong class="highlight">分布シフト</strong>と<strong class="highlight">学習率アニーリング</strong>という2つの要因を組み合わせた新しい<strong class="keyword">CPTスケーリング則（CPT Scaling Law）</strong>を導き出しました。</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-scroll"></i> CPTスケーリング則とは？</p>
<p>CPTプロセスにおけるモデルの損失を、訓練の任意のステップや、さまざまな<strong class="keyword">学習率スケジュール（Learning Rate Schedules, LRS）</strong>のもとで予測可能にする数理的な法則です。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-calendar-alt"></i> 学習率スケジュール (LRS)</p>
<p>訓練中に学習率をどのように変化させるかの計画のこと。例えば、最初は大きな学習率で素早く学習し、徐々に学習率を小さくして安定させる、といった戦略があります（例：定数、ステップ減衰、コサインアニーリングなど）。</p>
</div>
</div>
</div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-lightbulb"></i> この法則がもたらす理解と応用</h3>
<p>私たちの提案するCPTスケーリング則によって、CPTにおけるいくつかの重要な要素について、より包括的な理解が得られます。例えば…</p>
<ul class="unstyled-list" style="padding-left: 20px;">
<li><i class="fas fa-battery-half" style="color: var(--color-accent1);"></i> <strong class="keyword">損失ポテンシャル（Loss Potential）</strong>: 学習率アニーリングによって、将来的にどれだけ損失が減少しうるかの可能性。</li>
<li><i class="fas fa-mountain" style="color: var(--color-accent2);"></i> <strong class="keyword">ピーク学習率（Peak Learning Rate）</strong>: 学習率スケジュールの最大値。</li>
<li><i class="fas fa-shoe-prints" style="color: var(--color-accent3);"></i> <strong class="keyword">訓練ステップ（Training Steps）</strong>: モデルを訓練する回数。</li>
<li><i class="fas fa-history" style="color: var(--color-secondary);"></i> <strong class="keyword">リプレイ比率（Replay Ratio）</strong>: CPT中に、過去の一般ドメインのデータをどの程度の割合で混ぜて学習させるか。</li>
</ul>
<p>さらに、このアプローチは非常に実用的です。例えば、一般ドメインの性能と専門ドメインの性能のバランスを取るなど、様々なCPTの目標に応じて、これらの<strong class="highlight">訓練ハイパーパラメータをカスタマイズする</strong>のに役立ちます。</p>
<p style="text-align: center; font-family: 'Yomogi', cursive; font-size: 16px;">
<i class="fas fa-balance-scale-right"></i> 一般性能 vs <i class="fas fa-microchip"></i> ドメイン特化性能... 最適なバランスを見つけよう！
        </p>
</div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-vial"></i> 実験による実証</h3>
<p>広範囲な実験を通じて、私たちが提案するスケーリング則が、様々なCPT用データセットや訓練ハイパーパラメータの設定においても有効であることを示しました。これは、この法則の一般性と実用性を示唆しています。</p>
<p style="text-align: center;">
<span class="badge yellow"><i class="fas fa-check-circle"></i> 様々なデータセットで検証</span>
<span class="badge yellow"><i class="fas fa-check-circle"></i> 様々なハイパーパラメータで検証</span>
</p>
</div>
<div class="content-box glass-card" style="padding: 20px; border: 2px dashed var(--color-primary);">
<h3 class="subsection-title" style="margin-top: 0;"><i class="fas fa-info-circle"></i> 背景と課題：なぜCPTが重要なのか？</h3>
<p>LLMは目覚ましい能力を持っていますが、特定の専門分野（例：<strong class="highlight">コーディング <i class="fas fa-code"></i></strong>、<strong class="highlight">金融 <i class="fas fa-dollar-sign"></i></strong>、<strong class="highlight">数学 <i class="fas fa-calculator"></i></strong>など）でさらに能力を発揮させるためには、追加の学習が必要です。しかし、巨大なLLMを一から再訓練するのは非常にコストがかかります <i class="fas fa-wallet"></i>。</p>
<p>CPTは、この<strong class="highlight">再訓練コストを大幅に削減しつつ</strong>、特定のドメイン知識を効率的にモデルに教え込む手法として注目されています (Chen et al., 2023a; C¸ag˘atay Yıldız et al., 2024; Ibrahim et al., 2024)。</p>
<p>CPTの主な関心事は、一般ドメインの知識を維持しながら、新しい専門ドメインの性能をいかに向上させるか、という点にあります。しかし、よく知られている課題として<strong class="keyword">「破滅的忘却（Catastrophic Forgetting）」</strong>があります。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-brain"></i> <i class="fas fa-fire-extinguisher"></i> 破滅的忘却 (Catastrophic Forgetting)</p>
<p>新しいタスクやデータを学習した際に、以前に学習したタスクやデータの知識を急激に忘れてしまう現象のこと。人間の学習ではあまり見られませんが、ニューラルネットワークでは頻繁に起こりうる問題です (French, 1999; Gupta et al., 2023)。CPTではこの問題をどう軽減するかが重要です。</p>
</div>
</div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-stream"></i> 関連するこれまでの研究</h3>
<p>近年、継続的事前学習の文脈で、データの転移に関するいくつかの<strong class="keyword">スケーリング則</strong>が提案されています。これらは、モデルのサイズやデータ量が性能にどう影響するかを数式で表そうとする試みです。</p>
<div class="info-grid">
<div class="info-card">
<p class="note-title"><i class="fas fa-users"></i> Hernandez et al. (2021b), Barnett (2024)</p>
<p>ファインチューニングするデータセットのサイズやモデルのサイズに応じて、どれだけ効果的にデータ（知識）が転移されるかを記述する法則を発見しました。</p>
</div>
<div class="info-card">
<p class="note-title"><i class="fas fa-users"></i> Que et al. (2024), Gu et al. (2024)</p>
<p>一般ドメインの性能と専門ドメインの性能のバランスを取るために、最適な<strong class="highlight">リプレイ比率</strong>（過去のデータをどの程度混ぜるか）を見つけるための法則を提案しました。</p>
</div>
</div>
</div>
<div class="content-box framework-box">
<h3 class="subsection-title" style="margin-top:0; color: var(--color-primary); border-left-color: var(--color-primary);"><i class="fas fa-question-circle"></i> 本研究が解き明かしたい疑問 (Research Questions)</h3>
<p>しかし、これまでの研究では、CPTプロセス全体を通じた学習ダイナミクス、特に一般ドメインと専門ドメインの性能がどのように変動するかを定量的に記述しようとする試みはほとんどありませんでした。そこで、私たちは以下の2つの主要なリサーチクエスチョン（RQ）を設定しました。</p>
<div class="two-column">
<div class="column challenge-box">
<h4 class="challenge-title" style="font-family: 'Yomogi', cursive;"><i class="fas fa-search"></i> RQ1: 包括的な法則の発見</h4>
<p>最終的なCPT性能に影響を与える<strong>できるだけ多くの変数</strong>（学習率、データ量、モデル特性など）を含み、かつ<strong class="highlight">正確なCPTの法則</strong>を見つけ出すことができるか？</p>
<p><i class="fas fa-cogs"></i> <strong>意義:</strong> この問いを探求することで、研究者はCPT性能に影響する様々な要因を明確に理解し、予測に基づいてハイパーパラメータを最適化できるようになります。</p>
</div>
<div class="column challenge-box">
<h4 class="challenge-title" style="font-family: 'Yomogi', cursive;"><i class="fas fa-microchip"></i> RQ2: 学習過程の追跡</h4>
<p>過去の研究のように最終的な性能だけでなく、CPTプロセスの<strong>各ステップにおけるLLMの性能</strong>を追跡し、理解することができるか？</p>
<p><i class="fas fa-chart-area"></i> <strong>意義:</strong> この問いに答えることで、LLMがCPTの各段階でどのように学習を進めているのか、そのダイナミクスを深く理解できます。これは、今後のCPT研究にとって重要な理論的指針や新たな洞察をもたらします。</p>
</div>
</div>
</div>
<p style="margin-top: 20px; text-align: right; font-style: italic; font-family: 'Kaisei Decol', serif;">
<i class="fas fa-feather-alt"></i> このAbstractを通じて、本研究の輪郭を掴んでいただけたでしょうか。詳細は本文で！
    </p>
</div>
<div class="section-card" id="1._Introduction">
<h2 class="section-title"><i class="fas fa-book-open"></i>1. Introduction</h2>
<div class="bubble-box">
<p style="font-family: 'Yomogi', cursive; font-size: 1.1em; color: var(--color-primary); text-align: center;">🚀 <strong>このセクションのゴール</strong> 🚀</p>
<p>この「はじめに」のセクションでは、<strong>大規模言語モデル (LLM)</strong> の世界でますます重要になっている<strong>継続的事前学習 (Continual Pre-Training, CPT)</strong> の学習ダイナミクス、つまり「学習がどのように進んでいくか」の謎に迫ります。特に、CPTを行うと一般的な知識や能力と、特定の専門分野の知識や能力がどう変化するのか？そのメカニズムを解き明かそうとしています。著者たちは、CPT中のモデルの性能変化（損失カーブ）が、実は<span class="keyword">「データの性質の違い（分布シフト）」</span>と<span class="keyword">「学習の進め方（学習率アニーリング）」</span>という2つの大きな要因で説明できることを突き止めました。そして、これらを数式で表現した新しい<span class="keyword">「CPTスケーリング則」</span>を提案しています。この法則を使えば、CPTでモデルがどう賢くなっていくかを予測したり、学習方法を最適化したりできるかもしれない、というワクワクする内容です！</p>
</div>
<h3 class="subsection-title"><i class="fas fa-brain"></i> LLMの進化と継続的事前学習 (CPT) の登場</h3>
<div class="content-box">
<p>ここ数年で、<span class="keyword">大規模言語モデル (LLMs)</span> は目覚ましい進化を遂げ、文章を作ったり、質問に答えたり、翻訳したりと、まるで人間のように言葉を操る能力で、学術界だけでなく産業界からも熱い視線を集めています (Dubey et al., 2024; OpenAI, 2023)。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-lightbulb"></i> キーワード解説: 大規模言語モデル (LLMs)</div>
<p><strong>大規模言語モデル (LLMs)</strong> とは、大量のテキストデータで学習された、非常に大きなニューラルネットワークモデルのことです。GPT-3やBERTなどが有名ですね。人間が書いたような自然な文章を生成したり、質問応答、翻訳、要約など、様々な言語タスクで高い性能を発揮します。</p>
</div>
<p>そんなLLMを、特定の専門分野（例えば、プログラミング、金融、数学など）でさらにパワーアップさせたい！でも、毎回ゼロから巨大なモデルを学習し直すのは、時間もお金もかかって大変… 😥</p>
<p>そこで登場したのが <span class="keyword">継続的事前学習 (Continual Pre-Training, CPT)</span> という技術です。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-book-reader"></i> 定義: 継続的事前学習 (Continual Pre-Training, CPT)</div>
<p><strong>継続的事前学習 (CPT)</strong> とは、既に大規模な汎用データで事前学習されたLLMを、特定の専門分野のデータを使って追加学習（ファインチューニングに近いですが、ここでは事前学習の延長線上にあるという意味合いが強いです）することで、その分野での能力を向上させる手法です。これにより、ゼロからの再学習に伴う莫大なコストを削減しつつ、特定のタスクに特化したLLMを効率的に開発できます (Chen et al., 2023a; C¸ ag˘ atay Yıldız et al., 2024; Ibrahim et al., 2024)。</p>
<div style="text-align: center; margin-top: 15px;">
<div style="display: inline-block; text-align: center; margin: 10px; font-family: 'Yomogi', cursive;">
<i class="fas fa-brain fa-3x" style="color: var(--color-primary);"></i>
<p style="margin-top: 5px;">汎用LLM</p>
</div>
<span style="font-size: 2em; color: var(--color-accent1); vertical-align: middle;">+</span>
<div style="display: inline-block; text-align: center; margin: 10px; font-family: 'Yomogi', cursive;">
<i class="fas fa-folder-plus fa-3x" style="color: var(--color-secondary);"></i>
<p style="margin-top: 5px;">専門分野データ</p>
</div>
<span style="font-size: 2em; color: var(--color-accent1); vertical-align: middle;">➔</span>
<div style="display: inline-block; text-align: center; margin: 10px; font-family: 'Yomogi', cursive;">
<i class="fas fa-star fa-3x" style="color: var(--color-accent2);"></i>
<p style="margin-top: 5px;">専門LLM (CPT後)</p>
</div>
</div>
<p style="text-align:center; font-size:0.9em; color: var(--color-gray);">図：CPTの概念図</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> CPT中の性能変化の追跡</h3>
<div class="content-box">
<p>これまでの研究 (Gupta et al., 2023; Ibrahim et al., 2024; Que et al., 2024) に倣い、この論文ではCPTプロセス中のモデルの性能変化を、それぞれのドメイン（汎用ドメインと専門ドメイン）に対応する<span class="keyword">検証損失 (validation loss)</span> を使って追跡します。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-lightbulb"></i> キーワード解説: 検証損失 (Validation Loss)</div>
<p><strong>検証損失 (Validation Loss)</strong> とは、機械学習モデルの学習中に、学習データとは別の「検証データ」を使って計算される損失（誤差）のことです。学習データに対する損失が下がっていても、検証データに対する損失が上がってしまう場合、モデルが学習データに過剰適合（過学習）している可能性があります。検証損失は、モデルの汎化性能、つまり未知のデータに対する性能を測る重要な指標となります。</p>
<div style="text-align: center; margin-top: 15px;">
<i class="fas fa-file-alt fa-2x" style="color: var(--color-accent1); margin-right: 5px;"></i> <span style="font-family: 'Yomogi', cursive;">学習データ (モデルが見るデータ)</span> <i class="fas fa-arrow-right" style="margin: 0 10px; color: var(--color-gray);"></i> <i class="fas fa-cogs fa-2x" style="color: var(--color-primary); margin-right: 5px;"></i> <span style="font-family: 'Yomogi', cursive;">モデル学習</span>
<br/>
<i class="fas fa-file-signature fa-2x" style="color: var(--color-accent2); margin-right: 5px;"></i> <span style="font-family: 'Yomogi', cursive;">検証データ (モデルが見ないデータ)</span> <i class="fas fa-arrow-right" style="margin: 0 10px; color: var(--color-gray);"></i> <i class="fas fa-clipboard-check fa-2x" style="color: var(--color-secondary); margin-right: 5px;"></i> <span style="font-family: 'Yomogi', cursive;">性能評価 (検証損失)</span>
</div>
<p style="text-align:center; font-size:0.9em; color: var(--color-gray);">図：検証損失の役割</p>
</div>
<p>著者たちは、CPT中の損失カーブ（学習ステップごとの損失の変化を示すグラフ）が、実はある種の<span class="keyword">「転移カーブ (transfer curve)」</span>であることを見出しました。そして、このカーブは、<span class="highlight">「分布シフト (distribution shift)」</span>と<span class="highlight">「学習率アニーリング (LR annealing)」</span>という2つの効果を切り離して考えることで説明できると主張しています。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-primary);"><i class="fas fa-exchange-alt"></i> 分布シフト (Distribution Shift)</p>
<p>元の学習データ（事前学習データ、PTデータ）と新しい学習データ（CPTデータ）の性質がどれだけ違うか、ということです。例えば、一般的なニュース記事で学習したモデルを、法律の専門文書でCPTする場合、データの言葉遣いやトピックが大きく異なるため、分布シフトは大きいと言えます。分布シフトが大きいほど、転移損失カーブのずれも顕著になります。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-globe-americas fa-2x" style="color: var(--color-primary);"></i> <span style="font-family: 'Yomogi', cursive;">一般的なデータ</span>
<i class="fas fa-long-arrow-alt-right fa-2x" style="margin: 0 15px; color: var(--color-gray);"></i>
<i class="fas fa-microscope fa-2x" style="color: var(--color-secondary);"></i> <span style="font-family: 'Yomogi', cursive;">専門分野データ</span>
<p style="margin-top:5px; font-size:0.9em; color: var(--color-gray);">データの性質の違いが大きいほど、分布シフトも大きい</p>
</div>
</div>
<div class="info-card glass-card">
<p class="note-title" style="color: var(--color-secondary);"><i class="fas fa-sliders-h"></i> 学習率アニーリング (LR Annealing)</p>
<p>学習率を学習の進行とともに徐々に小さくしていくテクニックのことです。最初は大きな学習率で大まかに学習し、徐々に学習率を小さくすることで、より細かく最適な解に近づこうとします。この学習率アニーリングは、事前学習 (PT) 段階とCPT段階の両方で、局所的な損失の低下（つまり性能向上）をもたらします。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-tachometer-alt fa-2x" style="color: var(--color-primary);"></i> <span style="font-family: 'Yomogi', cursive;">学習初期: 高い学習率</span>
<i class="fas fa-arrow-down fa-2x" style="margin: 0 15px; color: var(--color-gray);"></i>
<i class="fas fa-tachometer-alt fa-2x" style="color: var(--color-secondary); opacity: 0.6;"></i> <span style="font-family: 'Yomogi', cursive;">学習終盤: 低い学習率</span>
<p style="margin-top:5px; font-size:0.9em; color: var(--color-gray);">学習率を徐々に下げることで安定した学習を目指す</p>
</div>
</div>
</div>
<p>これらの予備的な観察と実験的検証を通じて、著者たちはこれら2つの要因を統合した<span class="keyword">CPTスケーリング則 (CPT scaling law)</span> を発見しました。この法則を使うと、実際の損失変化をうまく再現したり、予測したりすることが可能になります。</p>
<div class="framework-box">
<div class="framework-title"><i class="fas fa-ruler-combined"></i> CPTスケーリング則のイメージ</div>
<p>CPTの損失カーブ =  分布シフトの影響 + 学習率アニーリングの影響</p>
<p style="text-align: center;">
<span style="font-family: 'Kaisei Decol', serif; font-size: 1.5em;">\( L_{CPT}(t) \)</span>
<span style="font-family: 'Yomogi', cursive; font-size: 1.5em;"> ≈ </span>
<span style="font-family: 'Kaisei Decol', serif; font-size: 1.5em;">\( f(\text{Distribution Shift}) \)</span>
<span style="font-family: 'Yomogi', cursive; font-size: 1.5em;"> + </span>
<span style="font-family: 'Kaisei Decol', serif; font-size: 1.5em;">\( g(\text{LR Annealing}) \)</span>
</p>
<p style="text-align:center; font-size:0.9em; color: var(--color-gray);">※ これは概念的な表現であり、実際の数式はもっと複雑です。</p>
</div>
</div>
<img alt="Figure 1: CPTにおける損失曲線の変化の様子" src="https://gist.githubusercontent.com/MonalizaTsai/12b5432a0853b7ebc1ab827983d82164/raw/bf1cd1c1426e12d889c8edc8f84e5e04a311189a/Figure1.png"/>
<div class="note-box">
<div class="note-title"><i class="fas fa-chart-pie"></i> 図1の解説: CPTにおける損失曲線の変化</div>
<p>この図は、CPTプロセス中の検証損失の変化を示しています。いくつかの重要なポイントがあります。</p>
<ul>
<li><strong>(a) 定数学習率の場合の理想的な曲線:</strong> もし学習率がずっと一定で、データの分布も変わらなかったらどうなるか、という理論上の線です。
                <ul>
<li><span style="color: #1f77b4;">青線 (Pre-Training with Dpt)</span>: 汎用データ($D_{pt}$)だけで学習し続けた場合の理想的な損失。</li>
<li><span style="color: #ff7f0e;">オレンジ線 (Pre-Training with Dcpt)</span>: 専門データ($D_{cpt}$)だけで学習し続けた場合の理想的な損失（ただし、実際にはCPT開始時点からの学習）。</li>
<li><span style="color: #2ca02c;">緑線 (Continual Pre-Training)</span>: 実際に汎用データで事前学習後、専門データでCPTを行った場合の損失。これが本論文の分析対象です。</li>
<li><span class="keyword">Transfer Point</span>: CPTを開始する時点。</li>
<li><span class="keyword">Distribution Shift</span>: 青線と緑線の間の差が、データ分布の違いによる影響を表します。</li>
</ul>
</li>
<li><strong>(b) $D_{pt}$ (FineWeb) 検証損失:</strong> 汎用データ (FineWebというデータセット) に対する実際の検証損失。CPTを開始すると、専門データに特化していくため、汎用データに対する性能は少し悪化する（損失が上がる）傾向が見られます。</li>
<li><strong>(c) $D_{cpt}$ (Knowledge Pile) 検証損失:</strong> 専門データ (Knowledge Pileというデータセット) に対する実際の検証損失。CPTを開始すると、専門データに適合していくため、性能が向上する（損失が下がる）傾向が見られます。</li>
<li><strong>(d), (e), (f) は学習率スケジュールがWSD (Warmup-Stable-Decay) の場合:</strong> こちらは学習率を途中で変化させる現実的な設定です。(a)-(c) と同様の傾向が見られますが、学習率の変化によって損失カーブがより複雑な形になっています。特に、学習率を上げるウォームアップ期間や、下げる減衰期間で損失が大きく動くことが分かります。</li>
</ul>
<p><strong><i class="fas fa-exclamation-triangle" style="color: var(--color-accent3);"></i> 注目ポイント:</strong> CPTの損失カーブ（緑線）は、<span style="text-decoration: underline; text-decoration-color: var(--color-accent1);">「汎用データでの学習を続けた場合の理想的な線（青線）」から徐々に離れていき、「専門データだけで学習した場合の理想的な線（オレンジ線）」に近づいていく</span>、というダイナミックな動きをしています。これが「転移カーブ」と呼ばれる所以です。また、学習率の変化（(d)-(f)の山や谷）も損失に大きな影響を与えています。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-tools"></i> 提案するCPTスケーリング則の意義</h3>
<div class="content-box">
<p>著者たちが提案するCPTスケーリング則は、CPTの学習ダイナミクスにおける重要な変数について、包括的なモデル化と理解を提供します。具体的には、以下のような要素です。</p>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-battery-half fa-2x icon-item"></i>
<p><span class="keyword">損失ポテンシャル (Loss potential)</span></p>
<p style="font-size: 0.9em;">学習率アニーリングによって将来的にどれだけ損失を下げられるかの潜在的な可能性。詳細は後述。</p>
</div>
<div class="feature-item">
<i class="fas fa-rocket fa-2x icon-item"></i>
<p><span class="keyword">ピーク学習率 (Peak LR)</span></p>
<p style="font-size: 0.9em;">学習中に設定される最大の学習率。</p>
</div>
<div class="feature-item">
<i class="far fa-clock fa-2x icon-item"></i>
<p><span class="keyword">学習ステップ数 (Training steps)</span></p>
<p style="font-size: 0.9em;">モデルをどれくらいの回数更新するか。</p>
</div>
<div class="feature-item">
<i class="fas fa-random fa-2x icon-item"></i>
<p><span class="keyword">リプレイ比率 (Replay ratio)</span></p>
<p style="font-size: 0.9em;">CPT中に、元の事前学習データをどれくらいの割合で混ぜて学習させるか。</p>
</div>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-lightbulb"></i> キーワード解説: 損失ポテンシャル (Loss Potential)</div>
<p><strong>損失ポテンシャル</strong>とは、学習率アニーリング（学習率を徐々に下げること）によって、将来的に損失がどれだけ低下する可能性があるかを示す指標です。まだ学習率が高い状態（＝アニーリングがあまり進んでいない状態）のモデルは、これから学習率を下げることでさらに損失を大きく下げられる「伸びしろ」があると考えられ、これを「損失ポテンシャルが高い」と表現します。この概念は、CPTにおいて、どの程度の事前学習済みモデルを使うべきかを考える上で重要になります。</p>
<div style="text-align: center; margin: 15px 0;">
<div style="display: inline-block; border: 2px dashed var(--color-accent2); padding: 10px; border-radius: 8px; margin-right: 20px; position: relative;">
<span class="badge purple" style="position: absolute; top: -10px; left: 50%; transform: translateX(-50%);">高い損失ポテンシャル</span>
<i class="fas fa-arrow-alt-circle-down fa-3x" style="color: var(--color-accent2);"></i>
<p style="font-family: 'Yomogi', cursive; margin-top: 5px;">大幅な損失低下の余地あり</p>
</div>
<div style="display: inline-block; border: 2px dashed var(--color-gray); padding: 10px; border-radius: 8px; position: relative;">
<span class="badge" style="background-color: var(--color-gray); position: absolute; top: -10px; left: 50%; transform: translateX(-50%);">低い損失ポテンシャル</span>
<i class="fas fa-arrow-alt-circle-down fa-3x" style="color: var(--color-gray); opacity: 0.5;"></i>
<p style="font-family: 'Yomogi', cursive; margin-top: 5px;">損失低下の余地は小さい</p>
</div>
</div>
</div>
<p>さらに、このスケーリング則は、これらの要因がCPTの各ステップで最終的なモデルの性能にどのように複合的に影響するかを示し、これらの要因を調整することでモデルを最適化する方法の指針を与えます。</p>
<div class="bubble-box">
<p style="font-family: 'Yomogi', cursive; font-size: 1.1em; color: var(--color-primary);"><i class="fas fa-search-plus"></i> スケーリング則の導出と応用から見えてきた重要な結論 <i class="fas fa-glasses"></i></p>
<ul class="unstyled-list" style="padding-left: 20px;">
<li style="margin-bottom: 10px;">
<span class="badge blue">発見1</span> ✏️ <span class="highlight">損失ポテンシャルが高い事前学習済みモデル</span>ほど、下流の専門ドメインにより良く適応できる。
                    <div style="text-align: center; margin-top: 5px;">
<i class="fas fa-seedling" style="color: var(--color-accent1); font-size: 1.2em;"></i> <span style="font-family: 'Yomogi', cursive;">PTモデル (高ポテンシャル)</span>
<i class="fas fa-long-arrow-alt-right" style="color: var(--color-gray); margin: 0 10px; font-size: 1.2em;"></i>
<i class="fas fa-graduation-cap" style="color: var(--color-accent2); font-size: 1.2em;"></i> <span style="font-family: 'Yomogi', cursive;">下流ドメインへの高い適応力</span>
</div>
</li>
<li style="margin-bottom: 10px;">
<span class="badge orange">発見2</span> 📝 もしCPTの学習時間（turning length, 後で説明されます）が無限に長くなった場合、汎用ドメインの性能は必然的に低下する。これは、汎用の事前学習が十分すぎるか、あるいは汎用ドメインと専門ドメインの間の分布シフトが非常に大きいことを意味する。
                    <div style="text-align: center; margin-top: 5px;">
<i class="far fa-clock" style="color: var(--color-secondary); font-size: 1.2em;"></i> <span style="font-family: 'Yomogi', cursive;">CPT学習時間 → ∞</span>
<i class="fas fa-long-arrow-alt-right" style="color: var(--color-gray); margin: 0 10px; font-size: 1.2em;"></i>
<i class="fas fa-thumbs-down" style="color: var(--color-accent3); font-size: 1.2em;"></i> <span style="font-family: 'Yomogi', cursive;">汎用ドメイン性能 ↓</span>
</div>
</li>
<li style="margin-bottom: 10px;">
<span class="badge purple">発見3</span> 📊 基本的に、特定の目標（例えば、汎用性能と専門性能のバランス）に向けて、このスケーリング則は、事前学習済みモデルの最適な損失ポテンシャル、ピーク学習率、事前学習データセットのリプレイ比率など、最適な学習ハイパーパラメータを予測することができる。
                    <div style="text-align: center; margin-top: 5px;">
<i class="fas fa-bullseye" style="color: var(--color-primary); font-size: 1.2em;"></i> <span style="font-family: 'Yomogi', cursive;">特定の目標</span>
<i class="fas fa-cogs" style="color: var(--color-gray); margin: 0 10px; font-size: 1.2em;"></i>
<i class="fas fa-magic" style="color: var(--color-accent1); font-size: 1.2em;"></i> <span style="font-family: 'Yomogi', cursive;">最適なハイパーパラメータ予測</span>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="section-card" id="2._Pilot_Observation">
<h2 class="section-title"><i class="fas fa-binoculars"></i> 2. Pilot Observation</h2>
<div class="content-box">
<p>このセクションでは、大規模言語モデル（LLM）の<span class="keyword">継続的事前学習（Continual Pre-Training, CPT）</span>プロセスにおいて、モデルの性能がどのように変化していくか、そのダイナミクスを予備的に観察します。</p>
<p>主な目的は以下の2点です：</p>
<ul class="unstyled-list">
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> CPT中に、<span class="highlight">汎用ドメイン</span>（例：一般的なウェブテキスト）と<span class="highlight">下流ドメイン</span>（例：特定の専門分野のテキスト）それぞれにおけるモデルの性能が、学習ステップごとにどのように進化するかを調査する。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 特に、CPTにおける<span class="keyword">検証損失曲線</span>の振る舞いに注目し、それがどのような要因によって変動するのかを探る。</li>
</ul>
<p>この観察を通じて、論文の中心的な主張である「CPT損失曲線は、<span class="highlight">ある隠れた事前学習曲線から別の隠れた事前学習曲線への遷移</span>として理解でき、この遷移は<span class="keyword">分布シフト</span>と<span class="keyword">学習率アニーリング</span>という2つの効果に分解して記述できる」という仮説の根拠を示します。最終的には、これらの知見を基に、CPTの性能を予測するための新しい<span class="keyword">スケーリング則</span>を導出することを目指します。</p>
</div>
<h3 class="section-title"><i class="fas fa-tasks"></i> 2.1. Task Formulation</h3>
<div class="content-box">
<p>まず、CPTプロセスにおける性能評価の枠組みを定義します。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i> 基本方針</p>
<p>CPTプロセスを通じて、汎用ドメインと下流ドメインの両方における性能のダイナミクスを調査します。過去の研究（Ibrahim et al., 2024; Que et al., 2024; Gu et al., 2024; Hernandez et al., 2021a）に倣い、モデルの性能は対応する<span class="keyword">検証損失（validation loss）</span>を調べることで評価します。したがって、分析の焦点は、事前学習（PT）用と継続的事前学習（CPT）用の両方の検証データセットにおける損失曲線となります。</p>
</div>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card glass-card">
<h4><i class="fas fa-database" style="color: var(--color-primary);"></i> 事前学習データセット (PT Dataset)</h4>
<p>これを \( D_{pt} \) と表記します。これは一般的な広範囲のテキストデータを含みます。</p>
<p><span class="badge blue">汎用ドメイン</span></p>
</div>
<div class="info-card glass-card">
<h4><i class="fas fa-database" style="color: var(--color-secondary);"></i> 継続的事前学習データセット (CPT Dataset)</h4>
<p>これを \( D_{cpt} \) と表記します。これは特定の専門分野やタスクに関連するテキストデータを含みます。</p>
<p><span class="badge orange">下流ドメイン</span></p>
</div>
</div>
<div class="arrow-connector" style="height:20px;"></div>
<p style="text-align:center; font-family: 'Yomogi', cursive;">これらのデータセットに対するモデルの損失を追跡します <i class="fas fa-chart-line"></i></p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-cogs"></i> Experiment Setup (実験設定)</p>
<p>主要な実験では、以下の設定を使用します：</p>
<ul class="unstyled-list">
<li><span class="badge yellow">モデル</span> LLaMAアーキテクチャに類似したモデルを使用。非埋め込みパラメータ数は1億600万～17億の範囲。
                    <div class="note-box" style="padding: 8px; margin-top: 5px;">
<p style="margin:0; font-size:0.9em;"><i class="fas fa-info-circle"></i> <strong>非埋め込みパラメータ:</strong> トークンの埋め込み層以外の、モデル本体の計算に関わるパラメータのこと。</p>
</div>
</li>
<li><span class="badge yellow">初期事前学習データ</span>汎用データセットとして <span class="keyword">FineWeb</span> (Penedo et al., 2024) を使用してLLMを初期事前学習します。</li>
<li><span class="badge yellow">CPTデータ</span>ドメイン特化データセットとして <span class="keyword">Knowledge-Pile</span> (Fei et al., 2024) を使用してCPTを行います。</li>
<li><span class="badge yellow">学習率スケジュール (LRS)</span> 事前学習（PT）および継続的事前学習（CPT）のモデル学習には、様々な学習率スケジュール（LRS）を活用します。これについては図1で示されています（ただし、図1の画像は提供されていません。後述の解説を参照してください）。</li>
</ul>
<p class="reference"><i class="fas fa-book"></i> 詳細な実験設定は付録Bに記載されています。</p>
</div>
<div class="bubble-box">
<p style="font-family: 'Yomogi', cursive; font-size: 1.1em;"><i class="fas fa-eye"></i> Observation (観察結果)</p>
<p>過去の研究（Ibrahim et al., 2024; Gupta et al., 2023）でも観察されているように、CPTプロセス中には以下の傾向が見られます：</p>
<div class="two-column">
<div class="column challenge-box" style="border-left-color: var(--color-primary);">
<h4 class="challenge-title" style="color: var(--color-primary);"><i class="fas fa-arrow-up"></i> \(D_{pt}\) 検証損失</h4>
<p>汎用ドメインデータセット（\(D_{pt}\)）に対する検証損失は<span class="highlight" style="background-color: rgba(74, 111, 165, 0.2);">増加する傾向</span>があります。これは、モデルが新しいドメイン（\(D_{cpt}\)）に適応する過程で、元の汎用的な知識の一部を「忘れてしまう」現象（<span class="keyword">破滅的忘却</span>）を示唆しています。</p>
</div>
<div class="column challenge-box" style="border-left-color: var(--color-accent1);">
<h4 class="challenge-title" style="color: var(--color-accent1);"><i class="fas fa-arrow-down"></i> \(D_{cpt}\) 検証損失</h4>
<p>一方、CPTドメインデータセット（\(D_{cpt}\)）に対する検証損失は<span class="highlight" style="background-color: rgba(92, 184, 92, 0.2);">減少する傾向</span>があります。これは、モデルがターゲットドメインの知識を学習し、適応していることを示しています。</p>
</div>
</div>
<p>さらに、PTフェーズとCPTフェーズの両方を通じて、損失曲線は<span class="keyword">学習率アニーリング（LR annealing）</span>によって大きく影響を受け、損失が急速に低下する様子が観察されます。</p>
<div class="note-box" style="margin-top: 15px;">
<p class="note-title"><i class="fas fa-lightbulb"></i> 学習率アニーリングとは？</p>
<p>学習率アニーリングは、学習の進行とともに学習率を徐々に小さくしていく手法です。学習初期には大きな学習率で大域的な最適解の近くに素早く移動し、学習終盤には小さな学習率で局所的な最適解に細かく収束させることを目的とします。これにより、学習の安定化と性能向上が期待できます。</p>
<div style="text-align: center; margin-top:10px;">
<svg height="100" viewbox="0 0 200 100" width="200" xmlns="http://www.w3.org/2000/svg">
<path d="M10 20 Q 50 20, 100 80 T 190 80" fill="none" stroke="var(--color-primary)" stroke-dasharray="4 2" stroke-width="2"></path>
<text fill="var(--color-dark)" font-family="Yomogi" font-size="10px" x="10" y="15">高い学習率</text>
<text fill="var(--color-dark)" font-family="Yomogi" font-size="10px" x="140" y="95">低い学習率</text>
<text fill="var(--color-secondary)" font-family="Yomogi" font-size="10px" transform="rotate(-30, 80, 55)" x="80" y="55">徐々に減少</text>
<circle cx="10" cy="20" fill="var(--color-primary)" r="3"></circle>
<circle cx="190" cy="80" fill="var(--color-primary)" r="3"></circle>
</svg>
</div>
</div>
</div>
</div>
<h3 class="section-title"><i class="fas fa-chart-area"></i> 2.2. CPT Transfer Loss Curve</h3>
<div class="content-box">
<p>CPT損失曲線のダイナミクスをより深く理解するために、研究者たちは実際のCPT損失曲線に加えて、さらに2種類の<span class="keyword">「隠れた」事前学習曲線（hidden pre-training curve）</span>を訓練し、比較分析しています。これらの曲線は、CPTプロセスがどのような学習経路を辿っているのかを明らかにするための重要な手がかりとなります。</p>
<div class="info-grid">
<div class="info-card glass-card">
<h4><i class="fas fa-brain" style="color: var(--color-primary);"></i> Hidden PT Curve Training on \(D_{pt}\)</h4>
<p>この曲線は、モデルが<span class="highlight">継続的に \(D_{pt}\)（汎用データセット）のみを使って事前学習された場合</span>の損失を示します。ここでの重要なポイントは、学習率スケジュール（LRS）などの訓練設定が、<span class="keyword">実際のCPTフェーズと同じ</span>であるという点です。</p>
<div class="pipeline" style="margin-top:10px;">
<div class="pipeline-step" style="background-color: rgba(74, 111, 165, 0.05);">
<span class="badge blue">データ</span> \(D_{pt}\) のみ
                    </div>
<div class="pipeline-step" style="background-color: rgba(74, 111, 165, 0.05);">
<span class="badge blue">学習設定</span> CPTフェーズと同一
                    </div>
</div>
<p>つまり、「もしCPTを行わずに、そのまま汎用データで学習を続けたらどうなるか？」というシナリオの損失曲線です。</p>
</div>
<div class="info-card glass-card">
<h4><i class="fas fa-microscope" style="color: var(--color-secondary);"></i> Hidden PT Curve Training on \(D_{cpt}\)</h4>
<p>この曲線は、モデルが<span class="highlight">最初から \(D_{cpt}\)（ドメイン特化データセット）のみを使って訓練された場合</span>の損失を示します。ここでも、学習率（LR）などの訓練設定は、<span class="keyword">完全なPTフェーズとCPTフェーズの両方で適用されるものと同じ</span>であるという点が重要です。</p>
<div class="pipeline" style="margin-top:10px;">
<div class="pipeline-step" style="background-color: rgba(255, 126, 95, 0.05);">
<span class="badge orange">データ</span> \(D_{cpt}\) のみ (スクラッチから)
                    </div>
<div class="pipeline-step" style="background-color: rgba(255, 126, 95, 0.05);">
<span class="badge orange">学習設定</span> PTフェーズ + CPTフェーズと同一
                    </div>
</div>
<p>つまり、「もし最初からドメイン特化データだけで学習していたら、どのような損失曲線になったか？」というシナリオの損失曲線です。これは、ドメイン特化学習の理想的な到達点の一つと考えることができます。</p>
</div>
</div>
<div class="arrow-connector"></div>
<div class="bubble-box" style="border-color: var(--color-accent1);">
<p style="font-family: 'Yomogi', cursive; font-size: 1.1em; color: var(--color-accent1);"><i class="fas fa-exchange-alt"></i> Transfer Loss Curve (遷移損失曲線)</p>
<p>図1（論文中の図）に示されるように、実際のCPT損失曲線は、\(D_{pt}\) と \(D_{cpt}\) の両方の検証セットにおいて<span class="keyword">遷移的な曲線</span>として振る舞います。この曲線は、<span class="highlight" style="background-color: rgba(74, 111, 165, 0.2);">「\(D_{pt}\) で学習し続けた場合の隠れたPT曲線」から逸脱</span>し、もう一方の<span class="highlight" style="background-color: rgba(255, 126, 95, 0.2);">「\(D_{cpt}\) で最初から学習した場合の隠れたPT曲線」に近づいていく</span>ように見えます。</p>
</div>
<img alt="図1: CPT損失曲線の振る舞い" class="section-image" src="cpt_loss_surface_transfer_slide.jpg" style="width: 80%; margin-top: 20px; margin-bottom: 20px;"/>
<p style="text-align: center; font-family: 'Yomogi', cursive; font-size: 0.9em;">図1: \(D_{pt}\) および \(D_{cpt}\) 検証損失のシフト。 (a) は FineWeb (\(D_{pt}\)) の検証損失、(b) は Knowledge Pile (\(D_{cpt}\)) の検証損失の推移を示しています。</p>
<div class="content-box">
<p><strong>図1の見方:</strong></p>
<ul class="unstyled-list">
<li><i class="fas fa-image" style="color:var(--color-primary)"></i> <strong>(a) \(D_{pt}\) (FineWeb) 検証損失のシフト:</strong>
<ul>
<li>横軸はCPTのステップ数（時間経過）、縦軸は\(D_{pt}\)データセットに対する検証損失です。</li>
<li>CPTを開始すると（グラフの"Pre-training"から"CPT in 120K Steps"へ移行する部分）、汎用ドメインの損失（\(D_{pt}\) Validation Loss）が<span class="highlight">初期には上昇</span>していることが見て取れます。これは、モデルが新しいドメイン（\(D_{cpt}\)）に特化し始めることで、元の汎用的な知識が若干失われることを示しています（<span class="keyword">破滅的忘却</span>の兆候）。</li>
<li>グラフ中の数式（例: \(Power: R^2 = ...\)）は、この損失曲線を特定の関数でフィッティングしようとした結果を示しています。</li>
<li><span class="keyword">Distribution Shift</span>というラベルは、この汎用ドメイン損失の上昇が、データ分布の変化（汎用データからドメイン特化データへの移行）によって引き起こされることを示唆しています。</li>
</ul>
</li>
<li><i class="fas fa-image" style="color:var(--color-secondary)"></i> <strong>(b) \(D_{cpt}\) (Knowledge Pile) 検証損失のシフト:</strong>
<ul>
<li>横軸は同様にCPTのステップ数、縦軸は\(D_{cpt}\)データセットに対する検証損失です。</li>
<li>CPTを開始すると、ドメイン特化データセットの損失（\(D_{cpt}\) Validation Loss）は<span class="highlight">明確に減少</span>しています。これは、モデルがターゲットドメインの知識を効果的に学習し、そのドメインへの適応が進んでいることを示しています。</li>
<li>こちらも、数式によるフィッティング結果や <span class="keyword">Distribution Shift</span> のラベルが見られます。\(D_{cpt}\)におけるDistribution Shiftは、モデルが新しい分布（\(D_{cpt}\)）に適応していく過程そのものを指していると考えられます。</li>
</ul>
</li>
</ul>
</div>
<div class="definition-box" style="margin-top: 20px;">
<p class="definition-title"><i class="fas fa-ruler-combined"></i> 分布シフト (Distribution Shift)</p>
<p>実際の（遷移）損失曲線と、「\(D_{pt}\)で学習し続けた場合の隠れたPT曲線」との間の<span class="keyword">不一致・隔たり</span>を<span class="keyword">分布シフト</span>と呼びます。これは、学習データが \(D_{pt}\) から \(D_{cpt}\) へと変わることによる影響を定量的に示したものと言えます。</p>
<div style="display: flex; align-items: center; margin-top: 10px; font-family: 'Yomogi', cursive;">
<div style="text-align: center; padding: 10px; border: 1px dashed var(--color-primary); border-radius: 8px; margin-right: 10px;">
<p style="margin:0; font-size:0.9em;">実際のCPT損失曲線<br/>(青い実線)</p>
<svg height="50" width="100"><path d="M10 40 C 30 10, 70 10, 90 40" fill="none" stroke="var(--color-primary)" stroke-width="2"></path></svg>
</div>
<p style="font-size: 20px; color: var(--color-accent2); margin: 0 10px;">≠</p>
<div style="text-align: center; padding: 10px; border: 1px dashed var(--color-gray); border-radius: 8px; margin-left: 10px;">
<p style="margin:0; font-size:0.9em;">隠れたPT曲線 (on \(D_{pt}\))<br/>(灰色の破線)</p>
<svg height="50" width="100"><path d="M10 40 C 30 10, 70 10, 90 40" fill="none" stroke="var(--color-gray)" stroke-dasharray="5,5" stroke-width="2"></path></svg>
</div>
<p style="font-size: 20px; color: var(--color-accent2); margin: 0 10px;">→</p>
<div style="text-align: center; padding: 10px; border: 1px solid var(--color-accent2); border-radius: 8px; background-color: rgba(149, 117, 205, 0.1);">
<p style="margin:0; font-size:0.9em; color:var(--color-accent2);">分布シフト</p>
</div>
</div>
</div>
<p style="margin-top: 15px;">CPTのステップ数が無限に近づくにつれて（つまり、CPTを非常に長時間続けると）、この遷移損失曲線は<span class="highlight" style="background-color: rgba(255, 126, 95, 0.2);">「\(D_{cpt}\)で最初から学習した場合の隠れたPT曲線」に収束する</span>と予想されます。</p>
<div class="note-box" style="background-color: rgba(92, 184, 92, 0.1); border-left-color: var(--color-accent1); margin-top:25px;">
<p class="note-title" style="color: var(--color-accent1);"><i class="fas fa-lightbulb"></i> Finding 1 (発見1)</p>
<p><strong>CPTのプロセスは、損失曲線が「\(D_{pt}\)で学習し続けた場合の隠れたPT曲線」から「\(D_{cpt}\)で最初から学習した場合の隠れたPT曲線」へと遷移する過程である。</strong></p>
<div style="text-align:center; margin-top:15px; font-family: 'Yomogi', cursive;">
<span style="padding: 5px 10px; border-radius: 5px; background-color: rgba(74,111,165,0.2); color: var(--color-primary);">隠れPT曲線 (\(D_{pt}\)ベース)</span>
<span style="font-size: 24px; color: var(--color-accent1); margin: 0 10px;">→</span>
<span style="padding: 5px 10px; border-radius: 5px; background-color: rgba(255,126,95,0.2); color: var(--color-secondary);">隠れPT曲線 (\(D_{cpt}\)ベース)</span>
</div>
<p style="text-align:center; margin-top:5px; font-size:0.9em;">この遷移が <span class="keyword">CPTの本質</span> と言えます。</p>
</div>
</div>
</div>
<div class="section-card" id="3._Continual_Learning_Dynamics_Law">
<h2 class="section-title"><i class="fas fa-wave-square"></i> 3. Continual Learning Dynamics Law</h2>
<div class="content-box">
<p>このセクションでは、大規模言語モデルの継続的事前学習（Continual Pre-Training, CPT）における<span class="keyword">転移曲線（transfer curve）</span>を定量的に分析します。特に、<span class="keyword">分布シフト（distribution shift）</span>と<span class="keyword">学習率アニーリング（learning rate annealing）</span>という2つの重要な要素を考慮に入れます。</p>
<p>主なアイデアは以下の通りです：</p>
<ul class="unstyled-list">
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> データ転移がなければ、CPTの損失曲線は、元の事前学習データセット（$D_{pt}$）で学習を続けた場合の<span class="keyword">隠れた事前学習曲線（hidden Pre-Training curve）</span>の軌跡を辿るはずです。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 実際のCPTでは、新しいドメインのデータセット（$D_{cpt}$）へ移行するため、この隠れた曲線から逸脱が生じます。この逸脱が<span class="keyword">分布シフト</span>として記述されます。</li>
</ul>
<p>つまり、隠れたPT曲線がPTモデルの状態を反映し、分布シフト項がデータ分布間の相対的な関係性（転移の度合い）を記述すると考えられます。これらの要素を組み合わせることで、CPTプロセス中の損失変動を予測する法則（スケーリング則）を導出することを目指します。</p>
</div>
<h3 class="section-title"><i class="fas fa-chart-line"></i> 3.1. Hidden Pre-Training Curve Training on $D _ { p t }$</h3>
<div class="content-box">
<p>まず、データ転移が発生しない理想的な状況を考えます。このとき、CPTプロセスは実質的に元の事前学習データセット $D_{pt}$ での学習継続と同じです。このような状況での損失の振る舞いを記述するために、Tissue et al. (2024) が導入した<span class="keyword">学習率アニーリングを考慮したスケーリング則</span>を用います。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-book-open"></i> スケーリング則（学習率アニーリング考慮）</div>
<p>学習ステップ $t$ における損失 $L(t)$ は、以下の式で表されます：</p>
<div class="formula">
                $$ L(t) = L_0 + A \cdot S_1^{-\alpha} - C \cdot S_2 $$
                <span class="reference">(式1)</span>
</div>
<p>ここで、各項は以下を意味します：</p>
<ul class="unstyled-list">
<li><span class="keyword">$L(t)$</span>: ステップ $t$ での損失値。</li>
<li><span class="keyword">$L_0$</span>: 学習が進んだ後に到達可能な最小の損失値（ベースライン）。正の定数。</li>
<li><span class="keyword">$A$</span>: 損失の減少スケールを調整する正の定数パラメータ。</li>
<li><span class="keyword">$S_1$</span>: <strong style="color:var(--color-accent2);">順方向領域 (forward area)</strong>。学習率 $\eta_i$ の累積和です。
                    <div class="formula" style="font-size:0.9em;">$$ S_1 = \sum_{i=1}^{t} \eta_i $$</div>
<div class="bubble-box" style="border-color: var(--color-accent2); margin-top:5px;">
<p style="margin:0;">📝 <span class="highlight" style="background-color: rgba(149, 117, 205, 0.2);">これは、学習がどれだけ進んだかを示す指標です。学習率が高いほど、またステップ数が多いほど大きな値を持ちます。</span></p>
</div>
</li>
<li><span class="keyword">$\alpha$</span>: 順方向領域 $S_1$ が損失に与える影響の度合い（べき乗）を調整する正の定数パラメータ。</li>
<li><span class="keyword">$C$</span>: 学習率アニーリングによる損失減少効果の大きさを調整する正の定数パラメータ。</li>
<li><span class="keyword">$S_2$</span>: <strong style="color:var(--color-accent1);">アニーリング領域 (annealing area)</strong>。学習率のアニーリング（徐々に減少させること）による影響を捉える項です。
                    <div class="formula" style="font-size:0.9em;">$$ S_2 = \sum_{i=1}^{t} \sum_{k=1}^{i} (\eta_{k-1} - \eta_k) \cdot \lambda^{i-k} $$</div>
<div class="bubble-box" style="border-color: var(--color-accent1); margin-top:5px;">
<p style="margin:0;">📝 <span class="highlight" style="background-color: rgba(92, 184, 92, 0.2);">学習率が減少する $(\eta_{k-1} - \eta_k &gt; 0)$ と、この項が大きくなり、損失 $L(t)$ を引き下げる効果があります。$\lambda$ は過去の学習率変化が現在の損失にどれだけ影響を与えるか（モメンタムのような効果）を調整するハイパーパラメータ（通常 $0 &lt; \lambda \le 1$）です。</span></p>
</div>
</li>
<li><span class="keyword">$\lambda$</span>: モメンタム項を記述するハイパーパラメータ。</li>
</ul>
</div>
<p>CPTプロセスにおいて、分布シフトがない場合のベースとなる損失 $L_{base}(t)$ は、このスケーリング則に従うと考えられます。PT（事前学習）段階とCPT（継続的事前学習）段階の両方を考慮して、以下のように記述されます。</p>
<div class="formula">
            $$ L_{base}(t) = L_0 + A \cdot (S_1^{pt} + S_1^{cpt})^{-\alpha} - C \cdot (S_2^{pt} + S_2^{cpt}) $$
            <span class="reference">(式2)</span>
</div>
<p>ここで、</p>
<ul class="unstyled-list">
<li><span class="keyword">$S_1^{pt}$</span> と <span class="keyword">$S_2^{pt}$</span>: PT段階での順方向領域とアニーリング領域。</li>
<li><span class="keyword">$S_1^{cpt}$</span> と <span class="keyword">$S_2^{cpt}$</span>: CPT段階での順方向領域とアニーリング領域。</li>
<li><span class="keyword">$t$</span>: CPTの学習ステップ数。</li>
</ul>
<div class="note-box">
<div class="note-title"><i class="fas fa-lightbulb"></i> ポイント</div>
<p>$L_{base}(t)$ は、CPTがPTからシームレスに同じデータ分布 ($D_{pt}$) で学習を継続した場合の理論的な損失曲線を表します。これが、実際のCPT損失曲線を評価する際の<span class="keyword">基準線</span>となります。</p>
</div>
</div>
<h3 class="section-title"><i class="fas fa-exchange-alt"></i> 3.2. Distribution Shift Term</h3>
<div class="content-box">
<p>次に、CPTプロセスで実際に観測される損失曲線が、$D_{pt}$ 上での隠れたPT曲線からどれだけ逸脱するかを記述する<span class="keyword">分布シフト項 ($\Delta L(t)$)</span> について考えます。このシフトは、$D_{pt}$ と $D_{cpt}$ という2つのデータセット間の<span class="keyword">分布の隔たり (distribution distance)</span> を反映します。また、多くの研究で指摘されているように、CPT段階での学習率スケジュール（LRS）もこのシフトに大きな影響を与えます。</p>
<div class="pipeline">
<div class="pipeline-step">
<p><span class="badge orange">ステップ1</span> <strong>定数学習率（Constant LRS）での分析</strong></p>
<p>まず、LRSの影響を切り分けるために、PTとCPTの両フェーズで同じ一定の学習率を使用した場合を分析します。この条件下で、異なる転移開始点（つまり、PTモデルの異なるチェックポイントからCPTを開始する）で実験を行いました。</p>
<p>論文のFigure 2（本解説では図は省略）で示されているように、これらの<span class="keyword">分布シフト項は、転移開始点によらずほぼ重なる</span>傾向が見られました。これは非常に重要な知見で、分布シフト項が転移開始点やPTモデルの特定の状態（チェックポイント）には依存しないことを示唆しています。</p>
<div class="glass-card">
<p><i class="fas fa-search-location"></i> <strong>発見</strong>: 分布シフト項は、PTモデルの状態から独立しているようです。</p>
</div>
<p>分布シフト項 $\Delta L(t)$ の関数形として、指数関数とべき乗則の形を比較検討した結果、以下のべき乗則の修正形が最もよくフィットすることがわかりました：</p>
<div class="formula">
                    $$ \Delta L(t) = B \cdot (1 - (E \cdot t + 1)^{-\beta}) $$
                    <span class="reference">(式3の原型、論文では図2のキャプションで触れられている)</span>
</div>
<p>ここで、</p>
<ul class="unstyled-list">
<li><span class="keyword">$B$</span>: シフトの最大量（飽和値）を決定する正の定数。</li>
<li><span class="keyword">$E$</span>: シフトが飽和する速さを調整する正の定数。</li>
<li><span class="keyword">$t$</span>: CPTの学習ステップ数。</li>
<li><span class="keyword">$\beta$</span>: シフト曲線の形状を決定する正の定数（べき指数）。</li>
</ul>
<div class="note-box">
<div class="note-title"><i class="fas fa-pencil-alt"></i> なぜ単純なべき乗則 $B \cdot t^{-\beta}$ ではないのか？</div>
<p>単純なべき乗則 $B \cdot t^{-\beta}$ だと、$t=0$（CPT開始直後）で $\Delta L(0)$ が発散するか、定義できません。しかし、CPT開始時点ではまだ分布シフトは起きていないはずなので、$\Delta L(0) = 0$ となる必要があります。上記の式形 $(E \cdot t + 1)^{-\beta}$ は、この条件を満たします。</p>
</div>
<p>この式を用いて、$D_{pt}$ と $D_{cpt}$ の両方の検証セットにおける転移損失曲線をフィッティングした結果が、論文のFigure 2に示されています。</p>
</div>
<div class="pipeline-step">
<p><span class="badge purple">ステップ2</span> <strong>他の学習率スケジュール（Other LRS）への拡張</strong></p>
<p>一定学習率以外の場合、学習率そのものではなく、<span class="keyword">順方向領域 $S_1$</span> が分布シフト項に影響を与えると考えられます。CPTにおける順方向領域 $S_1^{cpt}$ が小さい場合（例えば、学習率が低い、または学習ステップが短い場合）、分布シフトの度合いも小さくなります。これは論文のFigure 1でオレンジ色の転移曲線が異なる様子として示唆されています。</p>
<p>そこで、Tissue et al. (2024) のアプローチに従い、分布シフト項の式中の学習ステップ $t$ を、CPTにおける順方向領域 $S_1^{cpt}$ で置き換えます。</p>
<div class="formula">
                    $$ \Delta L(t) = B \cdot (1 - (1 + E \cdot S_1^{cpt})^{-\beta}) $$
                    <span class="reference">(式3)</span>
</div>
<p>ここで、$S_1^{cpt}$ はCPT段階での順方向領域（学習率の累積和）です。これにより、様々なLRSに対応できる一般化された分布シフト項の表現が得られます。</p>
<div class="bubble-box" style="border-color: var(--color-primary);">
<p style="margin:0;">💡 <span class="highlight" style="background-color: rgba(74, 111, 165, 0.2);">この置き換えにより、学習ステップ数だけでなく、学習率の大きさやスケジュールの影響も分布シフト項に組み込むことができます。</span></p>
</div>
</div>
</div>
</div>
<h3 class="section-title"><i class="fas fa-puzzle-piece"></i> 3.3. Final Transfer Curve</h3>
<div class="content-box">
<p>いよいよ、$D_{pt}$ 上での隠れたPT曲線（式2の $L_{base}(t)$）と分布シフト項（式3の $\Delta L(t)$）を組み合わせることで、CPT転移損失曲全体を記述する完全な定式化を行います。</p>
<div class="framework-box">
<div class="framework-title"><i class="fas fa-sitemap"></i> CPT転移損失曲線の最終形</div>
<p>学習ステップ $t$ におけるCPTの損失 $L(t)$ は、以下のように表されます。</p>
<div class="formula">
            $$
            \begin{array} { r l } &amp; { L ( t ) = L _ { b a s e } ( t ) + \Delta L ( t ) } \\ &amp; { = \underbrace { L _ { 0 } + A \cdot \left( S _ { 1 } ^ { p t } + S _ { 1 } ^ { c p t } \right) ^ { - \alpha } - C _ { 1 } \cdot S _ { 2 } ^ { p t } - C _ { 2 } \cdot S _ { 2 } ^ { c p t } } _ { \text{学習率アニーリングを伴うスケーリング則} } } \\ &amp; { + \underbrace { B \cdot \left( 1 - \left( 1 + E \cdot S _ { 1 } ^ { c p t } \right) ^ { - \beta } \right) } _ { \text{べき乗則に従う分布シフト} } } \end{array}
            $$
            <span class="reference">(式4)</span>
</div>
<div class="note-box" style="background-color: rgba(255, 126, 95, 0.1); border-left-color: var(--color-secondary);">
<div class="note-title" style="color: var(--color-secondary);"><i class="fas fa-exclamation-triangle"></i> 注意点: $C_1$ と $C_2$</div>
<p>アニーリング領域 $S_2^{pt}$ と $S_2^{cpt}$ にかかる係数が、$C_1$ と $C_2$ のように異なっています。これは、PT段階とCPT段階で学習データセットが異なるため、学習率アニーリングによる損失減少効果の大きさが変わることを考慮しています。</p>
</div>
</div>
<p>この損失曲線関数（式4）は、PT段階とCPT段階の両方で、任意の学習率スケジュール（LRS）における任意のステップでの損失を記述できると主張されています。</p>
<p>実験による検証：</p>
<p>論文では、広く採用されているWSD（Warmup-Stable-Decay）LRSとCosine LRSを用いてモデルを事前学習および継続的事前学習し、式4を用いて $D_{pt}$ および $D_{cpt}$ 検証セットの全損失曲線をフィッティングしています。結果は論文のFigure 3に示されています。</p>
</div>
<img alt="Figure 3 from the paper showing WSD and Cosine LRS experiments." src="cpt_loss_surface_transfer_slide.jpg"/>
<div class="content-box">
<p style="text-align: center; font-style: italic;">図3のイメージ (WSD PT and CPT LRS (a-c), Cosine PT and CPT LRS (d-f))</p>
<p>Figure 3 (a) と (d) はそれぞれWSDとCosineの学習率スケジュールを示しています。(b) と (e) は $D_{pt}$ (FineWeb) の検証損失、(c) と (f) は $D_{cpt}$ (Knowledge Pile) の検証損失を示しています。実線が実際の損失（Truth Loss）、破線が式4でフィッティングした損失（Fitted Loss）です。</p>
<div class="glass-card">
<p><i class="fas fa-chart-area"></i> <strong>Figure 3 の解釈</strong>:</p>
<p>図の中央（(b), (e)）と右側（(c), (f)）のパネルを見ると、提案された式（式4）が、異なるLRS（WSDとCosine）において、学習プロセス全体を通じた損失変動のトレンドをうまく捉えていることがわかります。</p>
<ul class="unstyled-list">
<li>例えば、図3(b)では、$D_{pt}$ の損失がCPT開始後に上昇し、その後LRSに応じて変動する様子がフィッティングされています。</li>
<li>図3(c)では、$D_{cpt}$ の損失がCPT開始後に急激に減少し、その後LRSに応じて安定していく様子がフィッティングされています。特に「Peak Point」や「Distribution Shift + LR Re-Warmup」といったラベルは、損失曲線の特徴的な部分を示しています。</li>
</ul>
<p>さらに、このフィッティングされた式を用いて、他のLRSでの損失曲線を予測することも可能であると述べられており、その結果は論文のFigure 10（付録）に示されています。また、CPT段階でバッチサイズやシーケンス長が変更される場合でも、このスケーリング則は適応可能であると主張されており、詳細は付録Fで示されています。</p>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-key"></i> Finding 2 (重要な発見)</div>
<p>CPT損失曲線は、以下の2つの要素に分解できます：</p>
<ol>
<li><strong style="color:var(--color-primary);">$D_{pt}$ での隠れたPT曲線</strong>: これは学習率アニーリングを伴う単純なスケーリング則として定式化されます。</li>
<li><strong style="color:var(--color-secondary);">分布シフト項</strong>: これは転移開始点に依存せず、べき乗則に従う遷移を示します。</li>
</ol>
</div>
<p><strong>転移損失表面（Transfer Loss Surface）</strong></p>
<p>この定式化をより深く理解するために、LLMの損失表面を<span class="keyword">「転移スライド」</span>として視覚化しています（論文のFigure 4）。</p>
</div>
<img alt="Figure 4. The loss surface of CPT process and two direction views." src="cpt_loss_surface_transfer_slide.jpg"/>
<div class="content-box">
<p style="text-align: center; font-style: italic;">Figure 4. CPTプロセスの損失表面と2つの方向からの眺め。</p>
<div class="glass-card">
<p><i class="fas -eye"></i> <strong>Figure 4 の解釈</strong>:</p>
<p>(a) はCPTプロセスの損失表面を3次元的に示しています。「Forward Area Direction」と「Annealing Area Direction」が損失表面の形状を決定する2つの主要な軸です。CPTプロセスは、一つの「スライド」（$D_{pt}$ に対応する損失表面）から別の「スライド」（$D_{cpt}$ に対応する損失表面）へ、べき乗則の形で移行すると表現されています。</p>
<ul class="unstyled-list">
<li><i class="fas fa-mountain" style="color: var(--color-accent1);"></i> <strong>データセット間の距離と勾配</strong>: 2つのデータセット間の分布の距離が大きい場合、転移表面の「勾配」は急になります。これにより、$D_{pt}$ の損失は急速に上昇します（忘却が起こりやすくなる）。</li>
<li><i class="fas fa-sliders-h" style="color: var(--color-accent2);"></i> <strong>学習率アニーリングと振動</strong>: 学習率がアニーリングされると、損失表面上の損失の「振動」の振幅が小さくなり、結果として損失が減少します。</li>
</ul>
<p>(b) は「Forward view」（順方向領域の方向から見た図）で、損失が初期に減少し、CPT開始点からジャンプして再び変化する様子を示しています。</p>
<p>(c) は「Annealing view」（アニーリング領域の方向から見た図）で、U字型の曲線上で損失が振動しながら減少していく様子を示しています。この図で「Loss Potential」（損失ポテンシャル）という概念が導入されています。</p>
</div>
<p>アニーリングの観点から、ある点の「高さ」を<span class="keyword">損失ポテンシャル (loss potential)</span> と名付けています。これは、学習率アニーリングを通じて将来的に損失がどれだけ低下するかの可能性を捉えるものです。定量的に、損失ポテンシャルは、<span class="highlight">PTアニーリングフェーズの最終学習率</span>と、<span class="highlight">事前学習フェーズの初期または最大学習率</span>の比として定義されます。</p>
<div class="bubble-box" style="border-color: var(--color-primary);">
<p style="margin:0;">🎯 <strong>具体例: 損失ポテンシャル</strong></p>
<p style="margin-bottom: 5px;">例えば、事前学習の初期学習率が 0.001 で、PTアニーリングフェーズの最終学習率が 0.0001 だった場合、損失ポテンシャルは $0.0001 / 0.001 = 0.1$ (または 10%) となります。</p>
<p style="margin:0;">もし最終学習率が 0.0005 なら、損失ポテンシャルは $0.0005 / 0.001 = 0.5$ (または 50%) となります。損失ポテンシャルが高いということは、アニーリングによる「伸びしろ」が大きいことを意味します。</p>
</div>
<div class="note-box">
<div class="note-title"><i class="fas fa-key"></i> Finding 3 (重要な発見)</div>
<p><span class="keyword">より高い損失ポテンシャルを持つPTモデル</span>は、常に<span class="keyword">より低い $D_{cpt}$ 検証損失</span>を達成します。したがって、オープンソースモデルをリリースする際には、下流ドメインへの適応性を高めるために、高い損失ポテンシャルを持つバージョンをリリースすることが有益であると提唱しています。</p>
</div>
</div>
<h3 class="section-title"><i class="fas fa-expand-arrows-alt"></i> 3.4. Extension to Model Size and Replay Ratio</h3>
<div class="content-box">
<p>このCPTスケーリング則は、さらにモデルサイズとリプレイ比率という2つの重要な要素にも拡張可能です。</p>
<div class="two-column">
<div class="column">
<div class="info-card">
<p class="subsection-title" style="font-size: 16px; color: var(--color-primary); border-left-color: var(--color-primary);"><i class="fas fa-robot"></i> モデルサイズ ($N$) への拡張</p>
<p>Tissue et al. (2024) の研究に倣い、モデルサイズ $N$（通常は非埋め込みパラメータ数）をCPTスケーリング則に組み込みます。</p>
<p>異なるモデルサイズにおける分布シフト項を分析した結果、<span class="keyword">絶対的なシフト値はモデルサイズによらずほぼ同一</span>であることが明らかになりました。この発見は重要で、分布シフトの量はモデルの大きさにあまり影響されないことを示唆しています。</p>
<p>この統合により、提案されたスケーリング則は、すべてのモデルサイズの転移損失曲線を同時にフィッティングできるようになります。詳細は論文の付録Eで示されています。</p>
<div class="bubble-box" style="border-color:var(--color-accent1); margin-top:10px;">
<p style="margin:0;">📊 <span class="highlight" style="background-color: rgba(92, 184, 92, 0.2);">つまり、モデルが大きくても小さくても、新しいドメインへの適応に伴う「損失の変化量（シフト）」自体はあまり変わらない、ということです。</span></p>
</div>
</div>
</div>
<div class="column">
<div class="info-card">
<p class="subsection-title" style="font-size: 16px; color: var(--color-secondary); border-left-color: var(--color-secondary);"><i class="fas fa-history"></i> リプレイ比率への拡張</p>
<p>CPTでは、新しいドメインのデータ ($D_{cpt}$) を学習する際に、元のドメインのデータ ($D_{pt}$) の一部を混ぜて学習する<span class="keyword">リプレイ</span>がよく用いられます。このリプレイの比率もスケーリング則に統合されます。</p>
<p>分析の結果、<span class="keyword">リプレイ比率は分布シフト項に指数関数的な影響を与える</span>ことが観察されました。つまり、リプレイ比率を変えると、分布シフトの大きさが指数関数的に変化するということです。詳細は論文の付録Hで説明されています。</p>
<div class="bubble-box" style="border-color:var(--color-accent2); margin-top:10px;">
<p style="margin:0;">📈 <span class="highlight" style="background-color: rgba(149, 117, 205, 0.2);">$D_{pt}$ のデータを多くリプレイするほど、分布シフトは小さくなる傾向があると考えられます。</span></p>
</div>
<p>Que et al. (2024) のD-CPT則と比較して、この論文のスケーリング則は、最終的な損失だけでなく、<span class="keyword">異なるリプレイ比率における学習プロセス全体の損失曲線を予測できる</span>点が優れています。</p>
</div>
</div>
</div>
</div>
</div>
<div class="section-card" id="4._Factor_Analyses_and_Applications">
<h2 class="section-title"><i class="fas fa-magnifying-glass-chart"></i> 4. Factor Analyses and Applications</h2>
<p class="content-box">
        このセクションでは、継続的事前学習（CPT）における重要な要素を掘り下げて分析します。特に、大規模言語モデルの性能に影響を与える主要な要因として、<span class="keyword">事前学習（PT）モデルの損失ポテンシャル</span>、<span class="keyword">PTデータセットとCPTデータセット間の分布の距離</span>、<span class="keyword">ピーク学習率（LR）</span>、そして<span class="keyword">CPTのステップ数</span>に焦点を当てます。
        <br/><br/>
        これらの要因がCPTの学習ダイナミクスにどのように関わってくるのかを理解するために、以前のセクションで導出した以下の<strong class="highlight">CPTスケーリング則（式4）</strong>を活用して、予測および分析を行います。
    </p>
<div class="formula">
<p>📝 <strong>CPTスケーリング則 (式4)</strong></p>
        $$ L(t) = L_{base}(t) + \Delta L(t) $$
        $$ = \underbrace{L_0 + A \cdot (S_1^{pt} + S_1^{cpt})^{-\alpha} - C_1 \cdot S_2^{pt} - C_2 \cdot S_2^{cpt}}_{\text{学習率アニーリングを伴うスケーリング則}} + \underbrace{B \cdot (1 - (1 + E \cdot S_1^{cpt})^{-\beta})}_{\text{べき乗則に従う分布シフト}} $$
        <div class="note-box">
<p class="note-title"><i class="fas fa-calculator"></i> 数式の内訳:</p>
<ul>
<li><span class="keyword_math">\( L(t) \)</span>: CPTのステップ \( t \) における損失。</li>
<li><span class="keyword_math">\( L_{base}(t) \)</span>: 分布シフトがない場合の基本となる損失。学習率アニーリングの影響をモデル化。
                    <ul>
<li><span class="keyword_math">\( L_0, A, C_1, C_2, \alpha \)</span>: 定数パラメータ。</li>
<li><span class="keyword_math">\( S_1^{pt}, S_1^{cpt} \)</span>: PTおよびCPT段階での前方領域（累積学習率）。<span class="highlight">学習の進行度</span>のようなもの。</li>
<li><span class="keyword_math">\( S_2^{pt}, S_2^{cpt} \)</span>: PTおよびCPT段階でのアニーリング領域。<span class="highlight">学習率の減少具合</span>による効果。</li>
</ul>
</li>
<li><span class="keyword_math">\( \Delta L(t) \)</span>: 分布シフトによる損失の変化。
                    <ul>
<li><span class="keyword_math">\( B, E, \beta \)</span>: 定数パラメータ。</li>
<li>この項は、PTデータセットからCPTデータセットへの<span class="highlight">データの性質の変化</span>による影響を表します。</li>
</ul>
</li>
</ul>
<p>この数式は、CPTプロセス中の損失曲線を、<span class="highlight">学習率スケジュールの影響</span>と<span class="highlight">データ分布の変化の影響</span>という2つの主要な力に分解して理解することを可能にします。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-lightbulb"></i> 4.1. Loss Potential (損失ポテンシャル)</h3>
<p class="content-box">
        多くの事前学習済みモデルは、学習の終盤で学習率を徐々に小さくする（アニーリングする）学習率スケジュール（LRS）を採用し、できるだけ低い損失値を達成しようとします。しかし、<span class="highlight">下流タスク（特定の専門分野のデータ）に最もよく適応するモデルが、必ずしも学習率を完全に下げ切った（アニーリングし尽くした）モデルであるとは限りません</span>。
    </p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-brain"></i> 損失ポテンシャルとは？</p>
<p>
            そこで登場するのが<span class="keyword">損失ポテンシャル (Loss Potential)</span>という概念です。これは、学習率アニーリングを通じて、モデルが<span class="highlight">将来的にどれだけ損失を低下させられるかの「可能性」や「余地」</span>を指します。
            下の図4(c)の「Annealing view」を見てみましょう。グラフの縦軸が損失を示し、U字型のカーブの底が最も低い損失です。学習中のモデルがこのカーブの高い位置にあればあるほど、学習率アニーリングによって底に向かって損失を大きく下げられる余地がある、つまり損失ポテンシャルが高いと言えます。
        </p>
</div>
<img alt="Figure 4: CPTプロセスの損失表面と2つの視点からの図" src="cpt_loss_surface_transfer_slide.jpg"/>
<p class="reference">図4: CPTプロセスの損失表面。(a) $D_{pt}$の損失表面を転移スライドとして図示。(b) 前方視点。(c) アニーリング視点、損失ポテンシャルを示唆。</p>
<p class="content-box">
        この損失ポテンシャルの影響を調べるために、2つの異なる戦略で実験を行いました。
    </p>
<div class="two-column">
<div class="column glass-card">
<h4><i class="fas fa-arrow-down"></i> 1. 再ウォームアップなし (W/o Re-warmup)</h4>
<p>
                この戦略では、CPTのピーク学習率をPTの最終学習率に設定します。つまり、<span class="highlight">PTで学習率が下がった状態から、そのままの低い学習率でCPTを開始</span>します。
                様々な損失ポテンシャルを持つモデル（PTの学習率アニーリングの進捗度が異なるモデル）を用意し、同じデータ量で学習率をゼロまでアニーリングさせながらCPTを行いました。
            </p>
<p>
                図5(b)は、CPTステップごとのCPTデータセット ($D_{cpt}$) の検証損失を示しています。
            </p>
<img alt="Figure 5b: 異なる損失ポテンシャルを持つモデルのD_cpt真の損失 vs CPTステップ (再ウォームアップなし)" src="loss_potential_learning_rate_schedule.jpg"/>
<p class="reference">図5(b): 異なる損失ポテンシャルを持つモデルの$D_{cpt}$真の損失 vs CPTステップ (再ウォームアップなし)。</p>
<p>
                この図から、<span class="keyword">損失ポテンシャルが高いモデル（PTでのアニーリングが進んでいないモデル）ほど、CPT後の最終的な損失が低くなる</span>傾向が見て取れます。
                さらに、私たちの提案する式4を用いて様々なCPTステップにおける最終損失を予測したところ（図5c）、この傾向がCPTステップが進んでも持続することが確認されました。
            </p>
<img alt="Figure 5c: 異なるCPTステップにおけるD_cpt予測損失 vs 損失ポテンシャル (再ウォームアップなし)" src="loss_potential_learning_rate_schedule_detail.jpg"/>
<p class="reference">図5(c): 異なるCPTステップにおける$D_{cpt}$予測損失 vs 損失ポテンシャル (再ウォームアップなし)。図中の赤星 <span style="color:red;">★</span> は、各CPTステップ数において最も低い$D_{cpt}$検証損失を達成できるモデルを示します。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-cogs"></i> なぜ損失ポテンシャルが高いと良いのか？ (再ウォームアップなし)</p>
<p>
                    式4の観点から見ると、損失ポテンシャルが異なるモデル間での主な違いは、PTのアニーリングパラメータ <span class="keyword_math">\( C_1 \)</span> とCPTのアニーリングパラメータ <span class="keyword_math">\( C_2 \)</span>、そして前方領域 <span class="keyword_math">\( S_1^{pt} \)</span> と <span class="keyword_math">\( S_1^{cpt} \)</span> の差にあります。
                </p>
<ul>
<li><span class="badge blue">アニーリング係数</span>: $D_{cpt}$検証セットでは、通常 <span class="keyword_math">\( C_2 &gt; C_1 \)</span> となります。これは、CPTデータに特化することで、より効果的に損失を下げられるためです。CPT段階により大きなアニーリング領域を割り当てる（つまり、<span class="keyword_math">\( S_2^{cpt} \)</span> が大きくなるようにする）ことで、$D_{cpt}$損失をより低くできます。損失ポテンシャルが高いモデルは、PTの最終学習率が高いため、CPTでより大きく学習率をアニーリングできる余地があり、結果的に <span class="keyword_math">\( S_2^{cpt} \)</span> が大きくなりやすいです。
                    </li>
<li><span class="badge purple">前方領域</span>: 損失ポテンシャルが高いモデルは、PT終了時の学習率が高いため、PTの前方領域 <span class="keyword_math">\( S_1^{pt} \)</span> と、そこからCPTを開始する際のCPTの前方領域 <span class="keyword_math">\( S_1^{cpt} \)</span> も大きくなる傾向があります。式4の <span class="keyword_math">\( (S_1^{pt} + S_1^{cpt})^{-\alpha} \)</span> の項を見ると、前方領域が大きいほど損失が低くなることが分かります。
                    </li>
</ul>
<p>
                    これらの理由から、<span class="highlight">損失ポテンシャルが高いモデルは、再ウォームアップなしの設定では常に$D_{cpt}$検証損失が低くなる傾向</span>があり、これは以前の研究結果とも一致します。
                </p>
</div>
</div>
<div class="column glass-card">
<h4><i class="fas fa-arrow-up"></i> 2. 再ウォームアップあり (With Re-warmup)</h4>
<p>
                CPTでより一般的に行われるのは、学習率を一度PTのピーク学習率の例えば10%程度まで再ウォームアップし、その後コサイン関数などでアニーリングさせる方法です。
            </p>
<p>
                PTで完全にアニーリングされていないモデルにとって、<span class="highlight">再ウォームアップは実質的に一度学習率を上げる操作（単一ステップのアニーリングの逆）であり、その後のアニーリング領域が徐々に増加する</span>ことになります。
                図5(e)と図5(f)は、再ウォームアップありの場合の$D_{cpt}$検証損失を示しています。
            </p>
<img alt="Figure 5e, 5f: 異なる損失ポテンシャルを持つモデルのD_cpt損失 vs CPTステップ/損失ポテンシャル (再ウォームアップあり)" src="dcpt_validation_loss_vs_loss_potential.jpg"/>
<p class="reference">図5(e): 異なる損失ポテンシャルを持つモデルの$D_{cpt}$真の損失 vs CPTステップ (再ウォームアップあり)。<br/>図5(f): 異なるCPTステップにおける$D_{cpt}$予測損失 vs 損失ポテンシャル (再ウォームアップあり)。図中の赤星 <span style="color:red;">★</span> は、各CPTステップ数において最も低い$D_{cpt}$検証損失を達成できるモデルを示します。</p>
<p>
                これらの図から、再ウォームアップを行った場合でも、<span class="keyword">損失ポテンシャルが高いモデルが一貫して低い最終損失を達成する</span>ことが分かります。これは、より大きなアニーリング係数と拡大された前方領域の組み合わせによるものと考えられます。
            </p>
<div class="bubble-box">
<p>📌 <strong>まとめ: 損失ポテンシャルの重要性</strong></p>
<p>
                    PTモデルの損失ポテンシャルは、CPTの性能に大きな影響を与えます。
                    <span class="highlight">PTの段階で学習率を完全に下げ切らず、ある程度の「余力」を残しておくこと</span>が、下流タスクへの適応において有利に働くことが示唆されました。
                    これは、Finding 3「損失ポテンシャルが高いPTモデルは常に低い $D_{cpt}$ 検証損失を達成する」という結論に繋がります。そのため、オープンソースモデルを公開する際は、損失ポテンシャルの高いバージョンも提供することが下流ドメインへの適応を容易にする上で有益であると提案されています。
                </p>
</div>
</div>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-images"></i> 図5全体の解説</p>
<p>図5は、PTモデルの損失ポテンシャルの影響を示しています。中央の図（bとe）は、異なる損失ポテンシャルを持つモデルの実際の損失を示しています。右側の図（cとf）は、式4を使用して、これらのモデルの損失を異なる訓練ステップにわたって予測したものです。赤い星 <span style="color:red;">★</span> は、特定のCPTステップ数において、$D_{cpt}$ 検証損失が最も低くなるモデルを示しています。(a)と(d)の図は、それぞれ再ウォームアップなし/ありの場合のCPTにおける学習率スケジュールを図示したものです（論文中では(a)と(d)の画像が不足していますが、内容から推測して補足します。提供された画像は(b), (c), (e), (f)に対応するものです）。</p>
<ul class="unstyled-list">
<li><img alt="Figure 5a equivalent: CPT with different loss potentials (w/o re-warmup setting)" src="loss_potential_learning_rate_schedule.jpg"/>
<span class="reference">(a) 異なる損失ポテンシャルでのCPT（再ウォームアップなし設定）の学習率スケジュール例</span></li>
<li><img alt="Figure 5d equivalent: CPT with different loss potentials (w/ re-warmup)" src="loss_potential_learning_rate_schedule_detail.jpg"/>
<span class="reference">(d) 異なる損失ポテンシャルでのCPT（再ウォームアップあり）の学習率スケジュール例</span></li>
</ul>
</div>
<h3 class="subsection-title"><i class="fas fa-exchange-alt"></i> 4.2. Distribution Distance between PT and CPT dataset (PTデータセットとCPTデータセット間の分布距離)</h3>
<p class="content-box">
        PTデータセット ($D_{pt}$) とCPTデータセット ($D_{cpt}$) の<span class="keyword">分布間の距離</span>は、CPTプロセス中に観測される分布シフトの大きさに大きな影響を与えます。
    </p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-map-signs"></i> 分布シフトとは？</p>
<p>
            モデルが元々学習していたデータ（PTデータセット）の特性と、新たに適応しようとしているデータ（CPTデータセット）の特性が異なる場合、モデルの内部表現や性能が変化します。この変化を<span class="keyword">分布シフト</span>と呼びます。
            例えば、$D_{cpt}$が非常に専門的なドメイン（法律や医療など）のデータである場合、一般的なウェブテキストなどからなる$D_{pt}$との間の分布距離は大きくなる傾向があります。これにより、<span class="highlight">激しい分布シフトが生じ、$D_{pt}$と$D_{cpt}$の両方の検証セットの性能に影響</span>を与えます。
        </p>
</div>
<p class="content-box">
        論文では、$D_{cpt}$として法律文書データセット「Pile of Law」と、より一般的な知識を含むデータセット「Knowledge Pile」を比較し、同じCPTステップ数でも分布シフトの差が顕著であることを図6(a)で示しています。
    </p>
<img alt="Figure 6a: 異なるD_cptデータセットにおける分布シフトの差" src="distribution_shift_vs_dataset.jpg"/>
<p class="reference">図6(a): 異なる$D_{cpt}$データセットにおける分布シフトの差。横軸はステップ数、縦軸はFineWeb（$D_{pt}$）の損失。Pile of LawをCPTデータセットとして使用した場合（オレンジ線）は、Knowledge Pile（青線）よりも$D_{pt}$の損失が急激に上昇しており、より強い分布シフトを示している。</p>
<p class="content-box">
        この図は、$D_{cpt}$ データセットの選択が、元の $D_{pt}$ データセットからの逸脱度合い（分布シフト）に大きく関わることを示しています。 Pile of Law のような専門性の高いデータセットは、一般的な FineWeb データセットとの隔たりが大きく、CPT を行うと FineWeb 上の性能 (汎用性能) が急速に悪化する (損失が上昇する) ことを意味します。一方、Knowledge Pile は比較的分布シフトが小さいことがわかります。
    </p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-undo-alt"></i> $D_{pt}$ Dataset Replay (PTデータセットのリプレイ)</p>
<p>
            経験的に、CPTプロセス中に$D_{pt}$の検証損失が急激に増加する（いわゆる<span class="highlight">破滅的忘却</span>）のを緩和するために、$D_{pt}$の一部を$D_{cpt}$に混合して学習する（リプレイする）ことが一般的に行われます。
            この<span class="keyword">リプレイ率</span>は、分布シフトに影響を与える重要な役割を果たします。
        </p>
<p>
            実験では、リプレイ率を変えることで、<span class="highlight">リプレイ率が高いほど分布シフトが小さくなり、元のデータセット分布からの逸脱を効果的に遅らせる</span>ことが明らかになりました。図6(b)と図6(c)は、異なるリプレイ率が$D_{pt}$と$D_{cpt}$の検証セットにおける分布シフトに与える影響を示しています。
        </p>
</div>
<img alt="Figure 6b, 6c: 異なるリプレイ率におけるD_ptとD_cpt検証セットの分布シフト" src="knowledge_pile_loss_vs_replay_ratio.jpg"/>
<p class="reference">図6(b): 異なるリプレイ率における$D_{pt}$検証セットの分布シフト（元のFineWebデータに対する損失）。<br/>図6(c): 異なるリプレイ率における$D_{cpt}$検証セットの分布シフト（Knowledge Pileデータに対する損失）。<br/>両図とも横軸はステップ数。リプレイ率が高い（例えば、KP:Fineweb=0:1は100% Finewebをリプレイ、つまり$D_{pt}$のみで学習）ほど、$D_{pt}$の損失上昇が抑えられ（図b）、$D_{cpt}$の損失低下は緩やかになる（図c）傾向が見られます。逆にリプレイ率が低い（例えば、KP:Fineweb=1:0は0%リプレイ）と、$D_{cpt}$の損失は急速に低下しますが、$D_{pt}$の損失は急上昇します。これは分布シフトの強さを示しています。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-images"></i> 図6全体の解説</p>
<p>図6は、$D_{cpt}$と$D_{pt}$データセット間の分布距離の違いによる分布シフトを比較しています。さらに、異なるリプレイ率が$D_{cpt}$と$D_{pt}$の両方の検証セット内の分布シフトに与える影響を調べています。(a)は異なるCPTデータセット（Knowledge PileとPile of Law）を用いた場合の$D_{pt}$（FineWeb）損失の変化を示し、分布シフトの大きさを比較しています。(b)と(c)は、Knowledge PileをCPTデータセットとし、FineWebのリプレイ率を変えた場合の、$D_{pt}$と$D_{cpt}$それぞれの損失変化を示しています。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-tachometer-alt"></i> 4.3. Peak Learning Rate (ピーク学習率)</h3>
<p class="content-box">
        実際のCPTシナリオでは、再ウォームアップ時の<span class="keyword">適切なピーク学習率（Peak LR）</span>を選択することが重要です。異なるピークLRは、$D_{pt}$と$D_{cpt}$の検証損失に大きく影響します。
        ここでは、式4を利用して、異なるピークLRにおける最終損失を予測します。PTモデルがWSD（Warmup-Stable-Decay）法でアニーリングされた後、異なるピークLRに再ウォームアップされると仮定します。
    </p>
<p class="content-box">
        図7(a)と図7(b)は、その結果を示しています。
    </p>
<img alt="Figure 7a, 7b, 7c" src="figure7_abc_combined.png" style="width: 100%; max-width: 800px;"/>
<p class="reference">図7: (a) 異なるピークLRにおける$D_{pt}$の予測損失。(b) 異なるピークLRにおける$D_{cpt}$の予測損失。(c) $D_{pt}$における臨界点と転換長。
    <br/><em>注: 論文中の図7は(a)(b)(c)が個別の図として提供されていますが、ここではそれらを統合した画像として扱います。提供された画像セットに個別の図7(a), (b), (c) がないため、論文のキャプションに基づいて説明します。</em></p>
<p class="content-box">
        図7(a)と(b)から分かるように、<span class="highlight">ピークLRが高いほど、ドメイン特化損失（$D_{cpt}$損失）の減少を加速させることができます</span>。しかしその一方で、<span class="highlight">汎用ドメイン損失（$D_{pt}$損失）の増加も速めてしまう</span>可能性があります。
    </p>
<div class="bubble-box">
<p>🎯 <strong>トレードオフの考慮</strong></p>
<p>
            ピークLRの選択は、<span class="keyword">ドメイン特化性能の向上速度</span>と<span class="keyword">汎用性能の維持</span>との間のトレードオフを考慮する必要があります。高いピークLRは新しい知識を素早く学習するのに役立ちますが、既存の知識を忘れてしまうリスクも高めます。
        </p>
</div>
<h3 class="subsection-title"><i class="fas fa-shoe-prints"></i> 4.4. Continual Pre-Training Steps (継続的事前学習ステップ数)</h3>
<p class="content-box">
        CPTプロセスでは、<span class="keyword">訓練ステップ数を事前に決定する</span>ことが不可欠です。一般的には、訓練ステップ数が多いほど、$D_{cpt}$検証損失は低くなる傾向があります。これは直感的にも理解できるでしょう。
    </p>
<p class="content-box">
        しかし、$D_{pt}$検証損失（汎用性能）に対するCPTステップ数の影響は、以下のような様々なパターンを示します。
    </p>
<div class="info-grid">
<div class="info-card">
<p class="badge orange"><i class="fas fa-arrow-trend-up"></i> パターン1</p>
<p>継続的に上昇する。</p>
</div>
<div class="info-card">
<p class="badge purple"><i class="fas fa-chart-line"></i> パターン2</p>
<p>初期に上昇した後、下降するが、元の損失レベルには戻らない。</p>
</div>
<div class="info-card">
<p class="badge green"><i class="fas fa-arrow-down-up-across-line"></i> パターン3</p>
<p>初期に上昇した後、下降し、元の損失レベルよりも低くなる。</p>
</div>
</div>
<p class="content-box">
        このような状況は、<span class="keyword">PTモデルの状態</span>（どれだけ事前学習が進んでいるか）と<span class="keyword">分布距離</span>（PTデータとCPTデータの違いの大きさ）に関連しています。
    </p>
<p class="content-box">
        図7(c)は、この現象を説明するための概念図です。
    </p>
<div class="glass-card">
<p>
            $D_{cpt}$に対する隠れたPT曲線（もしPTデータではなくCPTデータで最初から学習した場合の理想的な損失曲線）の<span class="keyword">既約損失 (irreducible loss)</span>（青い破線）は、$D_{pt}$に対するPT曲線上の特定のPTステップを生み出します。これを<span class="keyword">臨界点 (Critical Point)</span>と呼びます。
            臨界点は、CPTデータセットにおける理論上の学習限界に、PTデータセットでの学習で到達するのに必要なステップ数、と解釈できます。
        </p>
<div class="pipeline">
<div class="pipeline-step">
<strong>もしCPTがこの臨界点より<span class="highlight">前</span>に行われる場合:</strong>
<p>$D_{pt}$の損失曲線は、一般的に一度上昇してから下降し、初期損失よりも低い損失を達成する機会がまだあります。この、初期損失よりも低い損失を達成するために必要な最小訓練ステップ数を<span class="keyword">転換長 (Turning Length)</span>と呼びます。</p>
</div>
<div class="pipeline-step">
<strong>もしCPTが臨界点より<span class="highlight">後</span>に行われる場合:</strong>
<p>PTが十分に進んでいるため、$D_{pt}$の損失は上昇する一方か、あるいは下がっても初期損失より低くはなりにくいと考えられます。</p>
</div>
</div>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> Finding 4 (発見4)</p>
<p>
            汎用ドメイン$D_{pt}$において、<span class="highlight">事前学習が不十分であるか、または分布シフトが弱い場合</span>は、十分な継続的訓練の後、初期段階よりも低い損失を達成できる可能性があります（パターン3）。
            <br/><br/>
            それ以外の場合（事前学習が十分に進んでいる、または分布シフトが強い場合）は、訓練ステップ数に関わらず、初期段階よりも低い損失を得ることは難しくなります（パターン1または2）。このような状況では、<span class="highlight">訓練を重ねるほど汎用能力が悪化することが多い</span>です。
        </p>
</div>
<img alt="Figure 8: CPTのハイパーパラメータ最適化" src="figure8_abc_combined.png" style="width: 100%; max-width: 800px;"/>
<p class="reference">図8: 一般性能と下流性能のバランスを取るための異なる係数に基づくCPTのハイパーパラメータ最適化。(a) 最適な損失ポテンシャル。(b) 最適なピーク学習率。(c) 最適なリプレイ率。
    <br/><em>注: 論文中の図8は(a)(b)(c)が個別の図として提供されていますが、ここではそれらを統合した画像として扱います。提供された画像セットに個別の図8(a), (b), (c) がないため、論文のキャプションに基づいて説明します。これらの図は、セクション5で詳細に議論される内容を示唆しています。</em></p>
</div>
<div class="section-card" id="5._Balance_Between_$D___{_p_t_}$_and_$D___{_c_p_t_}$_Loss">
<h2 class="section-title"><i class="fas fa-balance-scale"></i>5. Balance Between $D _ { p t }$ and $D _ { c p t }$ Loss</h2>
<div class="content-box">
<p>このセクションでは、継続的事前学習（CPT）において、非常に重要な問題である<span class="keyword">汎用性能の維持</span>と<span class="keyword">ドメイン特化性能の向上</span>という2つの目標のバランスをどのように取るかについて議論します。</p>
<p>通常、事前学習データセット（$D_{pt}$）での検証損失と、継続的事前学習データセット（$D_{cpt}$）での検証損失は、<span class="highlight">トレードオフの関係</span>にあります。つまり、一方の性能を上げようとすると、もう一方の性能が犠牲になる傾向があるのです。このバランスを最適化することが、CPT中のモデル全体の性能を最大化する鍵となります。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-pencil-ruler"></i>重要な定義</div>
<ul class="unstyled-list">
<li>✏️ <span class="keyword">$\Delta L_{D_{pt}}$</span>: $D_{pt}$ 検証セットにおける損失の<span class="highlight">増加量</span>。これは、CPTによって元の汎用知識がどれだけ失われたか（忘却されたか）を示します。</li>
<li>✏️ <span class="keyword">$\Delta L_{D_{cpt}}$</span>: $D_{cpt}$ 検証セットにおける損失の<span class="highlight">減少量</span>。これは、CPTによって新しいドメインにどれだけ適応できたかを示します。</li>
</ul>
</div>
<p>これらの損失変化のバランスを取るために、正規化されたバランス係数を導入し、以下の最適化問題を考えます。</p>
<div class="formula">
<p><strong><i class="fas fa-calculator"></i> 損失のバランスを取るための目的関数 (式5)</strong></p>
            $$
            \operatorname* { m i n } _ { S _ { 1 } ^ { c p t } , S _ { 2 } ^ { c p t } } \quad \lambda _ { 1 } \Delta L _ { D _ { p t } } + \lambda _ { 2 } \Delta L _ { D _ { c p t } }
            $$
            <p>ここで、$S _ { 1 } ^ { c p t }$ はCPTフェーズでの学習率の総和（前方領域）、$S _ { 2 } ^ { c p t }$ はCPTフェーズでの学習率アニーリングの影響（アニーリング領域）を表す変数で、これらを調整することで目的関数を最小化します。</p>
<p>また、バランス係数 $\lambda_1$ と $\lambda_2$ については、以下の制約があります。</p>
            $$
            \lambda _ { 1 } + \lambda _ { 2 } = 1
            $$
        </div>
<div class="note-box">
<div class="note-title"><i class="fas fa-lightbulb"></i> バランス係数 $\lambda_1, \lambda_2$ の役割</div>
<p>これらの係数は、<span class="highlight">汎用性能とドメイン特化性能のどちらをより重視するか</span>という私たちの事前知識や目的に基づいて設定されます。</p>
<ul class="unstyled-list">
<li><i class="fas fa-weight-hanging"></i> <span class="keyword">$\lambda_1$ が大きい場合</span>: $D_{pt}$ の損失増加 $\Delta L_{D_{pt}}$ を小さく抑えることを重視します。つまり、<span class="highlight">汎用性能の維持</span>に重きを置きます。</li>
<li><i class="fas fa-weight-hanging"></i> <span class="keyword">$\lambda_2$ が大きい場合</span> (これは $\lambda_1$ が小さいことと同義): $D_{cpt}$ の損失減少 $\Delta L_{D_{cpt}}$ を大きくすることを重視します。つまり、<span class="highlight">ドメイン特化性能の向上</span>に重きを置きます。</li>
</ul>
<p>🎯 この目的関数を最小化することで、$D_{pt}$ での性能低下を抑えつつ、$D_{cpt}$ での性能を効率的に向上させる最適なCPT戦略を見つけることを目指します。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i>5.1. Optimal Hyper-Parameters</h3>
<div class="content-box">
<p>前述のバランス係数 $\lambda_1$ と $\lambda_2$ の設定値に応じて、CPTプロセスにおけるいくつかの<span class="keyword">最適なハイパーパラメータ</span>が存在することが示唆されます。これらを理解することで、特定の目標に合わせたCPT戦略を設計できます。</p>
<div class="info-grid">
<div class="info-card glass-card">
<h4><i class="fas fa-chart-line"></i> 最適な損失ポテンシャル (Optimal Loss Potential)</h4>
<p>論文中のFig. 8aで示されているように、異なる $\lambda_1$ の値に対して、<span class="keyword">最適な損失ポテンシャル</span>を持つモデルが存在します。</p>
<div class="definition-box">
<div class="definition-title"><i class="fas fa-battery-half"></i> 損失ポテンシャルとは？</div>
<p>学習率アニーリング（学習率を徐々に下げていく手法）によって、将来的にどれだけ損失が減少しうるか、その「可能性」や「余地」を指します。損失ポテンシャルが高いモデルは、まだ学習によって性能が向上する余地が大きいと考えられます。</p>
</div>
<p>この最適な損失ポテンシャルは、$D_{pt}$ でのアニーリング（汎用性能をある程度維持するための調整）と、下流ドメイン ($D_{cpt}$) のための十分な損失ポテンシャルを確保することのバランスを取るものです。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-balance-scale-right"></i> バランスの考え方</div>
<p>例えば、$\lambda_1$ が大きい（汎用性能重視）場合、PTモデルは $D_{pt}$ でよりアニーリングされている（損失ポテンシャルが低い）方が望ましいかもしれません。逆に $\lambda_1$ が小さい（ドメイン特化重視）場合は、下流タスクへの適応のために損失ポテンシャルが高い状態が望ましいかもしれません。</p>
</div>
</div>
<div class="info-card glass-card">
<h4><i class="fas fa-tachometer-alt"></i> 最適なピーク学習率 (Optimal Peak Learning Rate)</h4>
<p>CPTプロセスにおける<span class="keyword">最適なピーク学習率</span>も導き出すことができます。</p>
<p>$\lambda_1$ が大きい（つまり、$D_{pt}$ の損失増加を最小限に抑えたい）場合、より<span class="highlight">低いピーク学習率</span>が必要となります。これは、学習率が高いとモデルが急激に変化し、汎用性能が損なわれやすいためです。</p>
<p>この関係は、論文のFig. 8bに示されており、<span class="keyword">べき乗則に従う曲線</span> (power-law curve) を描きます。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-link"></i> $\lambda_1$ とピーク学習率の関係</div>
<p>📌 <span class="keyword">$\lambda_1$ 大 (汎用性能重視)</span> <i class="fas fa-arrow-right"></i> <span class="highlight">ピーク学習率 小</span></p>
<p>📌 <span class="keyword">$\lambda_1$ 小 (ドメイン特化重視)</span> <i class="fas fa-arrow-right"></i> <span class="highlight">ピーク学習率 大</span> (ただし、大きすぎると学習が不安定になる可能性あり)</p>
</div>
</div>
<div class="info-card glass-card">
<h4><i class="fas fa-redo-alt"></i> 最適なリプレイ比率 (Optimal Replay Ratio)</h4>
<p>リプレイ比率（CPT中に元の事前学習データ $D_{pt}$ をどれくらいの割合で混ぜるか）に関するスケーリング則（Eq. 8、本セクションでは直接示されていませんが、以前のセクションで議論された式を指します）に基づいて、<span class="keyword">最適なリプレイ比率</span>を決定できます。これは論文のFig. 8cに示されています。</p>
<ul class="unstyled-list">
<li><i class="fas fa-chart-line"></i> <span class="highlight">同じ分布の線 (Same distribution line, 青い破線)</span>: モデルを最初から事前学習する場合の最適なリプレイ比率は、検証損失の目標値と同じであるべきことを示しています。</li>
<li><i class="fas fa-exchange-alt"></i> <span class="highlight">CPT中の変化</span>: しかし、CPTプロセスでは、PTモデルは既に $D_{pt}$ で学習済みであるため、最適なリプレイ比率の線はシフトします。つまり、既に $D_{pt}$ の知識を持っているため、ゼロから学習する場合とは異なるリプレイ戦略が最適になるということです。</li>
</ul>
</div>
<div class="info-card glass-card">
<h4><i class="fas fa-hourglass-half"></i> 転換長 (Turning Length)</h4>
<p>論文の図13（本文中ではFig. 13と記載されていますが、提供された画像は `turning_length_vs_lambda1.jpg` です）に示されるように、異なるバランス係数に対して異なる<span class="keyword">転換長</span>が得られます。</p>
<img alt="Fig. 13: Turning Length vs Lambda1" src="turning_length_vs_lambda1.jpg"/>
<p class="reference">図13: 異なる係数に対するCPTの転換長</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-search-plus"></i> 図13の見方</div>
<p>この図は、横軸にバランス係数 $\lambda_1$ を、縦軸に転換長 (Turning Length) を示しています。転換長とは、複合損失（$\lambda_1 \Delta L_{D_{pt}} + \lambda_2 \Delta L_{D_{cpt}}$）が初期値を下回るまでに必要なCPTステップ数を指します。つまり、学習を進めることで「トータルで見て良くなった」と言えるまでのステップ数です。</p>
<ul class="unstyled-list">
<li><i class="fas fa-angle-double-down"></i> <span class="keyword">$\lambda_1$ が小さい場合</span>: グラフの左側に対応します。このとき、$D_{cpt}$ の損失減少が複合損失計算において支配的になります。その結果、複合損失は初期値を常に下回った状態（転換長が0に近い、またはマイナスになるイメージ）を維持します。つまり、ドメイン特化を重視すると、すぐにトータルの損失は改善します。図では、$\lambda_1$ が約0.65より小さい領域では転換長が0に近い値を取っています。</li>
<li><i class="fas fa-arrows-alt-h"></i> <span class="keyword">中程度の $\lambda_1$ の場合</span>: 特定の転換CPTステップが存在します。つまり、ある程度のステップ数を学習しないと、複合損失が初期値を下回りません。図では、$\lambda_1$ が約0.65から0.95の範囲で、$\lambda_1$ が大きくなるにつれて転換長が急激に増加し、最終的には10Kステップを超える非常に長い転換長が必要になることを示しています。これは、汎用性能の維持をある程度重視すると、総合的な改善が見られるまでにより多くの学習が必要になることを意味します。</li>
<li><i class="fas fa-angle-double-up"></i> <span class="keyword">$\lambda_1$ が大きい場合</span>: グラフの右端（$\lambda_1$ が0.95以上）に対応します。この場合、どれだけ学習ステップを重ねても、複合損失は常に初期損失よりも高くなります（転換長が無限大になるイメージ）。つまり、汎用性能の維持を極端に重視すると、CPTによる総合的な損失改善は期待できないことを示唆しています。図では、$\lambda_1$ が非常に大きい領域（図では0.95以上）で転換長が非常に大きな値（&gt;10Kステップ）を取っており、実質的に初期損失を下回るのが困難であることを示しています。</li>
</ul>
<p>色の異なる線（Strong, Moderate, Weak Distribution Shift）は、事前学習データと継続学習データの分布のズレの大きさを表しています。分布のズレが大きいほど（Strong Distribution Shift、水色の線）、より小さい $\lambda_1$ で転換長が急増し始めます。つまり、元データと新データの違いが大きいほど、汎用性を少しでも維持しようとすると、総合的な改善が難しくなることを示しています。</p>
</div>
</div>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-puzzle-piece"></i>5.2. Out-of-Domain Validation Set</h3>
<div class="content-box">
<p>これまでの議論で使われてきたスケーリング則（Eq. 4）は、主に事前学習データセット $D_{pt}$ と継続的事前学習データセット $D_{cpt}$ の検証セットに特化したものでした。しかし、実世界の応用では、これら以外の<span class="keyword">ドメイン外 (Out-of-Domain, OOD) の検証セット</span> $D_{ood}$ の性能も重要になることがあります。</p>
<div class="challenge-box">
<div class="challenge-title"><i class="fas fa-exclamation-triangle"></i>課題</div>
<p>提案されたスケーリング則 (Eq. 4) は、$D_{ood}$ 検証セットには直接適用できません。</p>
</div>
<p>この問題に対処するため、著者らは過去の研究 (Ye et al., 2024; Liu et al., 2025) に着想を得ています。これらの研究では、ドメイン外の検証損失が、他の<span class="highlight">基本となるドメインの損失の線形結合</span>で効果的に表現できることが示されています。</p>
<p>そこで、CPTプロセスにおいて、$D_{ood}$ データセットの損失を、$D_{pt}$ と $D_{cpt}$ の検証損失に基づく線形結合で表現するという仮説を立てます。</p>
<div class="formula">
<p><strong><i class="fas fa-link"></i> OODデータセットの損失の線形結合仮説 (式6)</strong></p>
            $$
            L _ { D _ { o o d } } = \lambda _ { 1 } ^ { \prime } L _ { D _ { p t } } + \lambda _ { 2 } ^ { \prime } L _ { D _ { c p t } }
            $$
            <p>ここで、$L_{D_{ood}}$ はOODデータセットの損失、$L_{D_{pt}}$ は$D_{pt}$ の損失、$L_{D_{cpt}}$ は$D_{cpt}$ の損失です。$\lambda_1^\prime$ と $\lambda_2^\prime$ は、データセットのみに関連し、他の学習ハイパーパラメータには依存しない係数です。</p>
</div>
<p>著者らはこの仮説を検証し、いくつかのOODデータセット例について $\lambda_1^\prime$ と $\lambda_2^\prime$ を計算しています（詳細はAppendix Jに記載）。</p>
<div class="info-card glass-card">
<h4><i class="fas fa-chart-pie"></i> $D_{ood}$ の損失予測</h4>
<p>$D_{ood}$ の検証損失はEq. 4の形式には従いませんが、係数 $\lambda_1^\prime$ と $\lambda_2^\prime$ を計算し特定することで、$D_{pt}$ と $D_{cpt}$ の損失曲線の線形結合を用いて $D_{ood}$ の<span class="keyword">損失曲線を予測することが可能</span>になります。</p>
<p>興味深いことに、この問題は、$D_{pt}$ と $D_{cpt}$ の損失間のバランス問題（Eq. 5で議論されたもの）に帰着します。これにより、学習率やリプレイ比率といった最適なハイパーパラメータを見つけることにつながり、これは前セクションで十分に議論された内容です。</p>
</div>
<img alt="Fig. 9: Predicted loss curve of D_ood validation set" src="ood_validation_loss_linear_combination.jpg"/>
<p class="reference">図9: $D_{pt}$ と $D_{cpt}$ の検証損失の線形結合を利用した $D_{ood}$ 検証セットの予測損失曲線。左側には $D_{pt}$ に類似した上昇曲線、右側には $D_{cpt}$ に類似した下降曲線の例が示されている。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-search-plus"></i> 図9の見方</div>
<p>図9は、様々なOODデータセットにおける損失の予測結果を示しています。各グラフのタイトルには、そのOODデータセット名と、線形結合の係数（例: Books = 0.475 $D_{cpt}$ + 0.611 $D_{pt}$）が記載されています。横軸は学習ステップ、縦軸はOODデータセットの損失です。実線（または点線）が実際の損失曲線、重ねてプロットされている線（論文中では言及されていませんが、通常は予測曲線）が線形結合による予測曲線です。</p>
<p>この図から、提案されたCPTスケーリング則とOODデータセットに対する線形結合が、実際のシナリオにおいて非常に効果的かつ実用的であることがわかります。ほぼ完璧な予測は、このアプローチの有効性を示唆しています。</p>
<p>さらに、計算された係数 $\lambda_1^\prime$ と $\lambda_2^\prime$ は、OODデータセットと $D_{pt}$ または $D_{cpt}$ との<span class="keyword">「類似性」</span>を表します。</p>
<p>図9が示すように、OODデータセットは大きく2種類に分類できます：</p>
<ul class="unstyled-list">
<li><i class="fas fa-arrow-trend-up"></i> <strong>$D_{pt}$ に類似したデータセット</strong> (比較的大きな $\lambda_1^\prime$ を持つ): これらのデータセットの損失曲線は、CPTが進むにつれて上昇する傾向があります（例：図9(a)のC4, SlimPajama, Books）。これは、$D_{pt}$（汎用データ）の性質を色濃く反映しているため、特化データ $D_{cpt}$ での学習が進むと、これらのデータセットに対する性能は低下することを示しています。</li>
<li><i class="fas fa-arrow-trend-down"></i> <strong>$D_{cpt}$ に類似したデータセット</strong> (比較的大きな $\lambda_2^\prime$ を持つ): これらのデータセットの損失曲線は、CPTが進むにつれて下降する傾向があります（例：図9(b)のStackExchange, Arxiv, Stories）。これは、$D_{cpt}$（特化データ）の性質を反映しているため、CPTによってこれらのデータセットに対する性能が向上することを示しています。Storiesデータセットは $\lambda_1^\prime$ が負の値（-0.516）を取っており、これは $D_{cpt}$ に非常に強く似ていて、$D_{pt}$ とは逆の傾向を示すことを意味します。</li>
</ul>
</div>
<div class="bubble-box">
<p><strong><i class="fas fa-lightbulb"></i> 発見5 (Finding 5)</strong></p>
<p>$D_{pt}$ と $D_{cpt}$ の損失のバランスを取るために設計された、<span class="keyword">最適な損失ポテンシャル、ピーク学習率、リプレイ比率</span>が存在します。さらに、転換長は異なるバランスの重み付けによって変化します。</p>
<p>$D_{ood}$ の最適化は、線形結合のトリックを利用することにより、$D_{pt}$ と $D_{cpt}$ の損失のバランスを取る問題と等価になります。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-folder-open"></i>5.3. Open-Source PT Models</h3>
<div class="content-box">
<p>大規模言語モデル（LLM）のコミュニティでは、事前学習済みモデル（PTモデル）を自前で学習するのではなく、<span class="keyword">オープンソースのモデル</span>を利用することが一般的です。しかし、これらのオープンソースPTモデルの多くは、学習に関する詳細情報（例：事前学習データセットの正確な分布、学習時の損失ポテンシャル、事前学習の総量など）が報告されていません。</p>
<div class="challenge-box">
<div class="challenge-title"><i class="fas fa-map-signs"></i>課題</div>
<p>これらの情報が不明であるため、本論文で提案されているCPTスケーリング則を直接適用することが困難になります。</p>
</div>
<p>この問題を解決するために、著者らは以下のシンプルな手法を提案し、スケーリング則を再度適用可能にします。</p>
<div class="pipeline">
<div class="pipeline-step">
<div class="step-number">1</div>
<div class="step-content">
<strong><i class="fas fa-database"></i> 未知のPTデータセット分布への対処:</strong><br/>
                    PTデータセットの分布が不明な場合、プロービングに基づく手法（Hayase et al., 2024）などが提案されていますが、ここではよりシンプルに、オープンソースの<span class="highlight">Common Crawlデータセットを代理の $D_{pt}$ (proxy $D_{pt}$)</span>として利用し、真の汎用性能のダイナミクスを近似します。
                </div>
</div>
<div class="pipeline-step">
<div class="step-number">2</div>
<div class="step-content">
<strong><i class="fas fa-tools"></i> 未知の変数をパラメータとしてフィッティング:</strong><br/>
                    スケーリング則をフィッティングする際に、いくつかの未知の変数を<span class="highlight">フィッティング対象のパラメータ</span>として扱います。例えば、$S_1^{pt}$（PTフェーズの学習率総和）を、非公開の真の $S_1^{pt}$ に近くなるようにフィッティングするパラメータと見なします。
                </div>
</div>
<div class="pipeline-step">
<div class="step-number">3</div>
<div class="step-content">
<strong><i class="fas fa-flag-checkered"></i> 最終学習率の仮定:</strong><br/>
                    最近のオープンソースPTモデルの多くは、性能向上のために最終学習率（LR）を最小値までアニーリング（徐々に下げる）しています。この傾向に基づき、$S_2^{cpt}$（CPTフェーズのアニーリング領域）を計算する際に、全てのオープンソースモデルの<span class="highlight">最終学習率はゼロであると仮定</span>します。（詳細はAppendix G参照）
                </div>
</div>
</div>
<p>これらの解決策の有効性を検証するため、著者らは LLaMA3.2-1B (Dubey et al., 2024) モデルを継続的に事前学習し、RedPajama (Computer, 2023) データセットを代理の $D_{pt}$ として選択しました。</p>
<img alt="Fig. 18 in Appendix G" src="redpajama_c4_loss_vs_step_cpt_proxy.jpg"/>
<p class="reference">図18 (Appendix Gより引用): LLaMA3.2-1BのCPT損失曲線のフィッティングと予測。代理$D_{pt}$としてRedPajama-C4を使用。</p>
<div class="note-box">
<div class="note-title"><i class="fas fa-search-plus"></i> 図18 (redpajama_c4_loss_vs_step_cpt_proxy.jpg) の見方</div>
<p>この図は、横軸に学習ステップ、縦軸にRedPajama-C4データセット（代理$D_{pt}$）の損失を示しています。紫色の「x」マーク（Ground Truth Loss）は実際の観測された損失値を、オレンジ色の破線（Predict Loss Curve）は提案手法によって予測された損失曲線を表しています。図中にはフィッティングに使われた数式（$L(s) = 2.742 + \dots$）も記載されています。</p>
<p>図が示すように、LLaMA3.2-1BのCPT損失曲線に対する<span class="keyword">ほぼ完璧なフィッティングと予測</span>は、提案された手法の有効性を示唆しています。</p>
<p>さらに、この結果は、提案されたスケーリング則が、PTモデル情報が未知であるCPTシナリオにも<span class="highlight">容易に拡張可能</span>であることを示しており、CPTの学習ダイナミクスを捉える上での本スケーリング則の優位性を実証しています。</p>
</div>
</div>
</div>
<div class="section-card" id="6._Discussion">
<h2 class="section-title"><i class="fas fa-comments"></i> 6. Discussion</h2>
<p style="margin-bottom: 25px; line-height: 1.6;">
        このセクションでは、本論文で提案された継続的事前学習（CPT）におけるスケーリング則について、その核心部分をさらに深く掘り下げていきます。具体的には、<span class="keyword">法則の数式的な構成（定式化）</span>、実際のデータを用いて法則のパラメータを決定する<span class="keyword">フィッティング方法</span>、そしてこの研究が持つ<span class="keyword">限界点</span>について、詳細に議論します。これらの議論を通じて、提案手法の特性と実用性をより明確に理解することを目指しましょう。 🔍
    </p>
<h3 class="subsection-title"><i class="fas fa-drafting-compass"></i> Laws Formulation (法則の定式化)</h3>
<div class="content-box">
<p>
            本論文で提案されたスケーリング則の中心的な要素の一つに、<span class="keyword">Eq. 1</span>で登場する <span class="keyword">$S_2$</span> という項があります。この $S_2$ は、学習率を徐々に下げていく「<span class="highlight">学習率アニーリング</span>」の効果を捉えるためのものです。実は、この $S_2$ の具体的な数式の形（定式化）は、一つに決まっているわけではなく、様々な可能性があります。
        </p>
<div class="bubble-box" style="margin-top: 20px; margin-bottom: 20px;">
<p><i class="fas fa-lightbulb"></i> <strong>$S_2$ の形は色々考えられる！</strong></p>
            例えば、Luo ら (2025) の研究では、<span class="keyword">多重べき乗形式 (multi-power form)</span> という、より複雑な $S_2$ の形が提案されています。これは、本論文でも参考にされている Tissue ら (2024) の研究を発展させたものです。
        </div>
<p>
            では、なぜこの論文では、特定の $S_2$ の定式化（Eq. 1で採用されている形）を選んだのでしょうか？ 🤔 その理由は主に以下の2点です。
        </p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="info-card glass-card">
<div class="icon-item" style="text-align: center;"><i class="fas fa-feather-alt fa-2x" style="color: var(--color-accent1);"></i></div>
<h4 style="text-align: center; color: var(--color-accent1); font-family: 'Yomogi', cursive;">パラメータ数が少ない <span class="badge green">シンプル</span></h4>
<p style="font-size: 13px; text-align: center;">採用した形式は、他の複雑な形式に比べてパラメータの数が少ないため、モデルがシンプルになり、扱いやすくなります。</p>
</div>
<div class="info-card glass-card">
<div class="icon-item" style="text-align: center;"><i class="fas fa-cogs fa-2x" style="color: var(--color-accent2);"></i></div>
<h4 style="text-align: center; color: var(--color-accent2); font-family: 'Yomogi', cursive;">実用的な効果が高い <span class="badge purple">効果的</span></h4>
<p style="font-size: 13px; text-align: center;">シンプルでありながら、実際の学習データに対してもうまく機能し、CPTのダイナミクスを効果的に捉えることができます。</p>
</div>
</div>
<div class="note-box" style="margin-top: 25px;">
<p class="note-title"><i class="fas fa-book-open"></i> Appendix Iでの比較検討</p>
<p>
                論文の付録Iでは、ここで採用した形式以外にも、$S_2$ のいくつかの派生形について比較検討が行われています。例えば：
            </p>
<ul style="list-style-type: '✏️ '; padding-left: 20px; font-size: 13px;">
<li>学習率で重み付けした係数 (<span class="keyword">LR-weighted coefficient</span>) を追加する形。</li>
<li>$S_2$ 項自体にさらにべき乗 (<span class="keyword">power</span>) を適用する形。</li>
</ul>
<p>
                これらの比較の結果、どの形式も一定の有効性を示したものの、<span class="highlight">本論文で採用された形式が、シンプルさ（パラメータの少なさ）の点で最も優れていた</span>と結論付けられています。✨
            </p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-sliders-h"></i> Laws Fitting (法則のフィッティング)</h3>
<div class="content-box">
<p>
            提案されたスケーリング則を実際のデータに適用し、その予測能力を活用するためには、まず法則に含まれるパラメータ（例えば $L_0, A, C_1, C_2, B, E, \alpha, \beta$ など）を、観測されたデータに基づいて決定する作業、すなわち「<span class="keyword">フィッティング (fitting)</span>」が必要です。
        </p>
<p>
            実験では、フィッティング用のデータを収集するために、主に以下の3種類の<span class="keyword">学習率スケジュール (Learning Rate Schedules, LRS)</span> が用いられました。これらは実際の応用場面でも広く使われているものです。
        </p>
<div class="feature-card-grid" style="margin-top:20px; margin-bottom:20px;">
<div class="feature-item">
<div class="icon-item"><i class="fas fa-minus fa-2x" style="color: var(--color-primary);"></i></div>
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-primary); margin-bottom: 5px;">定数学習率 (Constant LRS)</h4>
<p style="font-size: 13px;">学習を通じて学習率が一定の値に保たれます。</p>
</div>
<div class="feature-item">
<div class="icon-item"><i class="fas fa-wave-square fa-2x" style="color: var(--color-secondary);"></i></div>
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-secondary); margin-bottom: 5px;">コサイン学習率 (Cosine LRS)</h4>
<p style="font-size: 13px;">学習率がコサインカーブに従って滑らかに減少していきます。</p>
</div>
<div class="feature-item">
<div class="icon-item"><i class="fas fa-chart-line fa-2x" style="color: var(--color-accent1);"></i></div>
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-accent1); margin-bottom: 5px;">WSD学習率 (WSD LRS)</h4>
<p style="font-size: 13px;">Warmup（徐々に上昇）-Stable（一定）-Decay（減少）という段階を経る学習率です。</p>
</div>
</div>
<p style="text-align: center; font-style: italic; color: var(--color-gray); margin-bottom: 25px;">
<i class="fas fa-info-circle"></i> もちろん、これら以外にも多くの異なるLRSを利用してフィッティングデータを収集することが可能です。
        </p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-question-circle"></i> スケーリング則を適用する手順は？</p>
<div class="process-step">
<div class="step-number">1</div>
<div class="step-content">
<strong style="color: var(--color-primary);">短期間の学習:</strong> まず、一般的によく使われるLRS（例：定数学習率やコサイン学習率）を用いて、モデルを<span class="highlight">ほんの数ステップだけ</span>訓練します。🏃‍♂️
                </div>
</div>
<div class="process-step">
<div class="step-number">2</div>
<div class="step-content">
<strong style="color: var(--color-primary);">損失の記録:</strong> その短い訓練の間に、<span class="highlight">各ステップでの検証損失</span>を注意深く記録します。📝
                </div>
</div>
<div class="process-step">
<div class="step-number">3</div>
<div class="step-content">
<strong style="color: var(--color-primary);">パラメータのフィッティング:</strong> 収集した損失データを使って、スケーリング則の数式に含まれる未定のパラメータ群を<span class="highlight">最適に調整（フィッティング）</span>します。⚙️
                </div>
</div>
<div class="bubble-box" style="border-color: var(--color-accent1); margin-top:15px;">
<p style="text-align:center;">🎉 <strong style="color: var(--color-accent1); font-family: 'Yomogi', cursive; font-size:16px;">フィッティング完了！その威力は？</strong> 🎉</p>
<p style="text-align:center; font-size:13px;">
                     一度パラメータが定まれば、このスケーリング則は非常に強力です。なんと、<span class="highlight">まだ試していない他の特殊なLRSを用いた場合や、非常に長期間の訓練を行った場合の損失曲線までも予測</span>することが可能になるのです！🔮
                 </p>
</div>
</div>
<p style="margin-top: 25px;">
            この「<span class="highlight">少ない試行回数で効率よくパラメータを決定し、その後の挙動を予測する</span>」というアプローチは、<span class="keyword">Tissue ら (2024)</span> の研究でも見られる「<span class="keyword">フィッティングコストの節約 (fitting cost conservation)</span>」という考え方と共通しています。本論文の貢献は、このスケーリング則がCPTプロセスの<span class="highlight">最終的な損失だけでなく、学習の途中経過全体（ダイナミクス）を記述できる</span>点にあります。これにより、CPTがどのように進行していくかをより深く理解できるのです。📊
        </p>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-exclamation-circle"></i> Limitations (限界)</h3>
<div class="content-box">
<p>
            どのような科学的研究にも限界が存在するように、本研究で提案されたスケーリング則にもいくつかの限界点があります。これらを認識することは、今後の研究の発展にとって重要です。🧐
        </p>
<div class="challenge-box" style="margin-top:20px;">
<p class="challenge-title"><i class="fas fa-search-minus"></i> 理論的基盤に関する課題</p>
<p>
                本研究の主な限界の一つは、提案された法則が主に<span class="keyword">経験的分析 (empirical analyses)</span>、つまり実際のデータ収集と観察に基づく分析、および<span class="keyword">実験的検証 (experimental verifications)</span>、つまり実験を通じた確認、に依存している点です。
            </p>
<p style="margin-top: 10px;">
<span class="highlight">厳密な数学的理論に基づいた分析や証明は、現時点では十分とは言えません。</span>😥
            </p>
<div class="bubble-box" style="border-color: var(--color-secondary); margin-top: 15px; margin-bottom: 15px;">
<p><i class="fas fa-brain"></i> <strong>なぜ理論的証明が難しいの？</strong></p>
<p style="font-size: 13px;">
                    大規模言語モデル（LLM）の学習は、非常に多くの要因（例：モデルのアーキテクチャ、学習データの内容、ハイパーパラメータの設定など、数千にも及ぶ要因！）が複雑に絡み合っています。このような状況は「<span class="keyword">非トイ環境 (non-toy environment)</span>」と呼ばれ、単純化された実験室のような環境とは大きく異なります。まるで、予測不可能な要素がたくさん詰まったブラックボックスのようです。📦<br/>
                    このような複雑なシステムにおいて、純粋に理論的な演繹（一般的な原理から論理的に結論を導くこと）によって法則を構築し証明するのは、極めて困難な挑戦なのです。🧩
                </p>
</div>
<p>
                しかし、この限界点を認識しつつも、提案されたスケーリング則が持つ価値を見失うべきではありません。
            </p>
<div class="note-box" style="border-left-color: var(--color-accent1); background-color: rgba(92, 184, 92, 0.1); margin-top:15px;">
<p class="note-title" style="color: var(--color-accent1);"><i class="fas fa-check-circle"></i> それでも実用的価値は高い！</p>
<p>
                    理論的な裏付けが今後の課題であるとしても、このスケーリング則は、継続的事前学習（CPT）プロセスの<span class="keyword">学習ダイナミクス</span>（つまり、学習が時間とともにどのように進展していくか）を<span class="highlight">合理的かつ適切に反映</span>することができています。
                </p>
<p style="margin-top: 5px;">
                    これは、実際のCPTシナリオにおいて、<span class="highlight">学習の挙動を予測したり、ハイパーパラメータを調整したりする際に、非常に役立つ</span>ことを意味します。現場の研究者やエンジニアにとって、実践的な指針を与えてくれるのです。🛠️
                </p>
</div>
</div>
</div>
<div style="margin-top: 30px; padding: 15px; background: linear-gradient(45deg, rgba(74, 111, 165, 0.1), rgba(255, 126, 95, 0.1)); border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.05);">
<p style="font-family: 'Yomogi', cursive; text-align: center; font-size: 16px; color: var(--color-dark);">
<i class="fas fa-rocket" style="color: var(--color-primary);"></i> この議論セクションを通じて、提案手法の強みと今後の課題がより明確になったことでしょう。特に、数式の背景、フィッティングの実際、そして研究の限界を理解することは、論文全体の深い理解に繋がります。
        </p>
</div>
</div>
<div class="section-card" id="7._Conclusion">
<h2 class="section-title"><i class="fas fa-trophy"></i> 7. Conclusion</h2>
<p style="margin-bottom: 20px;">このセクションでは、本論文で探求してきた大規模言語モデル（LLM）の<strong class="keyword">継続的事前学習（Continual Pre-Training, CPT）</strong>における<strong class="keyword">学習ダイナミクス</strong>に関する研究成果を総括し、その意義と今後の展望について述べます。論文全体の締めくくりとして、私たちが何を明らかにし、それがどのような価値を持つのかを明確に示します。</p>
<div class="bubble-box">
<p><i class="fas fa-microscope" style="color: var(--color-primary);"></i> 本研究の核心は、LLMが新しい知識を継続的に学習していく過程、すなわちCPTプロセスにおける<strong class="highlight">学習のダイナミクス</strong>を深く理解することにありました。特に、モデルが元々持っていた<strong class="highlight">一般ドメイン</strong>の知識と、新たに適応しようとしている<strong class="highlight">下流ドメイン（特定の専門分野やタスク）</strong>の性能が、学習の各ステップでどのように変化していくのかを追跡しました。これらのドメイン性能は、<strong class="keyword">検証損失（validation loss）</strong>という指標を用いて評価しています。検証損失が低いほど、そのドメインに対するモデルの性能が高いことを意味します。</p>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> 🔑 CPTスケーリング則の提案</h3>
<p>長時間の観察と詳細な分析の結果、私たちは新しい<strong class="keyword">CPTスケーリング則 (CPT Scaling Law)</strong>を提案しました。この法則は、CPTプロセスにおける検証損失の変動を予測するための数理モデルであり、特に以下の2つの重要な要素を統合的に扱っている点が特徴です。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));">
<div class="info-card">
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-secondary);"><i class="fas fa-random"></i> 1. 分布シフト (Distribution Shift)</h4>
<p>これは、モデルが学習するデータの性質が変わることによる影響を指します。CPTでは、汎用的な大規模データ（一般ドメインデータ <span class="badge blue">D_pt</span>）で事前学習されたモデルを、特定の専門分野のデータ（下流ドメインデータ <span class="badge orange">D_cpt</span>）で追加学習させます。このとき、<span class="badge blue">D_pt</span>と<span class="badge orange">D_cpt</span>のデータの統計的な分布は異なるため、モデルの性能はこの「分布のズレ」に適応する必要があります。このスケーリング則は、このズレが検証損失にどう影響するかを定量化します。</p>
<div style="text-align: center; margin-top:15px; padding:10px; background-color: rgba(74, 111, 165, 0.05); border-radius: 8px;">
<svg height="100" viewbox="0 0 200 100" width="200" xmlns="http://www.w3.org/2000/svg">
<defs>
<marker id="arrowHeadOrange" markerheight="6" markerwidth="6" orient="auto-start-reverse" refx="8" refy="5" viewbox="0 0 10 10">
<path d="M 0 0 L 10 5 L 0 10 z" fill="#FF7E5F"></path>
</marker>
</defs>
<ellipse cx="60" cy="50" fill="rgba(74,111,165,0.3)" rx="45" ry="25" stroke="#4A6FA5" stroke-dasharray="4" stroke-width="2"></ellipse>
<text fill="#2C3E50" font-family="Yomogi" font-size="12px" x="40" y="55">一般D (Dpt)</text>
<ellipse cx="140" cy="50" fill="rgba(255,126,95,0.3)" rx="45" ry="25" stroke="#FF7E5F" stroke-width="2"></ellipse>
<text fill="#2C3E50" font-family="Yomogi" font-size="12px" x="115" y="55">下流D (Dcpt)</text>
<path d="M 95 35 C 105 20, 115 20, 125 35" fill="none" marker-end="url(#arrowHeadOrange)" stroke="#2ECC71" stroke-linecap="round" stroke-width="2.5"></path>
<text fill="#2ECC71" font-family="Yomogi" font-size="12px" x="95" y="15">分布シフト</text>
</svg>
<p style="font-family: 'Zen Kurenaido', sans-serif; font-size:12px; color: var(--color-gray); margin-top:5px;">一般ドメインから下流ドメインへのデータの性質の変化</p>
</div>
</div>
<div class="info-card">
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-secondary);"><i class="fas fa-sliders-h"></i> 2. 学習率アニーリング (Learning Rate Annealing)</h4>
<p>これは、学習プロセス中に<strong class="highlight">学習率（モデルが一度にどれだけパラメータを更新するかの度合い）を徐々に小さくしていく</strong>戦略です。学習初期には比較的大きな学習率で素早く学習を進め、学習が進むにつれて学習率を小さくすることで、より細かい調整を行い、安定した収束を目指します。この学習率の変化が検証損失の減少パターンに影響を与えるため、スケーリング則に組み込まれています。</p>
<div style="text-align: center; margin-top:15px; padding:10px; background-color: rgba(255,126,95,0.05); border-radius: 8px;">
<svg height="100" viewbox="0 0 200 100" width="200" xmlns="http://www.w3.org/2000/svg">
<path d="M 20 80 Q 50 80 80 50 T 140 30 Q 170 20 180 20" fill="none" stroke="#FF7E5F" stroke-linecap="round" stroke-width="2.5"></path>
<text fill="#2C3E50" font-family="Yomogi" font-size="12px" x="15" y="95">学習開始</text>
<text fill="#2C3E50" font-family="Yomogi" font-size="12px" x="140" y="15">学習終盤</text>
<text fill="#4A6FA5" font-family="Yomogi" font-size="12px" transform="rotate(-90 10,50)" x="0" y="30">学習率 大</text>
<text fill="#4A6FA5" font-family="Yomogi" font-size="12px" transform="rotate(-90 190,60)" x="185" y="75">学習率 小</text>
</svg>
<p style="font-family: 'Zen Kurenaido', sans-serif; font-size:12px; color: var(--color-gray); margin-top:5px;">学習の進行と共に学習率を下げていく様子</p>
</div>
</div>
</div>
<p style="margin-top:15px;"><i class="fas fa-bullseye" style="color: var(--color-accent1);"></i> 🎯 このCPTスケーリング則の主な目的は、研究者や開発者が一般的に用いる<strong class="highlight">様々な学習率スケジュール（例：一定学習率、コサイン減衰など）のもとで、CPTの任意の途中段階の学習ステップ</strong>における検証損失を<strong class="keyword">予測</strong>することです。これにより、実際に長時間の学習を行う前に、性能の推移を見積もることが可能になります。</p>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-cogs"></i> 💡 スケーリング則がもたらす理解と応用</h3>
<p>提案したCPTスケーリング則は、単に損失を予測するだけでなく、CPTプロセスへの理解を深め、実用的な応用を促進します。</p>
<div class="framework-box" style="border-color: var(--color-accent2);">
<div class="framework-title" style="color: var(--color-accent2); border-bottom-color: var(--color-accent2);"><i class="fas fa-brain"></i> 包括的な理解の提供</div>
<p>この法則は、CPTにおける以下のような複数の重要な要素（キーファクター）が、モデルの性能にどのように、そして互いにどのように関連しながら影響を与えるかについての<strong class="highlight">包括的な理解</strong>を提供します。</p>
<div class="feature-card-grid">
<div class="feature-item" style="background-color: #f0f4ff;">
<div class="icon-item"><i class="fas fa-battery-half fa-2x" style="color: var(--color-accent1);"></i></div>
<p><strong>損失ポテンシャル (Loss Potential)</strong></p>
<p class="reference" style="font-size:13px;">学習率アニーリングによって、将来的にどれだけ損失が低下する余地があるかを示す潜在的な能力。</p>
</div>
<div class="feature-item" style="background-color: #fff0f4;">
<div class="icon-item"><i class="fas fa-tachometer-alt fa-2x" style="color: var(--color-accent2);"></i></div>
<p><strong>ピーク学習率 (Peak Learning Rate)</strong></p>
<p class="reference" style="font-size:13px;">学習率スケジュールの最大値。これが高いと学習は速いが不安定になる可能性も。</p>
</div>
<div class="feature-item" style="background-color: #f4fff0;">
<div class="icon-item"><i class="fas fa-shoe-prints fa-2x" style="color: var(--color-accent3);"></i></div>
<p><strong>学習ステップ数 (Training Steps)</strong></p>
<p class="reference" style="font-size:13px;">CPTに追加で費やす学習のステップ（回数）。</p>
</div>
<div class="feature-item" style="background-color: #fff4f0;">
<div class="icon-item"><i class="fas fa-sync-alt fa-2x" style="color: var(--color-secondary);"></i></div>
<p><strong>リプレイ比率 (Replay Ratio)</strong></p>
<p class="reference" style="font-size:13px;">元の事前学習データと新しいドメインデータをどの割合で混ぜて学習に使うか。</p>
</div>
</div>
<p style="margin-top:15px;">これらの要素が複雑に絡み合い、最終的なモデル性能を決定します。本スケーリング則は、その関係性を解き明かす一助となります。</p>
</div>
<div class="note-box" style="margin-top: 20px; border-left-color:var(--color-accent1)">
<div class="note-title" style="color:var(--color-accent1)"><i class="fas fa-tools"></i> ハイパーパラメータの最適化支援</div>
<p>CPTスケーリング則を用いることで、特定の訓練目標に応じた<strong class="keyword">ハイパーパラメータの最適化</strong>が可能になります。例えば、「一般ドメインの性能を維持しつつ、下流ドメインの性能を最大化したい」といった目標や、「限られた計算資源の中で最良の性能を得たい」といった目標に対して、どのハイパーパラメータの組み合わせが最適かを予測し、選択する手助けとなります。これにより、試行錯誤のコストを削減し、効率的なCPTが実現できます。</p>
<div style="text-align: center; margin-top:15px;">
<svg height="120" viewbox="0 0 250 120" width="250" xmlns="http://www.w3.org/2000/svg">
<rect fill="#E8F5E9" height="100" rx="10" ry="10" width="230" x="10" y="10"></rect>
<circle cx="60" cy="60" fill="#FFFFFF" r="30" stroke="#4CAF50" stroke-width="2"></circle>
<path d="M50 60 L57 67 L70 52" fill="none" stroke="#4CAF50" stroke-linecap="round" stroke-linejoin="round" stroke-width="3"></path>
<text fill="#388E3C" font-family="Yomogi" font-size="14px" x="100" y="45">目標設定:</text>
<text fill="#2E7D32" font-family="Zen Kurenaido" font-size="13px" x="100" y="65">汎用性維持 + 専門性向上</text>
<text fill="#388E3C" font-family="Yomogi" font-size="14px" x="100" y="85">最適なパラメータは？</text>
<path d="M 60 90 Q 80 105, 100 87" fill="none" marker-end="url(#arrowHeadOrange)" stroke="#4CAF50" stroke-width="1.5"></path>
</svg>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-flask"></i> 🧪 さらなる実験による法則の拡張性実証</h3>
<p>私たちの研究は、基本的なCPTシナリオに留まりません。さらなる実験を通じて、提案したCPTスケーリング則がより複雑な状況にも拡張可能であることを示しました。</p>
<div class="info-grid">
<div class="info-card glass-card">
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-primary); display:flex; align-items:center;"><i class="fas fa-globe-americas" style="margin-right:8px;"></i> アウトオブドメインデータセット (Out-of-Domain Datasets)</h4>
<p>CPTの学習に直接使用されていない、全く新しいドメインのデータセット（OODデータセット）に対しても、本法則（を応用した手法）が性能予測に役立つことを確認しました。これは、一般ドメイン（<span class="badge blue">D_pt</span>）と下流ドメイン（<span class="badge orange">D_cpt</span>）の損失の線形結合としてOODデータセットの損失をモデル化することで可能になります。</p>
<div style="text-align: center; margin-top: 15px;">
<span class="badge yellow">L_ood</span> ≈ <span class="badge" style="background-color:#AED6F1; color:var(--color-dark)">λ1</span> * <span class="badge blue">L_Dpt</span> + <span class="badge" style="background-color:#FADBD8; color:var(--color-dark)">λ2</span> * <span class="badge orange">L_Dcpt</span>
</div>
</div>
<div class="info-card glass-card">
<h4 style="font-family: 'Yomogi', cursive; color: var(--color-primary); display:flex; align-items:center;"><i class="fas fa-question-circle" style="margin-right:8px;"></i> 未知情報を持つモデル (Models with Unknown Information)</h4>
<p>オープンソースなどで公開されている事前学習済みモデルを利用する場合、そのモデルの正確な学習履歴（元の学習データ<span class="badge blue">D_pt</span>の詳細、学習ステップ数、最終学習率など）が不明なことがあります。このような<strong class="highlight">情報が不完全なケース</strong>でも、我々のスケーリング則を適用し、CPTの学習ダイナミクスを予測できることを示しました。これは、未知の情報をスケーリング則のパラメータとして推定することで実現します。</p>
</div>
</div>
<div class="arrow-connector"></div>
<h3 class="subsection-title"><i class="fas fa-rocket"></i> 🚀 将来への期待</h3>
<div class="bubble-box" style="border-color: var(--color-accent1);">
<p><i class="fas fa-lightbulb" style="color: var(--color-accent3);"></i> 私たちは、本研究で提案した<strong class="keyword">CPTスケーリング則</strong>が、LLMの継続的事前学習とスケーリング則に関する研究者コミュニティの<strong class="highlight">理解を再構築する</strong>上で有望な一歩であると確信しています。</p>
<p>この法則は、CPTプロセスの複雑な振る舞いをより定量的に、かつ予測可能な形で捉えるための強力なツールとなります。今後の研究でさらに洗練され、多様なモデルアーキテクチャや学習データ、タスクへと適用範囲が広がることで、より効率的で効果的なLLMの開発と応用に貢献できるものと期待されます。</p>
<div style="text-align:center; margin-top:15px;">
<span style="font-size:2em; color:var(--color-primary); font-family: 'Yomogi', cursive;">理解の深化 <i class="fas fa-long-arrow-alt-right" style="color:var(--color-gray);"></i> 効率的な開発 <i class="fas fa-long-arrow-alt-right" style="color:var(--color-gray);"></i> LLMの進化</span>
</div>
</div>
<hr style="margin-top:30px; margin-bottom:10px; border:1px dashed var(--color-primary);"/>
<p class="reference" style="text-align:center; font-style:normal;">📝 <strong>結論のポイントまとめ</strong> 📝</p>
<ul class="unstyled-list" style="padding-left: 20px; font-family: 'Zen Kurenaido', sans-serif;">
<li style="margin-bottom:8px;"><i class="fas fa-check-circle" style="color: var(--color-accent1); margin-right:5px;"></i> LLMのCPTにおける学習ダイナミクスを検証損失を通じて分析。</li>
<li style="margin-bottom:8px;"><i class="fas fa-check-circle" style="color: var(--color-accent1); margin-right:5px;"></i> 分布シフトと学習率アニーリングを統合した<strong class="keyword">CPTスケーリング則</strong>を提案。</li>
<li style="margin-bottom:8px;"><i class="fas fa-check-circle" style="color: var(--color-accent1); margin-right:5px;"></i> この法則により、任意の学習ステップでの損失予測、CPTの重要因子の理解、ハイパーパラメータ最適化が可能に。</li>
<li style="margin-bottom:8px;"><i class="fas fa-check-circle" style="color: var(--color-accent1); margin-right:5px;"></i> OODデータセットや未知情報を持つモデルなど、複雑なシナリオへの拡張性も実証。</li>
<li style="margin-bottom:8px;"><i class="fas fa-check-circle" style="color: var(--color-accent1); margin-right:5px;"></i> 本研究がLLMのCPTとスケーリング則の理解を深め、今後の研究に貢献することを期待。</li>
</ul>
</div>
<div class="section-card" id="Impact_Statement">
<h2 class="section-title"><i class="fas fa-bullhorn"></i> Impact Statement</h2>
<div class="bubble-box" style="margin-top: 0; margin-bottom: 25px;">
<p style="font-family: 'Yomogi', cursive; font-size: 1.1em;">
<i class="fas fa-info-circle" style="color: var(--color-primary); margin-right: 5px;"></i>
            このセクションでは、本論文で提案された「<strong>継続的事前学習（CPT）のスケーリング法則</strong>」が、大規模言語モデル（LLM）の分野にどのような<strong>インパクト</strong>を与えるのか、その<strong>重要性</strong>と<strong>貢献</strong>について解説します。特に、この法則がCPTプロセスの理解を深め、実用的な応用を可能にすることに焦点を当てています。
        </p>
</div>
<h3 class="subsection-title"><i class="fas fa-layer-group"></i> CPTの重要性と本研究の貢献</h3>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(320px, 1fr)); gap: 25px;">
<div class="info-card glass-card">
<div style="text-align: center; margin-bottom: 10px;">
<span style="font-size: 3em; color: var(--color-accent1);">🔄</span>
</div>
<p><span class="keyword">継続的事前学習（CPT）</span>は、非常に強力な基盤となる大規模言語モデル（LLM）を、特定の専門分野（例：コーディング <i class="fas fa-code"></i>, 金融 <i class="fas fa-landmark"></i>, 数学 <i class="fas fa-calculator"></i> など）や特定のタスクに合わせてさらに強化するための、非常に効果的な手法です。✏️</p>
<div style="text-align: center; margin-top: 15px; padding: 10px; background-color: rgba(74, 111, 165, 0.05); border-radius: 8px;">
<span style="font-size: 2.5em; display: inline-block; transform: rotate(-5deg);">🏛️</span>
<span style="font-family: 'Yomogi', cursive; font-size: 2em; color: var(--color-primary); margin: 0 10px; display: inline-block; animation: bounce 2s infinite;">➔</span>
<div>
<span style="font-size: 2.2em; margin: 0 5px; display: inline-block; transform: rotate(3deg);">💻</span>
<span style="font-size: 2.2em; margin: 0 5px; display: inline-block; transform: rotate(-2deg);">🏦</span>
<span style="font-size: 2.2em; margin: 0 5px; display: inline-block; transform: rotate(4deg);">➕</span>
</div>
<p style="font-family: 'Yomogi', cursive; font-size: 0.9em; color: var(--color-gray); margin-top: 5px;">基盤モデルを専門分野へ特化！</p>
</div>
</div>
<div class="info-card glass-card">
<div style="text-align: center; margin-bottom: 10px;">
<span style="font-size: 3em; color: var(--color-accent2);">📈</span>
</div>
<p>本研究の核心的な貢献は、このCPTプロセスにおける<strong style="color: var(--color-secondary);">学習のダイナミクス</strong>（つまり、学習が時間とともにどのように進展し、モデルの性能がどう変化していくか）を<strong style="color: var(--color-accent1);">定量的に記述するためのスケーリング法則</strong>を提案した点にあります。📊</p>
<p>この法則を用いることで、CPTにおける様々な重要な要素（例：学習率、学習ステップ、データの種類など）が学習プロセスにどのように影響し合うのか、その複雑な関係性を明らかにすることができます。</p>
</div>
</div>
<div class="arrow-connector" style="margin: 30px 0; height: 40px;">
<style>
            .arrow-connector::after { animation: bounceArrow 1.5s infinite; }
            @keyframes bounceArrow { 0%, 20%, 50%, 80%, 100% { transform: translateY(0) translateX(-50%); } 40% { transform: translateY(-10px) translateX(-50%); } 60% { transform: translateY(-5px) translateX(-50%); } }
        </style>
</div>
<div class="framework-box" style="border: 2px dashed var(--color-primary); padding: 25px;">
<div class="framework-title" style="font-size: 1.2em; display: flex; align-items: center; justify-content: center; gap: 10px;">
<i class="fas fa-balance-scale-right fa-2x"></i> <span>スケーリング法則による高度なバランス調整と最適化</span>
</div>
<p style="text-align: center; margin-top: 15px;">
            提案されたスケーリング法則の強力な点の1つは、LLMが元々持っている「汎用的な知識・性能」と、CPTを通じて獲得しようとする「専門分野に特化した知識・性能」との間で、<strong class="highlight">最適なパフォーマンスバランス</strong>を見つけ出すことを可能にする点です。🎯
        </p>
<div style="display: flex; justify-content: space-around; align-items: center; margin: 20px 0; padding: 15px; background-color: #f9f9f9; border-radius: 8px;">
<div style="text-align: center;">
<span style="font-size: 2em;">🌍</span>
<p style="font-family: 'Yomogi', cursive; margin-top: 5px;">汎用性能</p>
</div>
<div style="font-size: 3em; color: var(--color-primary);">⚖️</div>
<div style="text-align: center;">
<span style="font-size: 2em;">🎯</span>
<p style="font-family: 'Yomogi', cursive; margin-top: 5px;">専門性能</p>
</div>
</div>
<p style="text-align: center;">具体的には、学習率のスケジュール、総学習ステップ数といった重要な<strong class="highlight">トレーニング・ハイパーパラメータを最適化</strong>するための定量的な指針を与えてくれます。これにより、無駄な試行錯誤を減らし、より効率的かつ効果的なCPTの実現が期待できます。</p>
</div>
<h3 class="subsection-title" style="margin-top: 30px;"><i class="fas fa-lightbulb"></i> スケーリング法則がもたらす具体的なインパクト</h3>
<p>著者らは、CPTという手法自体が一般にもたらす影響は多岐にわたると認識しつつも、この論文では特に<strong class="keyword">本研究で提案したスケーリング法則を用いることによって生まれるインパクト</strong>に焦点を当てて論じています。この法則は、CPTプロセスのより深い理解と、その挙動の「説明」に大きく貢献します。🔍</p>
<div class="feature-card-grid" style="gap: 25px; margin-top: 20px;">
<div class="feature-item glass-card" style="border-left: 5px solid var(--color-accent1);">
<div class="icon-item" style="margin-bottom: 15px;"><i class="fas fa-chart-line fa-3x" style="color: var(--color-accent1);"></i></div>
<h4 style="font-family: 'Kaisei Decol', serif; color: var(--color-accent1);">損失曲線のダイナミクス予測</h4>
<p>この法則を用いると、CPT中の学習がどのように進み、モデルの損失（誤り度合い）が各ステップでどのように変化するかを<strong class="highlight">予測</strong>できるようになります。これにより、事前に学習計画の妥当性を評価したり、期待される性能到達までのコストを見積もったりすることが可能になります。</p>
</div>
<div class="feature-item glass-card" style="border-left: 5px solid var(--color-secondary);">
<div class="icon-item" style="margin-bottom: 15px;"><i class="fas fa-cogs fa-3x" style="color: var(--color-secondary);"></i></div>
<h4 style="font-family: 'Kaisei Decol', serif; color: var(--color-secondary);">ハイパーパラメータの最適化</h4>
<p>学習率、学習ステップ数、データの再利用比率（リプレイ率）といったCPTの性能を大きく左右するハイパーパラメータを、特定の目的（例：汎用性能を維持しつつ専門性能を最大化する）に合わせて<strong class="highlight">最適化</strong>するための定量的な根拠を提供します。これにより、計算資源の無駄遣いを防ぎ、最良のモデルを効率的に獲得する手助けとなります。</p>
</div>
</div>
<div class="note-box" style="margin-top: 25px; border-left-color: var(--color-accent2);">
<div class="note-title" style="color: var(--color-accent2);"><i class="fas fa-check-circle"></i> 実用上の利点まとめ</div>
<p>これらの具体的な利点により、研究者や開発者は、継続的事前学習（CPT）を<strong style="color: var(--color-primary);">より戦略的</strong>に、そして<strong style="color: var(--color-accent1);">より効率的</strong>に行うことが可能になります。これは、LLMを多様な応用分野で活用する上で非常に重要な進歩と言えるでしょう。✨</p>
</div>
<h3 class="subsection-title" style="margin-top: 30px;"><i class="fas fa-rocket"></i> 研究の目標と社会への影響</h3>
<div class="two-column" style="gap: 25px;">
<div class="column">
<div class="content-box glass-card" style="padding: 20px; border-top: 5px solid var(--color-accent2);">
<p style="display: flex; align-items: center; font-family: 'Yomogi', cursive; font-size: 1.1em; color: var(--color-accent2); margin-bottom: 10px;">
<i class="fas fa-flag-checkered fa-fw" style="margin-right: 8px;"></i> 研究の主要目標
                </p>
<p>この論文が提示する研究の主な<strong class="keyword">目標</strong>は、<strong style="color: var(--color-accent2);">大規模言語モデル（LLM）という学術・技術分野全体の進歩</strong>に貢献することです。📈</p>
<p>CPTの学習ダイナミクスに関する理解を深めることは、LLMの応用範囲をさらに広げ、その潜在的な能力をより効果的に引き出す上で不可欠なステップです。</p>
</div>
</div>
<div class="column">
<div class="content-box glass-card" style="padding: 20px; border-top: 5px solid var(--color-gray);">
<p style="display: flex; align-items: center; font-family: 'Yomogi', cursive; font-size: 1.1em; color: var(--color-gray); margin-bottom: 10px;">
<i class="fas fa-globe-americas fa-fw" style="margin-right: 8px;"></i> 社会的影響への言及
                </p>
<p>潜在的な社会的影響については、著者らは「我々の研究には多くの潜在的な社会的影響（consequences）が考えられるものの、<strong class="highlight">この論文の文脈において現時点で特に強調すべき（highlighted）ものはない</strong>と感じている」との見解を示しています。📝</p>
<p>これは、LLM技術の急速な進展に伴う倫理的・社会的な側面への配慮を示唆しつつも、本論文の主眼はあくまでCPTの学習ダイナミクスに関する技術的・科学的貢献にあることを明確にしています。</p>
</div>
</div>
</div>
<div style="text-align: center; margin-top: 25px; padding: 15px; background-color: rgba(255, 248, 225, 0.7); border-radius: 8px; border: 1px dashed var(--color-accent3);">
<i class="fas fa-microscope" style="font-size: 1.8em; color: var(--color-primary); margin-right: 8px; vertical-align: middle;"></i>
<span style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-dark);">本論文は、LLMの継続学習における基礎的な理解を深める学術的貢献を重視</span>
<i class="fas fa-tools" style="font-size: 1.8em; color: var(--color-primary); margin-left: 8px; vertical-align: middle;"></i>
</div>
</div>
<div class="section-card" id="A._Related_Work">
<h2 class="section-title"><i class="fas fa-book-open"></i>A. Related Work</h2>
<p style="text-align: center; font-family: 'Yomogi', cursive; font-size: 16px; color: var(--color-gray);">
        このセクションでは、本研究の位置づけを明確にするために、関連する研究分野を掘り下げていきます。主に、<span class="highlight">継続的事前学習（CPT）</span>、<span class="highlight">スケーリング則</span>、そして<span class="highlight">ハイパーパラメータ最適化</span>という3つの重要な領域に焦点を当て、それぞれの分野における既存の研究と本研究の貢献を比較検討します。これにより、本研究がどのような新規性や進展をもたらすのかを明らかにします。
    </p>
<div class="info-grid" style="grid-template-columns: 1fr; gap: 20px;">
<div class="info-card">
<h3 class="subsection-title"><i class="fas fa-sync-alt"></i>Continual Pre-Training (CPT)</h3>
<div class="content-box">
<p><span class="keyword">継続的事前学習（Continual Pre-Training, CPT）</span>は、大規模言語モデル（LLM）を新しいドメインに継続的に事前学習させることを目的としています。これにより、特定の分野（例えば、<span class="badge blue">コード</span>、<span class="badge green">医療</span>、<span class="badge purple">法律</span>など）に適応させることができます。</p>
<div class="definition-box" style="margin-top: 15px; margin-bottom: 20px;">
<div class="definition-title"><i class="fas fa-microscope"></i>CPTの核心</div>
<ul class="unstyled-list">
<li>✏️ <strong>目的1: 新ドメインへの適応</strong>
<ul>
<li>Hui et al. (2024), DeepSeek-AI et al. (2024) (コード)</li>
<li>Chen et al. (2023b) (医療)</li>
<li>Colombo et al. (2024) (法律)</li>
</ul>
</li>
<li>✏️ <strong>目的2: ドメイン特化LLMのゼロからの訓練回避</strong> (Shi et al., 2024)</li>
<li>🎯 <strong>目標:</strong> 下流タスクの性能向上 ＋ <span class="highlight">破滅的忘却</span>の回避</li>
</ul>
</div>
<div class="bubble-box" style="margin-top:25px; margin-bottom:25px;">
<p style="font-family: 'Yomogi', cursive; font-size: 15px;">
<i class="fas fa-exclamation-triangle" style="color: var(--color-secondary);"></i> <strong>破滅的忘却 (Catastrophic Forgetting) とは？</strong><br/>
                        これは、ニューラルネットワークが新しいタスクを学習する際に、以前に学習したタスクの知識を急速に失ってしまう現象を指します (French, 1999; Lange et al., 2023; Gupta et al., 2023; Ibrahim et al., 2024)。CPTの大きな課題の一つです。
                    </p>
</div>
<p>既存のCPT手法の多くは、主に以下のいずれかの戦略を採用しています：</p>
<div class="two-column" style="margin-top: 15px; margin-bottom: 20px;">
<div class="column">
<div class="feature-item" style="background-color: rgba(74, 111, 165, 0.05); padding: 15px; border-radius: 8px;">
<i class="fas fa-history fa-2x" style="color: var(--color-primary); margin-bottom:10px;"></i>
<h4 style="font-family: 'Kaisei Decol', serif; color: var(--color-primary);">データリプレイ戦略</h4>
<p>適切な事前学習データを混合して再学習させることで、新旧の知識のバランスを取ります (Que et al., 2024; Gu et al., 2024)。</p>
<p style="font-size:12px; color: var(--color-gray);">例：古いデータセットと新しいデータセットを混ぜて学習。</p>
</div>
</div>
<div class="column">
<div class="feature-item" style="background-color: rgba(255, 126, 95, 0.05); padding: 15px; border-radius: 8px;">
<i class="fas fa-puzzle-piece fa-2x" style="color: var(--color-secondary); margin-bottom:10px;"></i>
<h4 style="font-family: 'Kaisei Decol', serif; color: var(--color-secondary);">パラメータ追加戦略</h4>
<p>新しいドメイン知識を吸収するために、モデルに追加のパラメータを導入します (Wang et al., 2022)。</p>
<p style="font-size:12px; color: var(--color-gray);">例：新しいドメイン専用の小さなモジュールを追加。</p>
</div>
</div>
</div>
<div class="framework-box" style="margin-top: 20px;">
<div class="framework-title"><i class="fas fa-lightbulb"></i>本研究の貢献 (CPTにおいて)</div>
<p>私たちの研究は、CPTの<span class="keyword">学習ダイナミクス</span>を包括的に調査し、一般ドメインと下流タスク（ドメイン特化タスク）の検証損失を記述するための<span class="keyword">CPTスケーリング則</span>を提案します。これにより、CPTプロセス中の性能変化をより深く理解し、予測することを目指します。</p>
</div>
</div>
</div>
<div class="info-card">
<h3 class="subsection-title"><i class="fas fa-chart-line"></i>Scaling Laws (スケーリング則)</h3>
<div class="content-box">
<p><span class="keyword">スケーリング則</span>は、モデルの性能がモデルサイズ、データセットサイズ、計算リソースなどの要因によってどのように変化するかを記述する法則です。</p>
<div class="note-box" style="margin-top: 15px; margin-bottom: 20px;">
<div class="note-title"><i class="fas fa-balance-scale"></i>代表的なスケーリング則研究</div>
<ul class="unstyled-list">
<li><span class="badge yellow">Kaplan et al. (2020)</span>: 検証損失 \(L\) と3つの要因（モデルサイズ \(N\)、データセットサイズ \(D\)、訓練計算量）の間に<span class="highlight">べき乗則関係</span>を経験的に発見。
                            <div style="text-align: center; margin-top:5px; font-family: 'Kaisei Decol', serif;">
                                \(L \propto \text{Factors}^{-\alpha}\)
                            </div>
</li>
<li><span class="badge yellow">Hoffmann et al. (2022) (Chinchilla)</span>: モデルサイズとデータセットサイズのバランスを取ることで計算効率が最適なLLM（Chinchilla）を開発。</li>
<li><span class="badge yellow">Tissue et al. (2024)</span>: <span class="highlight">学習率アニーリング</span>（学習率を徐々に減少させる手法）が学習ダイナミクスに与える影響を記述するスケーリング則を導入。これにより、様々な学習率スケジュール（LRS）下での任意の訓練ステップにおける損失予測が可能に。</li>
</ul>
</div>
<p class="challenge-box" style="padding: 10px; margin-bottom: 20px;">
<span class="challenge-title"><i class="fas fa-exclamation-circle"></i>既存スケーリング則の限界:</span> これらのスケーリング則は、主に<span class="highlight">事前学習シナリオに限定</span>されており、訓練データセットが途中で変化するようなCPTの状況には直接適用できません。
                </p>
<p>CPTにおけるスケーリング則に関する研究も進められています：</p>
<div class="feature-card-grid" style="grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));">
<div class="feature-item" style="padding:10px;">
<h4 style="font-family: 'Yomogi', cursive; font-size:15px; color: var(--color-accent1);">Hernandez et al. (2021b)</h4>
<p style="font-size:13px;">モデルサイズとCPTデータにおける<span class="keyword">転移学習のスケーリング則</span>を研究。</p>
</div>
<div class="feature-item" style="padding:10px;">
<h4 style="font-family: 'Yomogi', cursive; font-size:15px; color: var(--color-accent1);">Barnett (2024)</h4>
<p style="font-size:13px;">データセット間の分布差を示す<span class="keyword">転移ギャップ項</span>を組み込んだ経験的スケーリング則を提案。</p>
</div>
<div class="feature-item" style="padding:10px;">
<h4 style="font-family: 'Yomogi', cursive; font-size:15px; color: var(--color-accent1);">D-CPT (Que et al., 2024), CMR (Gu et al., 2024)</h4>
<p style="font-size:13px;"><span class="keyword">データリプレイ</span>や混合比率をCPTプロセスに導入したスケーリング則。</p>
</div>
<div class="feature-item" style="padding:10px;">
<h4 style="font-family: 'Yomogi', cursive; font-size:15px; color: var(--color-accent1);">Dou et al. (2024)</h4>
<p style="font-size:13px;">学習率とリプレイ比率の両方を考慮に入れるための<span class="keyword">二次関数</span>を提案。</p>
</div>
</div>
<div class="framework-box" style="margin-top: 20px;">
<div class="framework-title"><i class="fas fa-lightbulb"></i>本研究の貢献 (スケーリング則において)</div>
<p>しかし、これらの既存のCPTスケーリング則は、主に<span class="highlight">最終的な損失</span>を記述するものであり、CPTに関連する全ての要因を考慮しているわけではありません。私たちの提案する<span class="keyword">CPTスケーリング則</span>は、これらの関連要因を全て統合し、CPTの<span class="highlight">各ステップにおける損失を予測</span>することができます。これにより、CPTプロセス全体の学習ダイナミクスを記述することが可能になります。</p>
<div style="display: flex; justify-content: space-around; align-items: center; margin-top: 15px; font-family: 'Yomogi', cursive;">
<div style="text-align:center; padding:10px; border: 1px dashed var(--color-gray); border-radius:8px;">
<p>既存のCPTスケーリング則</p>
<p><i class="fas fa-times-circle" style="color: var(--color-secondary);"></i> 主に最終損失</p>
<p><i class="fas fa-times-circle" style="color: var(--color-secondary);"></i> 一部の要因のみ</p>
</div>
<div style="font-size: 24px; color: var(--color-primary); padding: 0 10px;">
<i class="fas fa-arrow-right"></i>
</div>
<div style="text-align:center; padding:10px; border: 1px dashed var(--color-accent1); border-radius:8px;">
<p>本研究のCPTスケーリング則</p>
<p><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 各ステップの損失</p>
<p><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 全ての関連要因</p>
<p><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 完全な学習ダイナミクス</p>
</div>
</div>
</div>
</div>
</div>
<div class="info-card">
<h3 class="subsection-title"><i class="fas fa-cogs"></i>Hyper-Parameter Optimization (ハイパーパラメータ最適化)</h3>
<div class="content-box">
<p>機械学習において、<span class="keyword">最適なハイパーパラメータ設定</span>を見つけることは、頑健なパフォーマンスを達成するために非常に重要です (Bergstra &amp; Bengio, 2012; Snoek et al., 2012)。</p>
<div class="definition-box" style="margin-top: 15px; margin-bottom: 20px;">
<div class="definition-title"><i class="fas fa-sliders-h"></i>LLMにおける主要なハイパーパラメータ</div>
<p>大規模言語モデル（LLM）における主要なハイパーパラメータには、以下のようなものがあります (Kaplan et al., 2020; Hu et al., 2024; Xie et al., 2025)：</p>
<ul class="tag-list">
<li class="tag">ピーク学習率</li>
<li class="tag">学習率スケジュール</li>
<li class="tag">バッチサイズ</li>
<li class="tag">訓練ステップ数</li>
<li class="tag">その他多数...</li>
</ul>
</div>
<p>ハイパーパラメータ最適化へのアプローチは進化してきました：</p>
<div class="pipeline" style="margin-top:15px; margin-bottom:20px;">
<div class="pipeline-step">
<strong>初期アプローチ (モデルフリー)</strong> <i class="fas fa-search" style="margin-left: 5px;"></i><br/>
                        主に<span class="highlight">グリッドサーチ</span>や<span class="highlight">ランダムサーチ</span>のような手法が利用されていました (Bergstra &amp; Bengio, 2012)。
                        <div class="note-box" style="padding: 5px 10px; margin-top:5px; font-size:13px;">
<strong>グリッドサーチ:</strong> 事前に定義したハイパーパラメータの組み合わせを全て試す。<br/>
<strong>ランダムサーチ:</strong> 事前に定義した範囲からランダムに組み合わせを選んで試す。
                        </div>
</div>
<div class="pipeline-step">
<strong>発展的アプローチ</strong> <i class="fas fa-brain" style="margin-left: 5px;"></i><br/>
                        その後、<span class="highlight">ベイズ最適化 (Bayesian Optimization)</span> のような手法が登場し、様々なハイパーパラメータの性能を予測し、それに応じて最も効果的なものを選択する試みがなされるようになりました (Balandat et al., 2020)。
                        <div class="note-box" style="padding: 5px 10px; margin-top:5px; font-size:13px;">
<strong>ベイズ最適化:</strong> 過去の試行結果に基づいて、次に試すべき有望なハイパーパラメータを効率的に探索する。
                        </div>
</div>
</div>
<div class="framework-box" style="margin-top: 20px;">
<div class="framework-title"><i class="fas fa-lightbulb"></i>本研究の貢献 (ハイパーパラメータ最適化において)</div>
<p>私たちの研究は、提案する<span class="keyword">CPTスケーリング則</span>を活用して、大規模言語モデルの継続的事前学習におけるハイパーパラメータに焦点を当てます。私たちが最適化するハイパーパラメータには、以下のようなものが含まれます：</p>
<ul class="tag-list">
<li class="tag badge blue">学習率スケジュール</li>
<li class="tag badge green">ピーク学習率</li>
<li class="tag badge purple">リプレイ比率</li>
<li class="tag badge yellow">など...</li>
</ul>
<p>CPTスケーリング則を用いることで、これらのハイパーパラメータがCPTの学習ダイナミクスにどのように影響するかを予測し、特定の目的に合わせた最適化を目指します。</p>
</div>
</div>
</div>
</div>
</div>
<div class="section-card" id="B._Experiment_Setups">
<h2 class="section-title"><i class="fas fa-cogs"></i> B. Experiment Setups</h2>
<p style="text-align: center; font-family: 'Yomogi', cursive; color: var(--color-gray); margin-bottom: 5px;">このセクションで説明する実験設定の全体像は、以下の<span class="keyword">Table 1</span>にまとめられています。</p>
<img alt="Table 1: Experimental settings" src="table1.png"/>
<div class="caption-box" style="margin-top:0px; margin-bottom: 20px; padding:10px; background-color: rgba(240,240,240,0.5); border: 1px dashed var(--color-gray); border-radius: 8px; text-align: left;">
<p style="font-family: 'Kaisei Decol', serif; font-size: 1.1em; color: var(--color-dark); margin-bottom:5px; text-align:center;">📋 <strong>Table 1: この研究で採用された実験設定</strong></p>
<p style="font-size: 0.9em; line-height: 1.4; color: var(--color-gray);">
<i class="fas fa-info-circle" style="color:var(--color-primary); margin-right: 5px;"></i>表の説明：この表は、本研究で行われた様々な実験の具体的な設定値を示しています。<span class="highlight">モデルサイズ</span>は、埋め込み層を除いたパラメータの数を指します。最適化アルゴリズムとしては<span class="keyword">AdamW</span> (Kingma &amp; Ba, 2015; Loshchilov &amp; Hutter, 2017) を採用し、ほとんどの実験で <span class="keyword">LLaMA-3 のトークナイザ</span> (Dubey et al., 2024) が使用されています。これらの情報は、実験の再現性や他の研究との比較において非常に重要です。
        </p>
</div>
<!-- 導入 -->
<p>さて、ここからは論文で提案された新しい数式（CPTスケーリング則）が、本当にさまざまな状況でうまく機能するのかを検証するために、どのような実験設定が用いられたのかを詳しく見ていくよ！ 🧪</p>
<p>この論文の大きな目標は、大規模言語モデル (LLM) の継続的事前学習 (CPT) における学習のダイナミクスを解明し、性能の変化を予測できるような数式を導き出すことだったよね。この「Experiment Setups」セクションは、その数式の<span class="keyword">信頼性を裏付けるための実験の基盤</span>となる、とっても重要な部分なんだ。いわば、料理でいうところのレシピの材料や下ごしらえを決める部分だね！ 🍳</p>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-rocket"></i>実験の目的とアプローチ</h3>
<p>この研究では、提案した数式（スケーリング則）が<span class="highlight">特定の条件下だけでなく、色々な設定で有効であること</span>を検証するために、複数の実験セットアップが使われているんだ。一つの設定だけで上手くいっても、それが本当に普遍的に適用できる法則なのかは分からないからね。だから、実験条件を意図的に変えて、数式の<span class="keyword">頑健さ（ロバストネス）</span>をチェックすることが重要なんだ。🔬</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-question-circle"></i> なぜ複数の実験設定が重要？</p>
<p>科学的な発見や法則の信頼性を高めるためには、それが<span class="highlight">様々な条件下でも成り立つこと</span>を示す必要があるんだ。例えば、ある薬が特定の年齢層にしか効かない場合と、幅広い年齢層に効果がある場合では、その薬の価値は大きく変わるよね。この数式も同じで、多くの実験設定でその有効性が確認できれば、より信頼性が高く、応用範囲の広いものとして認められるんだ。まさに「<span class="keyword">百聞は一見に如かず、百見は一験に如かず</span>」だね！</p>
</div>
</div>
<div class="info-grid">
<div class="info-card glass-card">
<h4 class="subsection-title" style="font-size: 16px;"><i class="fas fa-star"></i> Setting A: 実験の土台</h4>
<p>実験の大部分では、<span class="badge blue">Setting A</span> という名前の基本設定が使われているよ。これは、いわば実験の「標準モデル」や「出発点」となる条件セットだね。（詳細は先ほどのTable 1を見てね！)</p>
<p>この<span class="keyword">Setting A</span>を基準にして、後で説明するような特定のパラメータ（リプレイ比率、バッチサイズ、シーケンス長など）を変更することで、数式の性能がどのように変化するかを調べているんだ。 🛠️</p>
</div>
<div class="info-card glass-card">
<h4 class="subsection-title" style="font-size: 16px;"><i class="fas fa-sliders-h"></i> パラメータ変更実験</h4>
<p>提案された数式が、訓練のハイパーパラメータの変更に対しても頑健であるかを確認するために、以下の要素を変更した実験が行われたよ：</p>
<ul class="unstyled-list">
<li><i class="fas fa-redo" style="color:var(--color-accent1);"></i> <strong class="keyword">リプレイ比率 (Replay Ratio)</strong>: CPT中に、元の事前学習データ (D<sub>pt</sub>) をどれくらいの割合で混ぜるか。</li>
<li><i class="fas fa-box" style="color:var(--color-accent2);"></i> <strong class="keyword">バッチサイズ (Batch Size)</strong>: 一度のパラメータ更新で処理するデータサンプルの数。</li>
<li><i class="fas fa-text-height" style="color:var(--color-accent3);"></i> <strong class="keyword">シーケンス長 (Sequence Length)</strong>: モデルが一度に入力として受け取るトークンの数。</li>
</ul>
<p>これらの変更は、<span class="highlight">Setting A の設定を直接変更する</span>ことで行われたんだ。つまり、他の条件はできるだけ同じにして、特定のパラメータの影響だけを分離して評価しようとしているんだね。🔍</p>
</div>
</div>
<div class="bubble-box">
<p><i class="fas fa-lightbulb" style="color: var(--color-primary); margin-right: 5px;"></i><strong>ポイント！</strong></p>
<p>このセクションで重要なのは、論文の主張の<span class="keyword">実証的な裏付け</span>を行うための実験計画を明確にしている点だよ。どんな条件で実験が行われ、どんなデータに基づいて結論が導かれたのかを理解することで、論文全体の信頼性や意義を評価することができるんだ。次のセクション以降では、これらの実験設定のもとで得られた結果や、そこから導かれる考察が詳しく述べられることになるよ。だから、ここで「どんな土台で実験が行われたのか」をしっかり把握しておくことが大切なんだ！ 📌</p>
</div>
<div class="summary-box" style="margin-top:20px; padding:15px; background-color: rgba(255,126,95,0.05); border-left: 5px solid var(--color-secondary); border-radius: 8px;">
<h3 class="subsection-title" style="color: var(--color-secondary); border-bottom: none; padding-left:0; margin-top:0;"><i class="fas fa-clipboard-check"></i> このセクションのまとめ</h3>
<ul style="list-style-type: none; padding-left: 10px;">
<li style="margin-bottom: 10px;">📝 提案された数式の有効性を検証するため、複数の実験設定を使用。</li>
<li style="margin-bottom: 10px;">📊 全ての実験設定の概要は <span class="keyword">Table 1</span> に集約。</li>
<li style="margin-bottom: 10px;">🌟 大部分の実験は <span class="badge blue">Setting A</span> を基準として実施。</li>
<li style="margin-bottom: 10px;">🔄 リプレイ比率、バッチサイズ、シーケンス長などのパラメータを変更した実験は、Setting A を直接修正して実施。</li>
</ul>
<p style="font-family: 'Yomogi', cursive; text-align: center; margin-top: 15px; color: var(--color-dark);">
            これらの実験設定を通じて、提案手法の汎用性と堅牢性が検証されるんだね！💪
        </p>
</div>
</div>
<div class="section-card" id="C._Fitting_Details">
<h2 class="section-title"><i class="fas fa-cogs"></i> C. Fitting Details</h2>
<p>このセクションでは、論文で提案されたCPTスケーリング法則（特に式4で示されるモデル）のパラメータを、実際の観測データにどのように適合させるか（フィッティングするか）という、具体的な<span class="keyword">技術的詳細</span>について解説します。モデルが観測データをうまく説明できるようにパラメータを調整するこのプロセスは、スケーリング法則の予測精度や信頼性を左右する非常に重要なステップです。 📝</p>
<div class="bubble-box" style="margin-bottom: 25px;">
<p><i class="fas fa-bullseye"></i> <strong>このセクションのゴール</strong></p>
<p>提案されたCPTスケーリング法則の数式に含まれる未知のパラメータ ($L_0, A, C_1, C_2, B, E, \alpha, \beta$ など) の具体的な値を、実験データから推定する方法を明らかにします。</p>
</div>
<p>フィッティングのプロセスは、大きく分けて以下のステップで進められます。</p>
<div class="pipeline">
<div class="pipeline-step">
<span class="step-number">1</span>
<div class="step-content">
<h4><i class="fas fa-calculator"></i> 学習率関連項の事前計算 🧮</h4>
<p>PT（事前学習）とCPT（継続的事前学習）で使用した学習率スケジュール(LRS)から、スケーリング法則に必要な項である <span class="keyword">$S_{1}^{pt}, S_{2}^{pt}, S_{1}^{cpt}, S_{2}^{cpt}$</span> をあらかじめ計算しておきます。</p>
</div>
</div>
<div class="pipeline-step">
<span class="step-number">2</span>
<div class="step-content">
<h4><i class="fas fa-balance-scale"></i> 損失関数の選定と最小化 ⚖️</h4>
<p>モデルによる予測損失と実際に観測された損失との間の「ずれ」を定量化する損失関数として <span class="keyword">Huber損失</span> を採用し、この損失が最小になるようにパラメータを調整します。</p>
</div>
</div>
<div class="pipeline-step">
<span class="step-number">3</span>
<div class="step-content">
<h4><i class="fas fa-chart-line"></i> 最適化アルゴリズムの適用 ⚙️</h4>
<p>Huber損失を最小化するための具体的な最適化手法として <span class="keyword">L-BFGSアルゴリズム</span> を用います。この処理は <code>scipy</code> ライブラリの <code>minimize</code> 関数を使って実装されます。</p>
</div>
</div>
<div class="pipeline-step">
<span class="step-number">4</span>
<div class="step-content">
<h4><i class="fas fa-lightbulb"></i> 局所最適解への対策 🧭</h4>
<p>最適化計算が不適切な解（局所最適解）に陥るのを防ぐため、複数の異なる初期条件からフィッティングを開始し、その中で最も良い結果（損失が最小となるフィット）を選び出します。</p>
</div>
</div>
</div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-pencil-ruler"></i> 1. 学習率関連項の事前計算</h3>
<p>論文で提案されているCPTスケーリング法則（式4）は、学習の進行に伴う損失の変化をモデル化するものです。この法則には、学習率スケジュール(LRS)から導出されるいくつかの重要な構成要素が含まれています。これらは、事前学習(PT)フェーズと継続的事前学習(CPT)フェーズのそれぞれについて、フィッティングプロセスに先立って計算されます。</p>
<div class="info-grid">
<div class="info-card glass-card">
<div class="definition-box">
<p class="definition-title"><i class="fas fa-square-root-alt"></i> $S_1$: 順方向面積 (Forward Area / Summed LR)</p>
<p>この項は、学習プロセスのある時点 $t$ までの<span class="highlight">学習率 ($\eta_i$) の単純な累積和</span>を表します。直感的には、モデルがそれまでに「経験した」総学習量や、学習の進行度合いを示す指標と解釈できます。式(1)の文脈では、この $S_1$ が大きいほど損失が小さくなる傾向（$S_1^{-\alpha}$ の項）を示唆しています。</p>
<div class="formula">
                        $S_1 = \sum_{i=1}^{t} \eta_i$
                    </div>
<p>ここで、</p>
<ul>
<li>$\eta_i$: $i$番目の学習ステップにおける学習率。</li>
<li>$t$: 現在の学習ステップ数。</li>
</ul>
<p>論文では、PTフェーズとCPTフェーズそれぞれに対応する$S_1$が登場します:</p>
<ul>
<li><span class="badge blue">$S_{1}^{pt}$</span>: 事前学習フェーズにおける順方向面積。</li>
<li><span class="badge orange">$S_{1}^{cpt}$</span>: 継続的事前学習フェーズにおける順方向面積。</li>
</ul>
</div>
</div>
<div class="info-card glass-card">
<div class="definition-box">
<p class="definition-title"><i class="fas fa-wave-square"></i> $S_2$: アニーリング面積 (Annealing Area)</p>
<p>この項は、学習率の<span class="highlight">アニーリング（徐々に学習率を減少させる戦略）による効果</span>を定量化するものです。特に、学習率の変化 $(\eta_{k-1} - \eta_k)$ が、過去のステップに遡ってどのように影響を及ぼすかを、モーメンタムや割引率のような役割を果たすハイパーパラメータ $\lambda$ を用いて重み付けして集計します。$S_2$ が大きいほど、アニーリングによる損失低減効果が大きいことを意味します（式(1)の $-C \cdot S_2$ の項）。</p>
<div class="formula">
                        $S_2 = \sum_{i=1}^{t} \sum_{k=1}^{i} (\eta_{k-1} - \eta_k) \cdot \lambda^{i-k}$
                    </div>
<p>ここで、</p>
<ul>
<li>$\eta_k$: $k$番目の学習ステップにおける学習率。（$\eta_0$ は通常、$\eta_1$ と同じか、学習開始前の学習率として定義されます。）</li>
<li>$\lambda$: 学習率変化の履歴効果の持続性を制御するハイパーパラメータ。（0 &lt; $\lambda$ ≤ 1）</li>
<li>$t$: 現在の学習ステップ数。</li>
</ul>
<p>論文では、PTフェーズとCPTフェーズそれぞれに対応する$S_2$が登場します:</p>
<ul>
<li><span class="badge blue">$S_{2}^{pt}$</span>: 事前学習フェーズにおけるアニーリング面積。</li>
<li><span class="badge orange">$S_{2}^{cpt}$</span>: 継続的事前学習フェーズにおけるアニーリング面積。</li>
</ul>
</div>
</div>
</div>
<div class="note-box" style="margin-top: 15px;">
<p class="note-title"><i class="fas fa-exclamation-circle"></i> 重要ポイント</p>
<p>これらの $S_1$ と $S_2$ の値は、実際の学習ログ（各ステップでの学習率）があれば、<span class="keyword">事前に完全に計算可能</span>です。これにより、フィッティングプロセスではこれらの値を既知の入力として扱い、他のパラメータ ($L_0, A, C_1, C_2, B, E, \alpha, \beta$) の推定に集中できます。</p>
</div>
</div>
<div class="arrow-connector"></div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-balance-scale"></i> 2. 損失関数の選定と最小化</h3>
<p>フィッティングの目標は、モデルが予測する損失値と、実際に観測された損失値との間の「誤差」または「食い違い」を最小にするようなパラメータを見つけることです。この「誤差」を測る尺度として、論文では <span class="keyword">Huber損失 (Huber loss)</span> を採用しています。</p>
<div class="info-grid">
<div class="info-card glass-card">
<div class="definition-box">
<p class="definition-title"><i class="fas fa-ruler-combined"></i> Huber損失 (Huber, 1964)</p>
<p>Huber損失は、予測値と実測値の差が小さい場合は二乗誤差のように振る舞い（L2損失）、差が大きい場合は絶対誤差のように振る舞う（L1損失）という性質を持つ損失関数です。これにより、<span class="highlight">外れ値に対してロバスト（頑健）である</span>という利点があります。つまり、いくつかのデータ点が大きく予測から外れていても、それらに過度に影響されにくいのです。</p>
<p>具体的には、予測された対数損失（log loss）と観測された対数損失の間でHuber損失を計算します。</p>
<div class="formula">
                    $L_{\delta}(y, f(x)) = \begin{cases} \frac{1}{2}(y - f(x))^2 &amp; \text{for } |y - f(x)| \le \delta \\ \delta (|y - f(x)| - \frac{1}{2}\delta) &amp; \text{otherwise} \end{cases}$
                    </div>
<p>ここで、</p>
<ul>
<li>$y$: 観測された対数損失。</li>
<li>$f(x)$: モデル（式4）によって予測された対数損失。</li>
<li>$\delta$: Huber損失が二乗誤差から絶対誤差に切り替わる閾値。この$\delta$の調整もフィッティングの一部となることがあります。</li>
</ul>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> なぜHuber損失なのか？</p>
<p>Chinchillaスケーリング法則 (Hoffmann et al., 2022) の研究でも同様の手法が採用されており、大規模言語モデルの損失曲線のフィッティングにおいて実績のある方法です。損失データには時折ノイズや予期せぬ変動が含まれることがあるため、Huber損失のロバスト性が有効に機能します。</p>
</div>
</div>
<div class="info-card glass-card" style="text-align: center;">
<p style="font-family: 'Yomogi', cursive; font-size: 1.2em; color: var(--color-primary); margin-bottom: 10px;">Huber損失の挙動 📈</p>
<img alt="Huber損失のグラフ" src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Huber_loss.svg/600px-Huber_loss.svg.png" style="width: 80%; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); margin-top:10px;"/>
<p style="font-size: 0.9em; color: var(--color-gray); margin-top: 10px;">誤差が小さい領域では滑らかな二次関数（青）、大きい領域では線形関数（緑）として振る舞います。これにより、外れ値の影響を緩和します。</p>
<p class="reference">出典: Wikipedia</p>
</div>
</div>
<p>フィッティングの目的は、全てのデータ点におけるHuber損失の合計（または平均）を最小化するようなパラメータ群 $(L_0, A, C_1, C_2, B, E, \alpha, \beta)$ を見つけ出すことです。</p>
</div>
<div class="arrow-connector"></div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> 3. 最適化アルゴリズムの適用</h3>
<p>Huber損失を最小化するパラメータを見つけるために、<span class="keyword">L-BFGSアルゴリズム (Nocedal, 1980)</span> を使用します。L-BFGSは、大規模な最適化問題に対して効率的で広く使われている準ニュートン法の一種です。</p>
<div class="feature-card-grid">
<div class="feature-item glass-card">
<i class="fas fa-robot fa-2x" style="color:var(--color-accent1)"></i>
<p class="definition-title" style="margin-bottom: 5px; font-size:1.1em; color: var(--color-accent1)">L-BFGSアルゴリズム</p>
<p style="font-size:0.9em">Limited-memory Broyden–Fletcher–Goldfarb–Shanno algorithmの略。BFGSアルゴリズムのメモリ使用量を制限したバージョンで、大規模問題に適しています。関数の勾配情報（一階微分）を利用して、効率的に最小値を探します。</p>
</div>
<div class="feature-item glass-card">
<i class="fas fa-laptop-code fa-2x" style="color:var(--color-accent2)"></i>
<p class="definition-title" style="margin-bottom: 5px; font-size:1.1em; color: var(--color-accent2)"><code>scipy.optimize.minimize</code></p>
<p style="font-size:0.9em">Pythonの科学計算ライブラリ <code>SciPy</code> に含まれる汎用的な最適化関数です。様々なアルゴリズム（L-BFGS-Bを含む）を指定して、目的関数を最小化する変数の値を求めることができます。</p>
</div>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-code-branch"></i> 実装の流れ</p>
<ol>
<li><strong>目的関数の定義:</strong> 予測損失と観測損失からHuber損失を計算する関数を定義します。この関数は、推定したいパラメータ群を入力として受け取ります。</li>
<li><strong>勾配の計算 (オプション):</strong> L-BFGSは勾配情報を利用するため、目的関数の勾配を解析的に計算できる場合はそれを提供すると効率が上がります。提供しない場合は、数値的に近似されます。</li>
<li><strong><code>minimize</code>関数の呼び出し:</strong>
<ul>
<li>目的関数</li>
<li>パラメータの初期値</li>
<li>使用するメソッド (<code>'L-BFGS-B'</code>など)</li>
<li>必要に応じてパラメータの制約条件（例：A &gt; 0）や勾配関数</li>
</ul>
                    を指定して <code>minimize</code> 関数を呼び出します。
                </li>
<li><strong>結果の取得:</strong> 最適化が成功すると、損失を最小化するパラメータの値と、その時の最小損失値が得られます。</li>
</ol>
</div>
</div>
<div class="arrow-connector"></div>
<div class="content-box">
<h3 class="subsection-title"><i class="fas fa-lightbulb"></i> 4. 局所最適解への対策</h3>
<p>L-BFGSのような勾配ベースの最適化アルゴリズムは、<span class="keyword">局所最適解 (local minima)</span> に収束する可能性があります。これは、真の全体での最適解（大域的最適解, global minimum）ではないものの、その近傍では最も損失が小さい点のことです。CPTスケーリング法則のような複雑なモデルでは、損失関数が複数の谷（局所最適解）を持つことがあり得ます。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-exclamation-triangle"></i> 局所最適解の問題点</p>
<p>もし最適化アルゴリズムが局所最適解にトラップされると、得られるパラメータはモデルの真の性能を反映しない可能性があり、スケーリング法則の予測精度が低下する原因となります。</p>
</div>
<p>この問題に対処するため、論文では以下の戦略を採用しています:</p>
<div class="bubble-box">
<p><i class="fas fa-random"></i> <strong>複数の初期条件からのフィッティング実行</strong></p>
<p>パラメータの<span class="highlight">初期値をランダムに変えて</span>、L-BFGSによるフィッティングを複数回実行します。初期値が異なれば、アルゴリズムが探索する経路も変わり、異なる局所最適解（あるいは大域的最適解）にたどり着く可能性があります。</p>
<p><i class="fas fa-trophy"></i> <strong>最適なフィットの選択</strong></p>
<p>複数回のフィッティング試行の結果から、<span class="highlight">最もHuber損失が小さくなった（つまり、観測データとの当てはまりが最も良かった）パラメータセット</span>を最終的なフィッティング結果として採用します。これにより、局所最適解に陥るリスクを軽減し、より信頼性の高いパラメータ推定を目指します。</p>
</div>
<div style="text-align: center; margin-top: 20px;">
<i class="fas fa-map-signs fa-2x" style="color: var(--color-primary); margin-right: 10px;"></i>
<i class="fas fa-arrow-right fa-lg" style="color: var(--color-gray); margin-right: 10px;"></i>
<i class="fas fa-mountain fa-2x" style="color: var(--color-secondary); margin-right: 5px;"></i>
<i class="fas fa-mountain fa-2x" style="color: var(--color-accent1); margin-right: 10px;"></i>
<i class="fas fa-arrow-right fa-lg" style="color: var(--color-gray); margin-right: 10px;"></i>
<i class="fas fa-flag-checkered fa-2x" style="color: var(--color-primary);"></i>
<p style="font-family: 'Yomogi', cursive; font-size: 1.1em; color: var(--color-dark); margin-top: 5px;">
                様々なスタート地点 (初期条件) から探索を開始し、最も低い谷 (最適解) を見つけ出すイメージです。
            </p>
</div>
</div>
<hr style="border: 1px dashed var(--color-primary); margin: 30px 0;"/>
<div class="note-box">
<p class="note-title"><i class="fas fa-clipboard-check"></i> まとめ</p>
<p>この「C. Fitting Details」セクションでは、提案されたCPTスケーリング法則を実際のデータに適合させるための堅実な方法論が提示されています。</p>
<ul class="unstyled-list">
<li>✅ <strong>事前計算:</strong> 学習率スケジュールから $S_1, S_2$ を計算。</li>
<li>✅ <strong>損失関数:</strong> ロバストなHuber損失を採用。</li>
<li>✅ <strong>最適化:</strong> L-BFGSアルゴリズムを <code>scipy</code> で実装。</li>
<li>✅ <strong>局所最適解対策:</strong> 複数の初期条件から最良のフィットを選択。</li>
</ul>
<p>これらのステップにより、スケーリング法則のパラメータがデータに基づいて客観的に決定され、法則の予測能力と一般性が担保されることになります。これは、論文全体の信頼性を支える重要な技術的基盤と言えるでしょう。 📊</p>
</div>
</div>
<div class="section-card" id="D._Additional_Continual_Pre-Training_Results">
<h2 class="section-title"><i class="fas fa-flask"></i>D. Additional Continual Pre-Training Results</h2>
<div class="content-box">
<p>このセクションでは、論文の主要な発見を裏付けるための<span class="keyword">追加の継続的事前学習（CPT）に関する実験結果</span>を詳しく報告します。ここでの主な目的は、論文で提案されたCPTスケーリング則、特に中心的な役割を果たす<span class="keyword">数式4 (Eq. 4)</span>が、さまざまな条件下でも頑健に機能することを実証することです。具体的には、異なる<span class="highlight">学習率スケジュール（LRS）</span>、<span class="highlight">継続的事前学習用データセット（$D_{cpt}$）</span>、そして多様な<span class="highlight">リプレイ率</span>といった変動要因に対して、提案手法の有効性を検証していきます。これらの結果は、提案されたスケーリング則の一般性と実用性を示す上で非常に重要です。</p>
</div>
<div class="bubble-box">
<p>📝 <strong>このセクションのポイント</strong></p>
<ul class="unstyled-list">
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 提案された数式 (特に Eq. 4) の<span class="keyword">汎用性</span>と<span class="keyword">頑健性</span>の検証。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 様々な学習条件 (LRS, $D_{cpt}$データセット, リプレイ率) での実験結果の提示。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 図や表を用いた視覚的な結果の提示と、それに基づく考察。</li>
</ul>
</div>
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> Prediction of Other LRS</h3>
<div class="content-box">
<p>このサブセクションでは、提案されたCPTスケーリング則（特に図3でフィットされたパラメータを用いたEq. 4）が、<span class="keyword">まだ学習に使用していない新しい学習率スケジュール（LRS）</span>に対しても損失を正確に予測できるか検証します。これは、モデルの汎用性を示す上で重要なポイントです。</p>
<p>具体的には、図3で得られたフィットパラメータを使い、異なるLRS（図10aおよび図10dで示されるLRS）での損失値を予測しました。その結果が図10に示されています。</p>
</div>
<img alt="Figure 10" src="fig10.png"/>
<div class="glass-card">
<p class="note-title"><i class="fas fa-search"></i> 図10の解説</p>
<p>図10は、2つの異なる学習率スケジュール（LRS）設定における損失予測の結果を示しています。</p>
<div class="two-column">
<div class="column">
<p>図10a, 10b, 10c (上段):</p>
<ul class="unstyled-list">
<li><i class="fas fa-calendar-alt" style="color: var(--color-primary);"></i> <strong>(a) 学習率スケジュール:</strong> このグラフは、事前学習（PT）ではWSD (Warmup-Stable-Decay) LRSを使用し、継続的事前学習（CPT）では線形LRSを使用した場合の学習率の変化を示しています。縦軸が学習率、横軸が学習ステップです。
                        <div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> 用語解説: WSD LRS</p>
<p>WSD LRSは、学習の初期に学習率を徐々に上げるウォームアップ期間、その後一定期間安定した学習率を保つ期間、最後に学習率を徐々に下げる減衰期間を持つスケジュールです。これにより、学習の安定性と効率のバランスを取ります。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> 用語解説: 線形LRS</p>
<p>線形LRSは、学習が進むにつれて学習率を一定の割合で線形的に減少させるスケジュールです。</p>
</div>
</li>
<li><i class="fas fa-brain" style="color: var(--color-primary);"></i> <strong>(b) $D_{pt}$ (FineWeb) 損失の予測:</strong> 一般ドメインデータセット（FineWeb）の検証損失を示しています。実線（Truth Loss）が実際の損失値、破線（Predicted Loss）がEq. 4によって予測された損失値です。グラフから、WSD事前学習の真の損失（WSD Pre-Training Truth Loss）と、線形継続的事前学習の予測損失（Linear Continual Pre-Training Predicted Loss）がよく一致していることがわかります。</li>
<li><i class="fas fa-book-open" style="color: var(--color-primary);"></i> <strong>(c) $D_{cpt}$ (Knowledge Pile) 損失の予測:</strong> ドメイン特化データセット（Knowledge Pile）の検証損失を示しています。同様に、実線が実際の損失値、破線が予測損失値です。こちらでも、WSD事前学習の真の損失と、線形継続的事前学習の予測損失が高い精度で一致しています。</li>
</ul>
</div>
<div class="column">
<p>図10d, 10e, 10f (下段):</p>
<ul class="unstyled-list">
<li><i class="fas fa-calendar-alt" style="color: var(--color-secondary);"></i> <strong>(d) 学習率スケジュール:</strong> こちらは、PTではWSD LRS、CPTではコサインLRSを使用した場合の学習率の変化を示しています。
                        <div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> 用語解説: コサインLRS</p>
<p>コサインLRSは、学習率をコサイン関数の形状に沿って滑らかに減少させるスケジュールです。学習の終盤で学習率を非常に小さくすることで、より精密な最適化を目指します。</p>
</div>
</li>
<li><i class="fas fa-brain" style="color: var(--color-secondary);"></i> <strong>(e) $D_{pt}$ (FineWeb) 損失の予測:</strong> WSD事前学習の真の損失（WSD Pre-Training Truth Loss）と、コサイン継続的事前学習の予測損失（Cosine Continual Pre-Training Predicted Loss）が比較されており、ここでも両者がよく一致していることが確認できます。</li>
<li><i class="fas fa-book-open" style="color: var(--color-secondary);"></i> <strong>(f) $D_{cpt}$ (Knowledge Pile) 損失の予測:</strong> 同様に、$D_{cpt}$データセットにおいても、WSD事前学習の真の損失とコサイン継続的事前学習の予測損失が高い精度で一致しています。</li>
</ul>
</div>
</div>
<div class="arrow-connector"></div>
<p>これらの結果から、<span class="highlight">提案されたEq. 4は、異なる種類のLRSに対しても損失曲線を効果的に予測できる</span>ことが示されました。これは、この数式の一般性と実用性が高いことを意味します。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-database"></i> Other $D_{cpt}$ Dataset</h3>
<div class="content-box">
<p>次に、提案手法の頑健性をさらに検証するために、Knowledge-Pile (Fei et al., 2024) 以外の<span class="keyword">別の$D_{cpt}$データセット</span>である <span class="keyword">Pile-of-Law (Henderson* et al., 2022)</span> を用いて実験を行いました。ここでもEq. 4 を使用して転移損失曲線にフィットさせ、その結果を図11に示します。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> 用語解説: $D_{cpt}$ データセット</p>
<p>$D_{cpt}$ とは、Continual Pre-Training (継続的事前学習) で使用される、特定のドメインに特化したデータセットのことです。この論文では、Knowledge-Pile (知識集約型データ) や Pile-of-Law (法律文書データ) が例として挙げられています。</p>
</div>
</div>
<img alt="Figure 11" src="fig11.png"/>
<div class="glass-card">
<p class="note-title"><i class="fas fa-search"></i> 図11の解説</p>
<p>図11は、$D_{cpt}$データセットとしてPile-of-Lawを使用した場合の損失曲線のフィッティング結果を示しています。上段(a-c)と下段(d-f)で異なるLRS設定（具体的にはCPTフェーズでのLRSの違いやステップ数の違いが見られます）での結果が示されていますが、基本的な構造は同じです。</p>
<div class="two-column">
<div class="column">
<p>各段 (例: 上段 a, b, c):</p>
<ul class="unstyled-list">
<li><i class="fas fa-calendar-alt" style="color: var(--color-primary);"></i> <strong>(a) 学習率スケジュール:</strong> PTフェーズとCPTフェーズで使用されたLRSを示します。上段と下段では、CPTのLRS（コサイン関数だが、減衰の仕方が異なる可能性がある）や学習ステップ数が異なります。例えば、上段はCPTステップが約50000まで、下段は約70000までとなっています。</li>
<li><i class="fas fa-brain" style="color: var(--color-primary);"></i> <strong>(b) $D_{pt}$ (FineWeb) 検証損失:</strong> 一般ドメインデータセット（FineWeb）に対する検証損失です。実線（Truth Loss）が実際の損失、破線（Fitted Loss）がEq. 4でフィットされた損失曲線です。グラフ内の数式は、フィットに使用されたEq. 4の具体的なパラメータ値を示しています。例えば、上段(b)の数式は \(L(s) = 1.363 + 0.487(S_1^{pt} + S_1^{cpt})^{-0.620} - 0.294S_2^{pt} - 0.274S_2^{cpt} + 1.305(1 - (1 + 0.036(S_1^{cpt}))^{-0.511})\) となっています。
                        <div class="formula">
                            $$ L(s) = L_0 + A(S_1^{pt} + S_1^{cpt})^{-\alpha} - C_1 S_2^{pt} - C_2 S_2^{cpt} + B(1 - (1 + E \cdot S_1^{cpt})^{-\beta}) $$
                            <p class="reference">※ Eq. 4 の一般形</p>
</div>
</li>
<li><i class="fas fa-balance-scale" style="color: var(--color-primary);"></i> <strong>(c) $D_{cpt}$ (Pile-of-Law) 検証損失:</strong> ドメイン特化データセット（Pile-of-Law）に対する検証損失です。同様に、実線が実際の損失、破線がフィットされた損失曲線を示し、フィットに使用されたEq. 4のパラメータ値も記載されています。例えば、上段(c)の数式は \(L(s) = 2.098 + 0.453(S_1^{pt} + S_1^{cpt})^{-0.608} - 0.264S_2^{pt} - 0.298S_2^{cpt} + 1.497(1 - (1 + 0.311(S_1^{cpt}))^{-0.701})\) です。
                    </li>
</ul>
</div>
<div class="column">
<p class="note-title"><i class="fas fa-chart-pie"></i> 重要な観察点</p>
<p>図11の上下段ともに、$D_{pt}$ (FineWeb) と $D_{cpt}$ (Pile-of-Law) の両方の検証セットにおいて、<span class="highlight">Eq. 4でフィットされた曲線（破線）が実際の損失曲線（実線）と非常によく一致</span>しています。</p>
<p>これは、提案されたスケーリング則が、Knowledge-Pileだけでなく、<span class="keyword">法律文書という異なるドメインのデータセット（Pile-of-Law）に対しても有効</span>であることを示しています。</p>
<p>また、異なるLRS設定や学習ステップ数に対しても、安定して損失曲線を捉えられていることがわかります。</p>
<p><span class="badge purple">汎用性の証</span>: 提案手法が特定のデータセットや設定に過度に依存していないことを示唆しています。</p>
</div>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-random"></i> Different Replay Ratio</h3>
<div class="content-box">
<p>継続的事前学習（CPT）において、元の事前学習データセット（$D_{pt}$）の一部を新しいドメインデータセット（$D_{cpt}$）と混合して学習する<span class="keyword">リプレイ</span>という手法がよく用いられます。このリプレイの割合（<span class="keyword">リプレイ率</span>）は、汎用性能の維持とドメイン適応のバランスを取る上で重要なハイパーパラメータです。</p>
<p>このサブセクションでは、異なるリプレイ率を持つモデルの損失曲線を、それぞれ独立にEq. 4 を用いてフィットさせました。その結果が図12に示されています。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> 用語解説: リプレイ率 (Replay Ratio)</p>
<p>リプレイ率とは、継続的事前学習の際に、元の事前学習データセット ($D_{pt}$) のデータをどのくらいの割合で学習データに混ぜるかを示す値です。例えば、リプレイ率33%とは、$D_{pt}$由来のデータが1、$D_{cpt}$由来のデータが2の割合で混合されることを意味します（論文のFig.6のキャプション参照）。リプレイ率が高いほど、$D_{pt}$のデータが多く使われ、汎用性能が維持されやすくなる傾向がありますが、新しいドメインへの適応は遅れる可能性があります。</p>
</div>
</div>
<img alt="Figure 12. Using Eq. 4 to fit different Dpt replay ratio models independently." src="fig12.png"/>
<div class="glass-card">
<p class="note-title"><i class="fas fa-search"></i> 図12の解説</p>
<p>図12は、$D_{pt}$ (FineWeb) のリプレイ率がそれぞれ33%、50%、67%の場合の実験結果を示しています。各リプレイ率に対して、(a, d, g) 学習率スケジュール、(b, e, h) $D_{pt}$ (FineWeb) の損失、(c, f, i) $D_{cpt}$ (Knowledge Pile, KP) の損失がプロットされています。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));">
<div class="info-card">
<p class="badge orange">リプレイ率 33% (図12a, b, c)</p>
<ul class="unstyled-list">
<li><i class="fas fa-calendar-alt" style="color: var(--color-secondary);"></i> <strong>(a) LRS:</strong> PTとCPTのLRS。</li>
<li><i class="fas fa-brain" style="color: var(--color-secondary);"></i> <strong>(b) $D_{pt}$ (FineWeb) Loss:</strong> WSD事前学習の真の損失と、Eq. 4によるフィット損失。数式は \(L(s) = 3.067 + 0.481(S_1^{pt} + S_1^{cpt})^{-0.515} - 0.219S_2^{pt} - 0.208S_2^{cpt} + 0.477(1 - (1 + 6S_1^{cpt})^{-0.608})\) です。</li>
<li><i class="fas fa-book-open" style="color: var(--color-secondary);"></i> <strong>(c) $D_{cpt}$ (KP) Loss:</strong> WSD事前学習の真の損失と、Eq. 4によるフィット損失。数式は \(L(s) = 3.032 + 0.446(S_1^{pt} + S_1^{cpt})^{-0.596} - 0.201S_2^{pt} - 0.214S_2^{cpt} + 0.501(1 - (1 + 61S_1^{cpt})^{-0.465})\) です。</li>
</ul>
</div>
<div class="info-card">
<p class="badge orange">リプレイ率 50% (図12d, e, f)</p>
<ul class="unstyled-list">
<li><i class="fas fa-calendar-alt" style="color: var(--color-secondary);"></i> <strong>(d) LRS:</strong> PTとCPTのLRS。</li>
<li><i class="fas fa-brain" style="color: var(--color-secondary);"></i> <strong>(e) $D_{pt}$ (FineWeb) Loss:</strong> WSD事前学習の真の損失と、Eq. 4によるフィット損失。数式は \(L(s) = 3.068 + 0.480(S_1^{pt} + S_1^{cpt})^{-0.518} - 0.219S_2^{pt} - 0.206S_2^{cpt} + 0.461(1 - (1 + 3S_1^{cpt})^{-0.915})\) です。</li>
<li><i class="fas fa-book-open" style="color: var(--color-secondary);"></i> <strong>(f) $D_{cpt}$ (KP) Loss:</strong> WSD事前学習の真の損失と、Eq. 4によるフィット損失。数式は \(L(s) = 3.019 + 0.424(S_1^{pt} + S_1^{cpt})^{-0.648} - 0.277S_2^{pt} - 0.277S_2^{cpt} + 0.507(1 - (1 + 8S_1^{cpt})^{-0.466})\) です。</li>
</ul>
</div>
<div class="info-card">
<p class="badge orange">リプレイ率 67% (図12g, h, i)</p>
<ul class="unstyled-list">
<li><i class="fas fa-calendar-alt" style="color: var(--color-secondary);"></i> <strong>(g) LRS:</strong> PTとCPTのLRS。</li>
<li><i class="fas fa-brain" style="color: var(--color-secondary);"></i> <strong>(h) $D_{pt}$ (FineWeb) Loss:</strong> WSD事前学習の真の損失と、Eq. 4によるフィット損失。数式は \(L(s) = 3.069 + 0.479(S_1^{pt} + S_1^{cpt})^{-0.519} - 0.219S_2^{pt} - 0.208S_2^{cpt} + 0.105(1 - (1 + 11S_1^{cpt})^{-0.069})\) です。</li>
<li><i class="fas fa-book-open" style="color: var(--color-secondary);"></i> <strong>(i) $D_{cpt}$ (KP) Loss:</strong> WSD事前学習の真の損失と、Eq. 4によるフィット損失。数式は \(L(s) = 3.017 + 0.442(S_1^{pt} + S_1^{cpt})^{-0.685} - 0.274S_2^{pt} - 0.276S_2^{cpt} + 0.532(1 - (1 + 5693S_1^{cpt})^{-0.166})\) です。</li>
</ul>
</div>
</div>
<div class="arrow-connector"></div>
<p>すべてのリプレイ率（33%, 50%, 67%）において、$D_{pt}$および$D_{cpt}$の両方の検証セットで、<span class="highlight">Eq. 4によってフィットされた曲線（破線）が実際の損失曲線（実線）と密接に一致</span>していることが観察されます。これは、Eq. 4が<span class="keyword">リプレイ率という重要なハイパーパラメータが変化する状況においても、学習ダイナミクスを正確に捉えることができる</span>ことを示唆しています。</p>
<p><span class="badge purple">汎用性の強化</span>: リプレイ率はCPTの性能に大きな影響を与える要素であり、これに対応できることは提案手法の有用性を高めます。</p>
<p>注意点として、このサブセクションでは各リプレイ率に対してEq. 4を「独立に」フィットさせています。つまり、リプレイ率ごとに異なるパラメータセットが学習されています。論文の後半（Appendix H, Eq. 8）では、リプレイ率自体を数式に組み込んだ統一的な定式化も提案されています。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-table"></i> Table 1. Experimental settings adopted in this work.</h3>
<div class="content-box">
<p>このセクションでは、本研究で採用された実験設定の詳細が記載されている表1について解説します。</p>
</div>
<img alt="Table 1. Experimental settings" src="table1.png"/>
<div class="glass-card">
<p class="note-title"><i class="fas fa-search"></i> 表1の解説</p>
<p>表1は、この論文で行われた実験で使用された主要な設定をまとめたものです。以下に各項目の意味を説明します。</p>
<div class="table-wrapper">
<table>
<thead>
<tr>
<th>項目</th>
<th>設定A</th>
<th>設定B</th>
<th>設定C</th>
<th>設定D</th>
<th>設定E</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Model Size (N)</strong></td>
<td>106M</td>
<td>594M</td>
<td>1.7B</td>
<td>106M</td>
<td>106M</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>Model Size (N)</strong>: モデルの非埋め込みパラメータの数を示します。Mは百万（Mega）を表します。例えば、106Mは1億600万パラメータです。多様なモデルサイズで検証することで、提案手法のスケールに対する頑健性を見ています。</td>
</tr>
<tr>
<td><strong>Global Batch Size (GBS)</strong></td>
<td>1024</td>
<td>2048</td>
<td>2048</td>
<td>256, 4096</td>
<td>1024</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>Global Batch Size (GBS)</strong>: 1回の学習イテレーションで使用される総サンプル数です。GBSが大きいと学習が安定しやすい一方、メモリ消費も大きくなります。設定Dでは複数のGBSで実験していることが分かります。</td>
</tr>
<tr>
<td><strong>Sequence Length</strong></td>
<td>4096</td>
<td>4096</td>
<td>4096</td>
<td>2K, 8K</td>
<td>4096</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>Sequence Length</strong>: モデルが一度に処理するトークンの系列長です。Kは千（Kilo）を表します。長い系列長はより広範な文脈を捉えられますが、計算コストが増加します。設定Dでは2000と8000の系列長で実験しています。</td>
</tr>
<tr>
<td><strong>Peak LR (PT)</strong></td>
<td>1.2e-3</td>
<td>6.0e-4</td>
<td>3.0e-4</td>
<td>1.2e-3</td>
<td>1.2e-3</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>Peak LR (PT)</strong>: 事前学習（PT）フェーズでの学習率の最大値です。学習率は学習の進捗を左右する重要なハイパーパラメータです。e-3は \(10^{-3}\) を意味します。</td>
</tr>
<tr>
<td><strong>Peak LR (CPT)</strong></td>
<td>1.2e-4</td>
<td>6.0e-5</td>
<td>3.0e-5</td>
<td>1.2e-4</td>
<td>1.2e-4</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>Peak LR (CPT)</strong>: 継続的事前学習（CPT）フェーズでの学習率の最大値です。通常、PTのピークLRよりも小さい値が設定されます。</td>
</tr>
<tr>
<td><strong>$D_{pt}$ Dataset</strong></td>
<td>FineWeb</td>
<td>FineWeb</td>
<td>FineWeb</td>
<td>FineWeb</td>
<td>FineWeb</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>$D_{pt}$ Dataset</strong>: 事前学習に使用される一般ドメインのデータセットです。この研究では主にFineWebが使用されています。</td>
</tr>
<tr>
<td><strong>$D_{cpt}$ Dataset</strong></td>
<td>KP, PoL</td>
<td>KP</td>
<td>KP</td>
<td>KP</td>
<td>KP, PoL</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>$D_{cpt}$ Dataset</strong>: 継続的事前学習に使用されるドメイン特化データセットです。KPはKnowledge-Pile、PoLはPile-of-Lawを指します。設定AとEでは複数の$D_{cpt}$で検証しています。</td>
</tr>
<tr>
<td><strong>Tokenizer</strong></td>
<td colspan="5" style="text-align:center;">LLaMA-3's Tokenizer</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>Tokenizer</strong>: テキストをモデルが処理できる数値列（トークン）に変換するためのツールです。多くの実験でLLaMA-3のトークナイザが採用されています。これは実験の再現性や比較可能性を高めるためです。</td>
</tr>
<tr>
<td><strong>Optimizer</strong></td>
<td colspan="5" style="text-align:center;">AdamW</td>
</tr>
<tr>
<td colspan="6">🔍 <strong>Optimizer</strong>: モデルのパラメータを更新するための最適化アルゴリズムです。AdamW (Kingma &amp; Ba, 2015; Loshchilov &amp; Hutter, 2017) は大規模言語モデルの学習で広く用いられている手法です。</td>
</tr>
</tbody>
</table>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-cogs"></i> 補足情報</p>
<ul class="unstyled-list">
<li><i class="fas fa-atom" style="color: var(--color-accent1);"></i> <strong>AdamW Optimizer</strong>: Adam (Adaptive Moment Estimation) にWeight Decay（重み減衰）を適切に適用するよう改良された最適化アルゴリズムです。過学習を抑制し、汎化性能を高める効果が期待されます。</li>
<li><i class="fas fa-language" style="color: var(--color-accent1);"></i> <strong>LLaMA-3's Tokenizer</strong>: LLaMA-3モデルファミリー (Dubey et al., 2024) で使用されているトークナイザです。近年の高性能なLLMで採用されている標準的なものです。</li>
<li><i class="fas fa-info-circle" style="color: var(--color-accent1);"></i> 「ほとんどの実験はLLaMA-3のトークナイザを採用」とあることから、一部例外がある可能性も示唆されますが、主要な実験はこの設定に基づいていると考えられます。</li>
</ul>
</div>
<p>この表は、論文の実験がどのような具体的な設定で行われたかを理解するための基礎情報を提供します。特に、<span class="highlight">設定Aが多くの実験のベース</span>となっており、他の設定（B, C, D, E）はモデルサイズ、バッチサイズ、系列長などを変更して提案手法の頑健性を検証するために用いられています。</p>
</div>
<div class="bubble-box">
<p>🎉 <strong>このセクションのまとめ</strong></p>
<p>この「D. Additional Continual Pre-Training Results」セクションでは、提案されたCPTスケーリング則 (特にEq. 4) が、様々な条件下で有効であることを示す追加実験の結果が提示されました。</p>
<ul class="unstyled-list">
<li><i class="fas fa-lightbulb" style="color: var(--color-accent3);"></i> <strong>異なるLRSへの対応:</strong> 図3でフィットしたパラメータを用いて、新しいLRS（線形LRSやコサインLRS）における損失を予測し、実際の損失とよく一致することを確認しました (図10)。</li>
<li><i class="fas fa-lightbulb" style="color: var(--color-accent3);"></i> <strong>異なる$D_{cpt}$データセットへの対応:</strong> Knowledge-Pile以外の$D_{cpt}$データセットであるPile-of-Lawを用いても、Eq. 4が損失曲線を正確にフィットできることを示しました (図11)。</li>
<li><i class="fas fa-lightbulb" style="color: var(--color-accent3);"></i> <strong>異なるリプレイ率への対応:</strong> 様々なリプレイ率（33%, 50%, 67%）で学習されたモデルの損失曲線を、それぞれEq. 4で独立にフィットさせ、良好な一致を得ました (図12)。</li>
<li><i class="fas fa-lightbulb" style="color: var(--color-accent3);"></i> <strong>実験設定の詳細:</strong> 論文全体で使用された実験設定の詳細が表1にまとめられ、読者が実験条件を理解する助けとなっています。</li>
</ul>
<p>これらの結果は、提案されたCPTスケーリング則が、多様な学習条件やデータセットに対して<span class="keyword">一般性と頑健性を持つ</span>ことを強く支持しており、大規模言語モデルの継続的事前学習のダイナミクスを理解し予測するための強力なツールであることを示しています。</p>
</div>
</div>
<div class="section-card" id="E._Extension_To_Model_Size_Scaling">
<h2 class="section-title"><i class="fas fa-cubes"></i>E. モデルサイズスケーリングへの拡張</h2>
<div class="content-box">
<p>このセクションでは、ここまでの議論で構築してきた<span class="keyword">CPTスケーリング則</span>を、さまざまな<span class="keyword">モデルサイズ</span>に拡張することを目的としています。具体的には、モデルサイズが<span class="keyword">分布シフト項</span>や学習曲線全体にどのように影響を与えるかを調査し、提案する数式（Eq. 4）をモデルサイズのスケーリングに対応できるように拡張します。</p>
<p>主な論旨は以下の通りです：</p>
<ul class="unstyled-list">
<li><i class="fas fa-drafting-compass" style="color: var(--color-accent1);"></i> 分布シフト項はモデルサイズに依存しない可能性がある。</li>
<li><i class="fas fa-ruler-combined" style="color: var(--color-accent2);"></i> 学習率アニーリングのスケーリング則と組み合わせることで、モデルサイズを考慮したCPTスケーリング則を導出できる。</li>
<li><i class="fas fa-network-wired" style="color: var(--color-secondary);"></i> この拡張されたスケーリング則は、異なるモデルサイズの学習曲線をうまくフィッティングできる。</li>
</ul>
<p>ただし、実験で使用したモデルサイズは現在の主流LLMほど大きくないため、これらの結論は既存の結果に基づく仮定の上に成り立っている点に注意が必要です。</p>
</div>
<div class="subsection-title"><i class="fas fa-puzzle-piece"></i>異なるモデルサイズの分布シフト項</div>
<div class="content-box">
<p>まず、モデルサイズ <span class="keyword">$N$</span> が<span class="keyword">分布シフト項</span>にどのような影響を与えるかを調査します。</p>
<p><span class="badge blue">実験設定</span> エンベディング層を除いて、106M、594M、1.7Bという異なるサイズのモデルを用いて、一定の学習率（LR）でCPT実験を行いました。</p>
<div class="glass-card">
<p><i class="fas fa-flask" style="color: var(--color-primary);"></i> <strong>実験結果の観察:</strong></p>
<p>図14a（論文中のFigure 14a）に示されているように、異なるモデルサイズ <span class="keyword">$N$</span> における分布シフト項は、ほぼ一致する傾向が見られました。</p>
<img alt="Figure 14a and 14b" src="https://i.imgur.com/gJZPqS6.png"/>
<p class="reference">図14. 一定の学習率スケジュールにおける異なるモデルサイズの分布シフト。(a) $D_{pt}$ 分布シフト。(b) $D_{cpt}$ 分布シフト。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i>図14の見方</p>
<p>この図は、横軸にCPTステップ（学習の進行度）、縦軸に分布シフト（元の分布からのズレの大きさ）を示しています。異なる色の線が、それぞれ異なるモデルサイズ（106M, 594M, 1.7B）に対応します。</p>
<ul class="unstyled-list">
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> <strong>(a) $D_{pt}$ (FineWeb) 分布シフト:</strong> 継続事前学習データセット ($D_{cpt}$) で学習を進めた際に、元の事前学習データセット ($D_{pt}$) に対する性能がどれだけ変化したか（通常は悪化）を示します。線がほぼ重なっていることから、モデルサイズに関わらず分布シフトの挙動が似ていることが分かります。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> <strong>(b) $D_{cpt}$ (Knowledge Pile) 分布シフト:</strong> 継続事前学習データセット ($D_{cpt}$) で学習を進めた際に、$D_{cpt}$ に対する性能がどれだけ変化したか（通常は向上）を示します。こちらも同様に、モデルサイズによる大きな差異は見られません。</li>
</ul>
</div>
</div>
<div class="bubble-box">
<p><i class="fas fa-question-circle" style="color: var(--color-primary);"></i> <strong>仮説:</strong></p>
<p>これらの観察に基づいて、<span class="highlight">分布シフト項はモデルサイズ <span class="keyword">$N$</span> および転移学習の開始点の両方に依存しない</span>という仮説を立てることができます。</p>
<p><i class="fas fa-long-arrow-alt-right" style="color: var(--color-secondary);"></i> これは、データ転移が異なるモデルサイズ間で一貫した損失差をもたらすことを示唆しています。</p>
</div>
</div>
<div class="subsection-title"><i class="fas fa-expand-arrows-alt"></i>モデルサイズのスケーリング</div>
<div class="content-box">
<p>一方で、Tissueら (2024) の研究では、学習率（LR）アニーリングを伴うスケーリング則において、学習率アニーリングがモデルサイズ <span class="keyword">$N$</span> に応じてスケールし、$S_2 \propto N^\gamma$ となることが見出されています。</p>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i>用語解説: 学習率アニーリング ($S_2$)</p>
<p>$S_2$ は、以前のセクションで定義された通り、学習率の減衰（アニーリング）が学習に与える影響を定量化したものです。具体的には、$\begin{array} { r } { S _ { 2 } = \sum _ { i = 1 } ^ { t } \sum _ { k = 1 } ^ { i } \left( \eta _ { k - 1 } - \eta _ { k } \right) } \end{array} \cdot \lambda ^ { i - k }$ で表され、学習率の減少量とその履歴を考慮した項です。$N^\gamma$ に比例するということは、モデルサイズが大きいほど、学習率アニーリングの効果が大きくなる（または異なる形で現れる）ことを意味します。</p>
</div>
<p>上記の実験と分析に基づいて、提案されている式（Eq. 4）を拡張し、モデルサイズのスケーリングを組み込みます。</p>
<div class="formula">
<p><strong>拡張されたCPTスケーリング則 (Eq. 7):</strong></p>
<p>\( L(S^{pt}, S^{cpt}) = L_0 + A \cdot (S_1^{pt} + S_1^{cpt})^{-\alpha} - C \cdot S_2^{pt} \cdot N^{\gamma_1} \)</p>
<p>\( \phantom{L(S^{pt}, S^{cpt})} + B \cdot (1 - (1 + E \cdot S_1^{cpt})^{-\beta}) \)</p>
<p>\( \phantom{L(S^{pt}, S^{cpt})} - D \cdot S_2^{cpt} \cdot N^{\gamma_2} + F \cdot N^{-\gamma_3} \)</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-calculator"></i>数式の解釈 (Eq. 7)</p>
<ul class="unstyled-list">
<li><i class="fas fa-caret-right" style="color: var(--color-primary);"></i> <span class="keyword">$L(S^{pt}, S^{cpt})$</span>: 継続事前学習後の損失。</li>
<li><i class="fas fa-caret-right" style="color: var(--color-primary);"></i> <span class="keyword">$L_0, A, C, D, B, E, \alpha, \beta$</span>: Eq. 4と同様の定数パラメータ。</li>
<li><i class="fas fa-caret-right" style="color: var(--color-primary);"></i> <span class="keyword">$S_1^{pt}, S_1^{cpt}$</span>: それぞれ事前学習段階、継続事前学習段階でのフォワードエリア（累積学習率）。</li>
<li><i class="fas fa-caret-right" style="color: var(--color-primary);"></i> <span class="keyword">$S_2^{pt}, S_2^{cpt}$</span>: それぞれ事前学習段階、継続事前学習段階でのアニーリングエリア。</li>
<li><i class="fas fa-paint-brush" style="color: var(--color-accent2);"></i> <span class="keyword">$N$</span>: モデルサイズ。</li>
<li><i class="fas fa-paint-brush" style="color: var(--color-accent2);"></i> <span class="keyword">$F, \gamma_1, \gamma_2, \gamma_3$</span>: モデルサイズスケーリングに関連する新たな定数パラメータ。</li>
<li><i class="fas fa-highlighter" style="color: var(--color-secondary);"></i> <span class="highlight">$C \cdot S_2^{pt} \cdot N^{\gamma_1}$</span>: 事前学習のアニーリング項にモデルサイズの影響を導入。</li>
<li><i class="fas fa-highlighter" style="color: var(--color-secondary);"></i> <span class="highlight">$D \cdot S_2^{cpt} \cdot N^{\gamma_2}$</span>: 継続事前学習のアニーリング項にモデルサイズの影響を導入。</li>
<li><i class="fas fa-info-circle" style="color: var(--color-accent1);"></i> <span class="highlight">$F \cdot N^{-\gamma_3}$</span>: この項は、従来のChinchillaスケーリング則 (Hoffmann et al., 2022) におけるモデルサイズ項であり、モデルサイズが大きいほど損失が小さくなる効果を表します。</li>
<li>分布シフト項 <span class="highlight">$B \cdot (1 - (1 + E \cdot S_1^{cpt})^{-\beta})$</span> は、仮説通りモデルサイズ <span class="keyword">$N$</span> に依存しない形で残っています。</li>
</ul>
</div>
<p>このEq. 7を用いて、すべてのモデルサイズの転移曲線をフィッティングした結果が図15に示されています。</p>
<img alt="Figure 15" src="https://i.imgur.com/vHq4gXF.png"/>
<p class="reference">図15. Eq. 7 を用いて、$D_{pt}$ および $D_{cpt}$ 検証セットにおける全モデルサイズの損失曲線をフィッティングした結果。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-chart-line"></i>図15の見方</p>
<p>この図は、異なるモデルサイズ（106M、594M、1.7B - 点線や実線で表現）の学習曲線（横軸：ステップ、縦軸：検証損失）と、Eq. 7によるフィッティング曲線（色付きの線）を示しています。</p>
<ul class="unstyled-list">
<li><i class="fas fa-calendar-alt" style="color: var(--color-accent1);"></i> <strong>(a) 学習率スケジュール:</strong> PT（事前学習）とCPT（継続事前学習）で用いられた学習率のスケジュールです。</li>
<li><i class="fas fa-file-alt" style="color: var(--color-accent2);"></i> <strong>(b) FineWeb ($D_{pt}$) 検証損失:</strong> 元の事前学習データセットに対する損失の変化。異なるモデルサイズの実際の損失（点や細線）と、Eq. 7でフィッティングされた曲線（太線）が示されています。Eq. 7が各モデルサイズの挙動を捉えられているかが分かります。</li>
<li><i class="fas fa-folder-open" style="color: var(--color-secondary);"></i> <strong>(c) Knowledge Pile ($D_{cpt}$) 検証損失:</strong> 継続事前学習データセットに対する損失の変化。同様に、実際の損失とフィッティング曲線が示されています。</li>
</ul>
<p>全体として、提案されたEq. 7が、複数のモデルサイズにまたがる学習曲線のトレンドを包括的に捉えられていることを示唆しています。</p>
</div>
</div>
<div class="subsection-title"><i class="fas fa-check-double"></i>より大きなモデルサイズへの適合性検証</div>
<div class="content-box">
<p>さらに、元のEq. 4（モデルサイズスケーリング項を含まないバージョン）を、より大きなモデルサイズ（594Mと1.7B）のCPT損失曲線に<span class="keyword">個別に</span>フィッティングしました。その結果が図16に示されています。</p>
<img alt="Figure 16" src="https://i.imgur.com/D4sT8xJ.png"/>
<p class="reference">図16. より大きなモデルサイズ（594Mと1.7B）のCPT損失曲線に対するEq. 4の個別フィッティング。(a)-(c) 594Mモデル、(d)-(f) 1720M(1.7B)モデル。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-search-plus"></i>図16の見方</p>
<p>この図は、2つの異なるモデルサイズ（594Mと1.7B）に対して、それぞれEq. 4を用いてCPTの学習曲線をフィッティングした結果を示しています。図15とは異なり、ここでは各モデルサイズに対して個別にフィッティングパラメータを調整しています。</p>
<p class="badge orange">594Mモデル ((a)-(c))</p>
<ul class="unstyled-list">
<li><i class="fas fa-wave-square" style="color: var(--color-accent1);"></i> <strong>(a) 学習率スケジュール:</strong> このモデルで使用された学習率スケジュール。</li>
<li><i class="fas fa-book" style="color: var(--color-accent2);"></i> <strong>(b) $D_{pt}$ (FineWeb) 損失:</strong> 594Mモデルにおける$D_{pt}$の真の損失（点線）とEq. 4によるフィッティング曲線（実線）。</li>
<li><i class="fas fa-lightbulb" style="color: var(--color-secondary);"></i> <strong>(c) $D_{cpt}$ (Knowledge Pile) 損失:</strong> 594Mモデルにおける$D_{cpt}$の真の損失（点線）とEq. 4によるフィッティング曲線（実線）。</li>
</ul>
<p class="badge purple">1.7Bモデル ((d)-(f))</p>
<ul class="unstyled-list">
<li><i class="fas fa-wave-square" style="color: var(--color-accent1);"></i> <strong>(d) 学習率スケジュール:</strong> このモデルで使用された学習率スケジュール。</li>
<li><i class="fas fa-book" style="color: var(--color-accent2);"></i> <strong>(e) $D_{pt}$ (FineWeb) 損失:</strong> 1.7Bモデルにおける$D_{pt}$の真の損失（点線）とEq. 4によるフィッティング曲線（実線）。</li>
<li><i class="fas fa-lightbulb" style="color: var(--color-secondary);"></i> <strong>(f) $D_{cpt}$ (Knowledge Pile) 損失:</strong> 1.7Bモデルにおける$D_{cpt}$の真の損失（点線）とEq. 4によるフィッティング曲線（実線）。</li>
</ul>
<p>これらの結果は、<span class="highlight">Eq. 4が特定のモデルサイズに対しても柔軟に適応し、学習曲線をよく捉えることができる</span>ことを示しています。これは、Eq. 7のより一般的な形（複数のモデルサイズを同時に扱う）だけでなく、個々のモデルに対してもEq. 4の基本構造が有効であることを裏付けています。</p>
</div>
<p>この結果は、提案された数式（Eq. 4）が様々なモデルサイズにわたって適応可能であることを示しています。</p>
</div>
<div class="subsection-title"><i class="fas fa-exclamation-triangle"></i>考察と注意点</div>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-glasses"></i>重要な注意点</p>
<p>本研究の実験で使用されたモデルサイズ（最大1.7B）は、今日の主流となっているLLM（例えば7B、70Bなど）のスケールにはまだ達していません。そのため、<span class="highlight">モデルサイズに関する実験結果は、既存の研究成果に基づく仮定の上に成り立っています</span>。</p>
<p>もし、この仮定（特に分布シフト項がモデルサイズに依存しないという点や、アニーリング項のスケーリング則など）がより大きなモデルサイズ（例：7B、70B）でも成り立つならば、以下の結論が得られる可能性があります：</p>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-brain"></i>大規模モデルにおける示唆</p>
<p><span class="keyword">同じ絶対的な分布シフト値</span>の影響を受けると仮定した場合：</p>
<ul class="unstyled-list">
<li><i class="fas fa-arrow-down" style="color: var(--color-secondary);"></i> より大きなモデルは、一般ドメイン（$D_{pt}$）に対してより脆弱になる（性能低下が顕著になる）。
                    <br/><span class="reference">（分布シフトによる損失増加の影響が、相対的に大きくなる可能性があるため）</span></li>
<li><i class="fas fa-arrow-up" style="color: var(--color-accent1);"></i> より大きなモデルは、下流ドメイン（$D_{cpt}$）への適応性がより高くなる。
                    <br/><span class="reference">（モデルの表現力が高い分、新しいドメイン知識を学習しやすい可能性があるため）</span></li>
</ul>
</div>
<p>これは、大規模モデルのCPT戦略を考える上で重要な示唆となりますが、さらなる大規模実験による検証が必要です。</p>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-pencil-ruler"></i>セクションEのまとめ</p>
<p>このセクションでは、CPTスケーリング則をモデルサイズの変化に対応させる試みが行われました。</p>
<ol class="process-step unstyled-list">
<li><span class="step-number">1</span><span class="step-content"><strong>分布シフト項のモデルサイズ不変性の仮説:</strong> 実験結果から、分布シフト項はモデルサイズに大きく依存しない可能性が示唆されました。</span></li>
<li><span class="step-number">2</span><span class="step-content"><strong>モデルサイズスケーリングの導入:</strong> 既存の学習率アニーリングのスケーリング則とChinchillaスケーリング則の知見を取り入れ、Eq. 4を拡張したEq. 7を提案しました。この式は、アニーリング項と全体的なベースロスにモデルサイズの影響を組み込んでいます。</span></li>
<li><span class="step-number">3</span><span class="step-content"><strong>実験的検証:</strong> 提案されたEq. 7が複数のモデルサイズ（106M～1.7B）の学習曲線を同時にフィッティングできること、また元のEq. 4も個々のモデルサイズに対して有効であることが示されました。</span></li>
<li><span class="step-number">4</span><span class="step-content"><strong>限界と将来の展望:</strong> 実験したモデルサイズが現在のSOTA LLMより小さいという限界を認識しつつ、仮定が大規模モデルでも成り立つ場合の興味深い示唆（大規模モデルの脆弱性と適応性）について言及しました。</span></li>
</ol>
<p>この拡張により、CPTのダイナミクスをより一般的に理解し、異なるスケールのモデルに対しても予測や最適化を行うための基礎が提供されました。</p>
</div>
</div>
<div class="section-card" id="F._Batch_Size_and_Sequence_Length">
<h2 class="section-title"><i class="fas fa-cubes"></i> F. Batch Size and Sequence Length</h2>
<div class="content-box">
<p>このセクションでは、<span class="keyword">継続的事前学習（CPT）</span>プロセスにおける<span class="keyword">バッチサイズ</span>と<span class="keyword">シーケンス長</span>の変更が、学習ダイナミクス、特に論文で提案された<span class="keyword">スケーリング則（式4）</span>にどのような影響を与えるかを調査します。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> このセクションの主な目的と論旨</p>
<ul>
<li><strong>目的</strong>: CPT中にバッチサイズやシーケンス長を変更した場合でも、提案されているスケーリング則（特に分布シフト項の形式）が頑健性を保つか検証する。</li>
<li><strong>論旨</strong>: 実験結果から、バッチサイズやシーケンス長が変化するより現実的なCPTシナリオにおいても、式(4)で記述される分布シフトの基本的な形式は維持され、損失曲線をよくフィッティングできることを示す。</li>
</ul>
</div>
</div>
<div class="content-box">
<p>これまでの実験では、事前学習（PT）と継続的事前学習（CPT）で同じバッチサイズを維持していましたが、実際の状況では異なる場合があります。</p>
<div class="info-grid">
<div class="info-card">
<p><i class="fas fa-server"></i> <strong>計算資源とデータセットの制約</strong></p>
<p>計算資源や利用可能なデータセットが限られている場合、CPTプロセスではPTよりも<span class="highlight">小さいグローバルバッチサイズ</span>を採用することがあります。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-database fa-2x" style="color: var(--color-primary);"></i>
<span style="font-size: 20px; color: var(--color-primary); margin: 0 10px;">→</span>
<i class="fas fa-battery-quarter fa-2x" style="color: var(--color-secondary);"></i>
<p style="font-family: 'Yomogi', cursive;">PT: 大量データ → CPT: リソース制限でバッチサイズ縮小</p>
</div>
</div>
<div class="info-card">
<p><i class="fas fa-ruler-combined"></i> <strong>コンテキスト長の増加</strong></p>
<p>CPTの目的がLLMの<span class="keyword">コンテキスト長（処理可能なトークン数）</span>を増やすことである場合、<span class="highlight">シーケンス長</span>と<span class="keyword">RoPE base</span>も増加させる必要があります。</p>
<p>具体例として、本研究ではシーケンス長を<span class="highlight">8K (8192トークン)</span> にする場合、<span class="keyword">RoPE base</span>を<span class="highlight">10000</span>から<span class="highlight">500000</span>へと大幅に増加させています。</p>
<div style="text-align: center; margin-top: 10px;">
<i class="fas fa-text-width fa-2x" style="color: var(--color-primary);"></i>
<span style="font-size: 20px; color: var(--color-primary); margin: 0 10px;">→</span>
<i class="fas fa-arrows-alt-h fa-2x" style="color: var(--color-accent1);"></i>
<p style="font-family: 'Yomogi', cursive;">シーケンス長 <i class="fas fa-arrow-up"></i> ⇒ RoPE base <i class="fas fa-arrow-up"></i></p>
</div>
</div>
</div>
<div class="definition-box" style="margin-top: 15px;">
<p class="definition-title"><i class="fas fa-book-open"></i> 用語解説: RoPE base</p>
<p><span class="keyword">RoPE (Rotary Position Embedding)</span> は、トランスフォーマーモデルにおいてトークンの位置情報を符号化するための手法の一つです。<span class="keyword">RoPE base</span> はその計算に使用される基数のことで、通常 \(10000\) のような値が使われます。モデルがより長いシーケンス（コンテキスト）を効果的に処理できるようにするために、シーケンス長を拡張する際には、この RoPE base の値を調整（多くの場合、大きく）することがあります。この調整により、長い距離にあるトークン間の相対的な位置関係をより良く区別できるようになります。</p>
</div>
<p>本セクションでは、これらの状況を模倣し、バッチサイズをPT時より大きくしたり小さくしたり、シーケンス長を変更したりするCPT実験を行っています。その結果は図17に示されています。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> 異なるバッチサイズにおける分布シフトの検証</h3>
<div class="content-box">
<p>この実験の主な関心事は、<span class="keyword">バッチサイズ</span>や<span class="keyword">シーケンス長</span>が変化したときに、<span class="keyword">分布シフト項</span>の数学的な<span class="highlight">形式が同じままであるか</span>どうかです。</p>
<div class="note-box">
<p class="note-title"><i class="fas fa-search"></i> 検証の焦点：定数学習率（LR）の利用</p>
<p>著者らは、分布シフト項の特性を純粋に評価するために、学習率（LR）を一定（<span class="keyword">constant LR</span>）に設定しています。これにより、学習率スケジューリング（例：ウォームアップや減衰）による損失曲線の変動要因を排除し、データ分布の変化（つまり分布シフト）のみが損失曲線に与える影響を分離して観察することができます。</p>
</div>
<p>検証には、論文の前半で導入された以下の <span class="keyword">式(4)</span> を使用して、異なるバッチサイズ設定での損失曲線をフィッティングします。</p>
<div class="formula">
<p style="font-family: 'Yomogi', cursive; font-size: 16px;">式(4): CPTスケーリング則</p>
<p>\( L(t) = L_{base}(t) + \Delta L(t) \)</p>
<p>ここで、</p>
<ul>
<li><span class="highlight">\(L(t)\)</span>: CPTステップ \(t\) における総損失</li>
<li><span class="highlight">\(L_{base}(t)\)</span>: 学習率(LR)アニーリングの影響を考慮したベースの損失曲線。<br/>
                    \( L_{base}(t) = L_0 + A \cdot (S_1^{pt} + S_1^{cpt})^{-\alpha} - C_1 \cdot S_2^{pt} - C_2 \cdot S_2^{cpt} \)
                </li>
<li><span class="highlight">\(\Delta L(t)\)</span>: データ分布のシフトによる損失の変化（分布シフト項）。<br/>
                    \( \Delta L(t) = B \cdot (1 - (1 + E \cdot S_1^{cpt})^{-\beta}) \)
                </li>
</ul>
<p style="font-family: 'Yomogi', cursive; margin-top:10px;">この実験では特に、<span class="keyword">\(\Delta L(t)\)</span> の形（パラメータ \(B, E, \beta\) で特徴づけられる）がバッチサイズ等の変更後も維持されるかに注目します。</p>
</div>
<p><i class="fas fa-image"></i> <strong>図17の実験結果について</strong></p>
<p>論文のAppendix Fに記載されている図17(a)および図17(b)は、それぞれ <span class="keyword">$D_{pt}$ (FineWeb)</span> と <span class="keyword">$D_{cpt}$ (Knowledge Pile)</span> の検証セットにおける損失曲線を示しています。これらの図では、様々なバッチサイズとシーケンス長の設定での実験結果がプロットされています。</p>
<div class="glass-card" style="margin-top:15px;">
<p style="font-family: 'Yomogi', cursive; font-size: 18px; color: var(--color-primary); text-align:center;"><strong><i class="fas fa-microscope"></i> 図17の概略とポイント</strong></p>
<p>図17は、横軸にCPTステップ数、縦軸に検証損失をとったグラフです。複数の線がプロットされており、それぞれが異なる設定に対応しています。</p>
<ul class="unstyled-list">
<li><i class="fas fa-ruler-horizontal" style="color: var(--color-accent1);"></i> <strong>PT (ベースライン)</strong>: シーケンス長4K, グローバルバッチサイズ(GBS)1024</li>
<li><i class="fas fa-expand-arrows-alt" style="color: var(--color-accent2);"></i> <strong>設定1</strong>: シーケンス長8K (増加), GBS 512 (減少)</li>
<li><i class="fas fa-compress-arrows-alt" style="color: var(--color-secondary);"></i> <strong>設定2</strong>: シーケンス長4K (維持), GBS 256 (減少)</li>
<li><i class="fas fa-exchange-alt" style="color: var(--color-accent3);"></i> <strong>設定3</strong>: シーケンス長2K (減少), GBS 4096 (増加)</li>
</ul>
<p>これらの異なる設定で得られた損失曲線（$D_{pt}$ と $D_{cpt}$ の両方）に対して、式(4)の分布シフト項 $\Delta L(t)$ がフィッティングされています。重要なのは、各曲線にフィッティングされた $\Delta L(t)$ の<span class="highlight">数式的な形が共通している</span>ことです。</p>
<p style="font-family: 'Yomogi', cursive;">例：$\Delta L(t) = \text{定数} \cdot (1 - (1 + \text{定数} \cdot S_1^{cpt})^{-\text{定数}})$ という基本構造は変わらず、係数値のみが各条件で異なる。</p>
</div>
<div class="bubble-box" style="margin-top:20px;">
<p style="font-family: 'Yomogi', cursive; font-size:18px; color: var(--color-primary);"><i class="fas fa-bullseye"></i> 実験結果の核心</p>
<p>図17a（$D_{pt}$ 検証損失）と図17b（$D_{cpt}$ 検証損失）に示されるように、<span class="highlight">バッチサイズが大きい場合も小さい場合も</span>、また<span class="highlight">シーケンス長が変更された場合も</span>、転移ステップ（CPTステップ）が異なるすべての損失曲線が、<span class="keyword">単一の形式を持つ分布シフト項</span>にフィッティングできることが確認されました。</p>
<div style="text-align:center; margin: 15px 0;">
<span class="badge blue">バッチサイズ大</span>
<span class="badge purple">バッチサイズ小</span>
<span class="badge orange">シーケンス長変更</span>
<i class="fas fa-arrow-right" style="margin: 0 10px; color: var(--color-dark);"></i>
<span class="badge green" style="background-color: var(--color-accent1); color:white;">共通の分布シフト項の形式</span>
</div>
<p>これは、論文で提案された <span class="keyword">式(4)</span> が、バッチサイズやシーケンス長の変更といった、より現実的なCPTのセットアップにも<span class="highlight">適用可能である</span>ことを示しています。</p>
</div>
</div>
<div class="note-box" style="margin-top: 25px;">
<p class="note-title"><i class="fas fa-check-circle"></i> このセクションの結論</p>
<p>継続的事前学習（CPT）においてバッチサイズやシーケンス長を変更しても、論文で提案されたスケーリング則（式4）、特にその<span class="keyword">分布シフト項の数学的な形式</span>は頑健性を保ち、損失曲線の変動をよく説明できることが示されました。これは、提案手法が多様な実用的シナリオに対応できる可能性を示唆しています。</p>
</div>
</div>
<div class="section-card" id="G._Open-Source_Pre-Training_Models">
<h2 class="section-title"><i class="fas fa-box-open"></i> G. Open-Source Pre-Training Models</h2>
<div class="content-box">
<p>このセクションでは、より現実的なシナリオとして、事前学習（PT）モデルが<span class="keyword">オープンソース</span>のものであり、その正確な事前学習プロセスが不明であるケースについて議論します。このような場合、通常、以下の情報が未知となります。</p>
<ul class="unstyled-list">
<li>✏️ <span class="highlight">事前学習データセット（PTデータセット）の分布</span></li>
<li>📉 <span class="highlight">損失ポテンシャル (Loss Potential)</span></li>
<li>⏱️ <span class="highlight">事前学習の訓練量</span></li>
</ul>
<p>これらの未知の要素が、継続的事前学習（CPT）のスケーリング則を適用する上で課題となります。このセクションでは、これらの課題に対処するためのアプローチと、その有効性を検証する実験について解説します。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-question-circle"></i> PT訓練量と損失ポテンシャルの未知性</h3>
<div class="content-box">
<p>オープンソースモデルでは、多くの場合、事前学習（PT）にどれだけの計算資源（訓練量）が投入されたか、そしてモデルが持つ「<span class="keyword">損失ポテンシャル</span>」が不明です。これらが分からないと、以下の計算が困難になります。</p>
<div class="info-grid">
<div class="info-card glass-card">
<p><strong>PTフォワードエリア \(S_{1}^{pt}\)</strong>: 事前学習段階での学習率の総和。これが未知です。</p>
<div class="formula">
                    $$ S_{1}^{pt} = \sum_{i=1}^{t_{pt}} \eta_i^{pt} $$
                </div>
<p class="reference">（\(t_{pt}\)はPTのステップ数、\(\eta_i^{pt}\)はPTのiステップ目の学習率）</p>
</div>
<div class="info-card glass-card">
<p><strong>CPTアニーリングエリア \(S_{2}^{cpt}\) 計算に必要な最終学習率(LR)</strong>: 事前学習の最終学習率が不明なため、継続的事前学習（CPT）のアニーリング（学習率減衰）効果を正確に見積もる \(S_{2}^{cpt}\) の計算が難しくなります。</p>
<div class="formula">
                    $$ S_{2}^{cpt} = \sum_{i=1}^{t_{cpt}} \sum_{k=1}^{i} (\eta_{k-1}^{cpt} - \eta_{k}^{cpt}) \lambda^{i-k} $$
                </div>
<p class="reference">（これはCPT単独のアニーリングエリア。\(\eta_{final}^{pt}\) が \(S_2\) 全体の計算に影響）</p>
</div>
</div>
<div class="bubble-box">
<p><i class="fas fa-lightbulb"></i> <strong>対処法と考え方：</strong></p>
<ul>
<li><span class="keyword">PTフォワードエリア \(S_{1}^{pt}\)</span> については、これを<strong>フィッティング対象のパラメータ</strong>として扱います。つまり、CPTのデータから推定します。</li>
<li>多くのオープンソースモデルは、公開時のベンチマーク性能を最大化するために、学習率をゼロまたは非常に小さい値（最小LR）までアニーリング（減衰）させることが一般的です。そこで、大胆な仮定として「<strong>全てのオープンソースモデルの最終学習率はゼロである</strong>」とします。
                    <div class="note-box">
<p class="note-title"><i class="fas fa-exclamation-triangle"></i> 仮定のポイント</p>
                        この仮定により、PTフェーズのアニーリングエリア \(S_{2}^{pt}\) の計算や、CPTフェーズの学習率スケジュールの設計が簡略化され、特にCPTアニーリングエリア \(S_{2}^{cpt}\) の計算が容易になります。
                    </div>
</li>
<li>オープンソースモデルに対するCPTの学習率は、一度ゼロから特定の<span class="keyword">ピーク学習率</span>まで再ウォームアップし、その後、特定の学習率スケジュール（LRS）に従ってアニール（減衰）させると考えます。</li>
</ul>
</div>
<p class="reference">※ 本文中に図17のキャプションに似たテキスト「4.0 3.8 PT Seqlen=4K ... Step (a) $D_{pt}$ (FineWeb) loss curve ... (b) $D_{cpt}$ (Knowledge Pile) loss curve ...」が挿入されていますが、これは論文の編集上の混入と考えられ、このセクションの主題であるオープンソースモデルの文脈とは直接関連が薄いため、ここでは詳細な解説を省略します。主題は、未知の情報に対してどのようにアプローチし、提案手法（Eq. 4）を適用するかです。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-database"></i> PTデータセット分布の未知性</h3>
<div class="content-box">
<p>論文で提案されているスケーリング則（特にEq. 4）は、基本的には事前学習データセット（\(D_{pt}\)）と継続的事前学習データセット（\(D_{cpt}\)）の検証セットにおける損失曲線に対して成り立つものです。</p>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-search-minus"></i> 問題点</p>
            しかし、オープンソースモデルの場合、そのモデルが実際にどのような<span class="keyword">事前学習データセット（\(D_{pt}\)）</span>で学習されたのか、その正確な分布を知ることはできません。
        </div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-lightbulb"></i> 解決策アプローチ</p>
<p>この問題に対処するため、<span class="keyword">代理の \(D_{pt}\) (proxy \(D_{pt}\))</span> を使用するという戦略を取ります。具体的には、広く利用可能で、一般的な知識をカバーしていると考えられるオープンソースの <strong>Common Crawl由来の検証セット</strong>などを、真の \(D_{pt}\) の代わりとして選択します。</p>
<div style="text-align: center; margin: 20px 0;">
<div style="display: inline-block; padding: 10px; border: 2px dashed var(--color-primary); border-radius: 8px; background-color: rgba(74,111,165,0.05);">
<i class="fas fa-file-alt fa-2x" style="color: var(--color-primary);"></i>
<p style="margin-top: 5px; font-family: 'Yomogi', cursive;">真の \(D_{pt}\) (不明)</p>
</div>
<i class="fas fa-arrow-right fa-2x" style="margin: 0 20px; color: var(--color-accent1);"></i>
<div style="display: inline-block; padding: 10px; border: 2px dashed var(--color-accent1); border-radius: 8px; background-color: rgba(92,184,92,0.05);">
<i class="fas fa-file-invoice fa-2x" style="color: var(--color-accent1);"></i>
<p style="margin-top: 5px; font-family: 'Yomogi', cursive;">代理 \(D_{pt}\) (例: Common Crawl)</p>
</div>
</div>
<p>この代理 \(D_{pt}\) を用いて、モデルの一般ドメインにおける性能変化を追跡します。</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-flask"></i> 仮説の検証実験</h3>
<div class="content-box">
<p>上記の2つの仮説（PT訓練量/損失ポテンシャルの扱いや代理 \(D_{pt}\) の利用）が妥当であるかを検証するために、以下の実験が行われました。</p>
<div class="definition-box" style="margin-bottom: 20px;">
<p class="definition-title"><i class="fas fa-bullseye"></i> 実験の共通目標</p>
<p>提案されているスケーリング則（Eq. 4）が、PT情報が不完全なオープンソースモデルに対しても、CPT中の損失を効果的に予測できるかを確認する。</p>
<p class="formula">$$ L(t) = \underbrace{L_0 + A \cdot (S_1^{pt} + S_1^{cpt})^{-\alpha} - C_1 \cdot S_2^{pt} - C_2 \cdot S_2^{cpt}}_{\text{LRアニーリングを伴うスケーリング則}} + \underbrace{B \cdot (1 - (1 + E \cdot S_1^{cpt})^{-\beta})}_{\text{べき乗則の分布シフト}} $$</p>
<p class="reference">Eq. 4: CPTの損失関数。\(S_1^{pt}\)などをフィッティングパラメータとして扱う。</p>
</div>
<p><strong>実験の詳細:</strong></p>
<p>少数の訓練ステップでパラメータをフィッティングし、その後、より長いステップでの代理 \(D_{pt}\) と真の \(D_{cpt}\) の損失を予測します。</p>
<div class="two-column">
<div class="column">
<div class="info-card glass-card">
<p class="note-title"><i class="fas fa-microscope"></i> 実験1: LLaMA3.2-1B を用いた検証</p>
<ul>
<li><strong>ベースモデル:</strong> <span class="badge blue">LLaMA3.2-1B</span> (Dubey et al., 2024)</li>
<li><strong>CPTデータセット (\(D_{cpt}\)):</strong> <span class="badge orange">Pile-of-Law</span> (Henderson* et al., 2022)</li>
<li><strong>代理PTデータセット (Proxy \(D_{pt}\)):</strong> <span class="badge purple">RedPajama</span> (Computer, 2023) データセット内の <span class="badge yellow">C4</span> 部分</li>
</ul>
</div>
</div>
<div class="column">
<div class="info-card glass-card">
<p class="note-title"><i class="fas fa-atom"></i> 実験2: 自社モデルを未知と仮定した検証</p>
<ul>
<li><strong>ベースモデル:</strong> FineWebで事前学習した自社モデル（PT情報を未知と見なす）</li>
<li><strong>CPTデータセット (\(D_{cpt}\)):</strong> <span class="badge orange">Pile-of-Law</span></li>
<li><strong>代理PTデータセット (Proxy \(D_{pt}\)):</strong> <span class="badge purple">RedPajama</span> データセット内の <span class="badge yellow">C4</span> 部分</li>
</ul>
</div>
</div>
</div>
<p><i class="fas fa-chart-line"></i> <strong>結果のポイント (Fig. 18参照):</strong></p>
<p>論文中のFig. 18で示されているように、提案手法（Eq. 4と上記の仮定）を用いることで、代理 \(D_{pt}\) と真の \(D_{cpt}\) の両方に対して、将来の損失を効果的に予測できたと報告されています。</p>
<div class="pipeline">
<div class="pipeline-step">
<span class="step-number">1</span>
<div class="step-content"><strong>モデル選択:</strong> オープンソースモデル (LLaMA3.2-1B or 自社モデル)</div>
</div>
<div class="pipeline-step">
<span class="step-number">2</span>
<div class="step-content"><strong>代理 \(D_{pt}\) 設定:</strong> RedPajama-C4 を Proxy \(D_{pt}\) とする</div>
</div>
<div class="pipeline-step">
<span class="step-number">3</span>
<div class="step-content"><strong>CPTデータ (\(D_{cpt}\)) 準備:</strong> Pile-of-Law</div>
</div>
<div class="pipeline-step">
<span class="step-number">4</span>
<div class="step-content"><strong>初期ステップでフィッティング:</strong> Eq. 4 の未知パラメータ（\(S_1^{pt}\)など）を推定</div>
</div>
<div class="pipeline-step">
<span class="step-number">5</span>
<div class="step-content"><strong>長期ステップの損失予測:</strong> フィットしたEq. 4 を用いて、Proxy \(D_{pt}\) と \(D_{cpt}\) の損失を予測</div>
</div>
</div>
<img alt="Fig. 18 (a) Model I Fitted vs Predicted LRS" src="fitted_vs_predicted_lrs.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (a) モデルI (LLaMA3.2-1B) のフィッティング用と予測用の学習率スケジュール</p>
<p>この図は、LLaMA3.2-1Bモデル（モデルI）の実験における学習率スケジュールを示しています。青線がパラメータフィッティングに使用した学習期間の学習率（Fitted LRS）、オレンジ線がその後の予測期間の学習率（Predict LRS）です。Fitted LRSは初期の短いステップで、Predict LRSはより長期のステップにわたっています。</p>
<img alt="Fig. 18 (b) Model I Fitted RedPajama-C4 Loss" src="redpajama_c4_loss_vs_step.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (b) モデルI のRedPajama-C4（代理 \(D_{pt}\)）におけるフィッティングされた損失曲線</p>
<p>LLaMA3.2-1Bモデル（モデルI）をRedPajama-C4（代理 \(D_{pt}\)）データセットでCPTした際の損失曲線です。赤色のバツ印が実際の損失値（Ground Truth Loss）で、青色の破線がEq. 4を使ってフィッティングされた損失曲線（Fitted loss curve）です。初期の短い訓練ステップ（ここでは約5000ステップまで）で非常によく一致していることが分かります。</p>
<img alt="Fig. 18 (c) Model I Predicted RedPajama-C4 Loss" src="redpajama_c4_loss_vs_step_cpt.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (c) モデルI のRedPajama-C4（代理 \(D_{pt}\)）における予測された損失曲線</p>
<p>Fig. 18 (b)でフィッティングしたEq. 4を用いて、LLaMA3.2-1Bモデル（モデルI）のRedPajama-C4（代理 \(D_{pt}\)）における、より長期の訓練ステップ（ここでは約20000ステップまで）での損失を予測した結果です。紫色のバツ印が実際の損失値（Ground Truth Loss）、オレンジ色の破線が予測された損失曲線（Predicted loss curve）です。予測も実際の損失の傾向をよく捉えていることが示されています。</p>
<img alt="Fig. 18 (d) Model II Fitted vs Predicted LRS" src="fitted_predicted_lrs_comparison.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (d) モデルII (自社モデル) のフィッティング用と予測用の学習率スケジュール</p>
<p>これは、FineWebで事前学習した自社モデル（モデルII）を未知情報を持つモデルとして扱った場合の学習率スケジュールです。同様に、青線がフィッティング用、オレンジ線が予測用です。モデルIと比較して、予測期間がやや短め（約20000ステップまで）に設定されています。</p>
<img alt="Fig. 18 (e) Model II Fitted RedPajama-C4 Loss" src="redpajama_c4_loss_vs_step_proxy.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (e) モデルII のRedPajama-C4（代理 \(D_{pt}\)）におけるフィッティングされた損失曲線</p>
<p>自社モデル（モデルII）をRedPajama-C4（代理 \(D_{pt}\)）データセットでCPTした際の損失曲線です。赤色のバツ印が実際の損失値、青色の破線がEq. 4でフィッティングされた曲線です。こちらは約10000ステップまでのデータでフィッティングされています。やはりよく一致しています。</p>
<!-- 図 (f) は論文内で redpajama_c4_loss_vs_step_cpt_proxy.jpg となっているが、図のタイトルと内容から、(e) と同じ図に見える。論文のFig.18の構成から、(f) はモデルIIの代理Dpt予測であるべき。ファイル名が _proxy となっているため、ここではファイル名に従い掲載するが、論文中の意図と異なる可能性に留意。便宜上、キャプションは予測として記述する。 -->
<img alt="Fig. 18 (f) Model II Predicted RedPajama-C4 Loss" src="redpajama_c4_loss_vs_step_cpt_proxy.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (f) モデルII のRedPajama-C4（代理 \(D_{pt}\)）における予測された損失曲線</p>
<p>Fig. 18 (e)でフィッティングしたEq. 4を用いて、自社モデル（モデルII）のRedPajama-C4（代理 \(D_{pt}\)）における、より長期の訓練ステップでの損失を予測した結果です。紫色のバツ印が実際の損失値、オレンジ色の破線が予測された損失曲線です。図の見た目は(e)と酷似していますが、論文の構成上、これは予測結果を示すものと考えられます。<strong>(注意: 元画像ファイル名が `_proxy` となっており、`_cpt_proxy` であれば予測を示す可能性が高いが、ここでは提供されたファイル名に従う。)</strong></p>
<img alt="Fig. 18 (g) Model I Fitted vs Predicted LRS for Pile-of-Law" src="fitted_predicted_learning_rate_schedules.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (g) モデルI (LLaMA3.2-1B) のPile-of-Law (\(D_{cpt}\)) に対するフィッティング用と予測用の学習率スケジュール</p>
<p>モデルI (LLaMA3.2-1B) を用いて、真のCPTデータセットであるPile-of-Lawに対する損失を予測する際の学習率スケジュールです。青線がフィッティング用、オレンジ線が予測用で、(a) と同じものです。</p>
<img alt="Fig. 18 (h) Model I Fitted Pile-of-Law Loss" src="pile_of_law_loss_vs_step.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (h) モデルI のPile-of-Law (\(D_{cpt}\)) におけるフィッティングされた損失曲線</p>
<p>LLaMA3.2-1Bモデル（モデルI）をPile-of-Law (\(D_{cpt}\)) データセットでCPTした際の損失曲線（フィッティング）。赤色のバツ印が実際の損失値、青色の破線がEq. 4でフィッティングされた曲線です。約5000ステップまでのデータでフィッティングされ、非常に良い一致を示しています。</p>
<img alt="Fig. 18 (i) Model I Predicted Pile-of-Law Loss" src="pile_of_law_loss_vs_step_cpt.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (i) モデルI のPile-of-Law (\(D_{cpt}\)) における予測された損失曲線</p>
<p>Fig. 18 (h)でフィッティングしたEq. 4を用いて、LLaMA3.2-1Bモデル（モデルI）のPile-of-Law (\(D_{cpt}\)) における、より長期の訓練ステップ（約20000ステップまで）での損失を予測した結果です。紫色のバツ印が実際の損失値、オレンジ色の破線が予測曲線です。予測精度が高いことがわかります。</p>
<!-- 図 (j) は論文内で fitted_vs_predicted_learning_rate_schedules.jpg となっているが、Fig.18の構成から、モデルIIのPile-of-Lawに対するLRSであるべき。ファイル名がモデルIのものと重複しているが、ここでは(d)と同じものとして解釈し、モデルII用としてキャプションを記述する。 -->
<img alt="Fig. 18 (j) Model II Fitted vs Predicted LRS for Pile-of-Law" src="fitted_predicted_learning_rate_schedules.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (j) モデルII (自社モデル) のPile-of-Law (\(D_{cpt}\)) に対するフィッティング用と予測用の学習率スケジュール</p>
<p>自社モデル（モデルII）をPile-of-Law (\(D_{cpt}\)) データセットでCPTする際の学習率スケジュールです。(d)と同じものが使われていると解釈します。</p>
<img alt="Fig. 18 (k) Model II Fitted Pile-of-Law Loss" src="pile_of_law_loss_vs_step_cpt_proxy.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (k) モデルII のPile-of-Law (\(D_{cpt}\)) におけるフィッティングされた損失曲線</p>
<p>自社モデル（モデルII）をPile-of-Law (\(D_{cpt}\)) データセットでCPTした際の損失曲線（フィッティング）。赤色のバツ印が実際の損失値、青色の破線がEq. 4でフィッティングされた曲線です。約10000ステップまでのデータでフィッティングされています。こちらも良い一致を示しています。<strong>(注意: 元画像ファイル名が `_cpt_proxy` となっており、意図としては(k) Fitted, (l) Predictedである可能性が高いが、提供されたファイル名と論文の図の順番に基づき、これをFittedとして解釈)</strong></p>
<!-- 図 (l) は論文内で pile_of_law_loss_vs_step_cpt_proxy.jpg となっているが、Fig.18の構成から、モデルIIのPile-of-Lawの予測であるべき。ファイル名が(k)と同じになっている。ここでは(k)の予測版としてキャプションを記述する。 -->
<img alt="Fig. 18 (l) Model II Predicted Pile-of-Law Loss" src="pile_of_law_loss_vs_step_cpt_proxy.jpg"/>
<p class="reference" style="text-align:center;">Fig. 18 (l) モデルII のPile-of-Law (\(D_{cpt}\)) における予測された損失曲線</p>
<p>Fig. 18 (k)でフィッティングしたEq. 4を用いて、自社モデル（モデルII）のPile-of-Law (\(D_{cpt}\)) における、より長期の訓練ステップ（ファイル名は(k)と同じだが、論文の構成上、約50000ステップまでの予測を示すと解釈）での損失を予測した結果です。紫色のバツ印が実際の損失値、オレンジ色の破線が予測曲線です。<strong>(注意: 提供された画像ファイル名は (k) と同じ `pile_of_law_loss_vs_step_cpt_proxy.jpg` ですが、Fig.18の(l)に該当する図として解釈し、予測結果として説明します。実際の論文では異なる図が使用されている可能性があります。)</strong></p>
<div class="note-box" style="margin-top: 20px;">
<p class="note-title"><i class="fas fa-check-circle"></i> 結論</p>
            これらの実験結果は、提案されたアプローチ（未知のPT情報をパラメータとしてフィッティングする、代理の \(D_{pt}\) を利用する）が、オープンソースモデルのようなPT情報が不完全な場合でも、CPTの学習ダイナミクスを記述し、<span class="keyword">ハイパーパラメータ最適化</span>を行う上で有効であることを示唆しています。
        </div>
</div>
</div>
<div class="section-card" id="H._Adding_Replay_Ratio_to_Our_Formulation">
<h2 class="section-title"><i class="fas fa-cogs"></i> H. Adding Replay Ratio to Our Formulation</h2>
<p>このセクションでは、これまでのCPT（Continual Pre-Training：連続事前学習）の定式化に、<span class="keyword">データ混合比率（リプレイ比率）</span>という重要な要素を組み込む試みについて解説します。これにより、CPT中の学習ダイナミクスをより深く、そして統一的に理解することを目指します。</p>
<div class="glass-card">
<p>🎯 <strong>このセクションの主な目的と論旨:</strong></p>
<ul class="unstyled-list">
<li><i class="fas fa-layer-group"></i> 先行研究である <span class="keyword">D-CPT law (Que et al., 2024)</span> は、$D_{pt}$（事前学習データ）と $D_{cpt}$（連続事前学習データ）の混合比率を考慮したスケーリング則を提案しました。本研究は、この考え方を発展させ、我々自身の定式化にデータ混合比率を統合します。</li>
<li><i class="fas fa-link"></i> これまでの研究（付録Dで言及）では、データ比率が異なると、損失曲線はそれぞれ異なる方程式で個別にフィッティングできることが示されていました。しかし、このセクションでは、単一の<span class="highlight">統一的な数式</span>で、様々なデータ混合比率下の全ての損失曲線を表現することを目指します。</li>
<li><i class="fas fa-exchange-alt"></i> リプレイ比率が「分布シフト項」と「学習率（LR）アニーリング項」の両方に影響を与えること、特に<span class="keyword">指数関数的な形式</span>でこの影響をモデル化することが有効であることを見出します。これは <span class="keyword">Data Mixing Law (Ye et al., 2024)</span> の知見とも整合します。</li>
</ul>
</div>
<h3 class="subsection-title"><i class="fas fa-random"></i> 🧩 リプレイ比率が学習ダイナミクスに与える影響</h3>
<p>データ混合比率、すなわち<span class="keyword">リプレイ比率</span>（どれだけ元の事前学習データ $D_{pt}$ を再利用するか）は、CPTの挙動に大きな影響を与えます。具体的には、主に以下の2つの要素が影響を受けます。</p>
<div class="info-grid" style="grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));">
<div class="info-card">
<h4><i class="fas fa-arrows-alt-h"></i> 1. 分布シフト項 (Distribution Shift Term)</h4>
<p>📌 $D_{pt}$ の比率が高い（つまり、元の事前学習データが多くリプレイされる）ほど、新しいデータ $D_{cpt}$ への分布のズレは<span class="highlight">弱く</span>なります。</p>
<div style="text-align: center; margin-top: 15px;">
<span class="badge blue">$D_{pt}$ 比率 <i class="fas fa-arrow-up"></i></span> <i class="fas fa-long-arrow-alt-right fa-2x" style="color: var(--color-primary); vertical-align: middle;"></i> <span class="badge green">分布シフト <i class="fas fa-arrow-down"></i></span>
</div>
</div>
<div class="info-card">
<h4><i class="fas fa-chart-line"></i> 2. 学習率（LR）アニーリング項 (LR Annealing Term)</h4>
<p>📌 $D_{pt}$ の比率が高い場合：</p>
<ul>
<li>$D_{cpt}$ 検証損失におけるLRアニーリング項は<span class="highlight">小さく</span>なります。</li>
<li>逆に、$D_{pt}$ 検証損失におけるLRアニーリング項は<span class="highlight">大きく</span>なります。</li>
</ul>
<div style="text-align: center; margin-top: 10px;">
<div style="margin-bottom: 5px;">
<span class="badge blue">$D_{pt}$ 比率 <i class="fas fa-arrow-up"></i></span> <i class="fas fa-long-arrow-alt-right fa-2x" style="color: var(--color-primary); vertical-align: middle;"></i> <span class="badge orange">$D_{cpt}$ 検証損失のLRアニーリング項 <i class="fas fa-arrow-down"></i></span>
</div>
<div>
<span class="badge blue">$D_{pt}$ 比率 <i class="fas fa-arrow-up"></i></span> <i class="fas fa-long-arrow-alt-right fa-2x" style="color: var(--color-primary); vertical-align: middle;"></i> <span class="badge purple">$D_{pt}$ 検証損失のLRアニーリング項 <i class="fas fa-arrow-up"></i></span>
</div>
</div>
</div>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-lightbulb"></i> 発見と方針：指数関数形式の採用</p>
<p>これらのリプレイ比率による影響を最も効果的に捉えることができるのは、<span class="keyword">指数関数形式 (exponential form)</span> であることが実験的に確認されました。この選択は、<span class="keyword">Data Mixing Law (Ye et al., 2024)</span> という別の研究で見られる傾向とも一致しており、理論的な裏付けも期待できます。</p>
<p>そこで、この指数関数形式を、元々の定式化における「分布シフト項」と「LRアニーリング項」の両方に組み込むことにしました。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-calculator"></i> 📝 リプレイ比率を組み込んだ新しい定式化</h3>
<p>上記の考察に基づき、リプレイ比率を考慮した新しい損失関数 $L_{pt}$ (PTデータでの損失) と $\mathfrak{L}_{cpt}$ (CPTデータでの損失) を以下のように定義します。</p>
<div class="formula">
<p><strong>事前学習データ ($D_{pt}$) 検証損失:</strong></p>
<mjx-container class="MathJax" jax="CHTML">
            $$
            L_{pt} = L_0 + A \cdot (S_1^{pt} + S_1^{cpt})^{-\alpha} - C \cdot S_2^{pt} - D \cdot S_2^{cpt} e^{a_1 r_{pt}} + B \cdot \left(1 - (1 + E \cdot S_1^{cpt})^{-\beta}\right) \left(1 - e^{-a_2 r_{pt}}\right)
            $$
        </mjx-container>
<p style="margin-top: 15px;"><strong>連続事前学習データ ($D_{cpt}$) 検証損失:</strong></p>
<mjx-container class="MathJax" jax="CHTML">
            $$
            \mathfrak{L}_{cpt} = L_0 + A \cdot (S_1^{pt} + S_1^{cpt})^{-\alpha} - C \cdot S_2^{pt} - D \cdot S_2^{cpt} e^{a_1 r_{cpt}} + B \cdot \left(1 - (1 + E \cdot S_1^{cpt})^{-\beta}\right) \left(e^{a_2 r_{cpt}}\right)
            $$
        </mjx-container>
</div>
<div class="definition-box">
<p class="definition-title"><i class="fas fa-book-open"></i> 数式中の各項目の解説</p>
<ul>
<li><span class="keyword">$L_0, A, C, D, B, E, \alpha, \beta$</span>: これらは元々の定式化に含まれる定数パラメータです。</li>
<li><span class="keyword">$S_1^{pt}, S_1^{cpt}$</span>: それぞれ事前学習段階、連続事前学習段階での<span class="highlight">前方領域 (summed LR)</span> を表します。学習率の総和です。</li>
<li><span class="keyword">$S_2^{pt}, S_2^{cpt}$</span>: それぞれ事前学習段階、連続事前学習段階での<span class="highlight">アニーリング領域 (annealing area)</span> を表します。学習率の減衰具合を反映します。</li>
<li><span class="keyword">$r_{pt}$</span>: 事前学習データ ($D_{pt}$) の混合比率です。</li>
<li><span class="keyword">$r_{cpt}$</span>: 連続事前学習データ ($D_{cpt}$) の混合比率です。</li>
<li><span class="keyword">$r_{pt} + r_{cpt} = 1$</span>: $D_{pt}$ と $D_{cpt}$ の混合比率の合計は1になります。</li>
<li><span class="keyword">$a_1, a_2$</span>: リプレイ比率の影響を調整するための新たに追加されたパラメータです。</li>
</ul>
</div>
<div class="bubble-box">
<p>💡 <strong>分布シフト項の修正ポイント:</strong></p>
<p>重要な点として、分布シフト項（上記の数式では $B \cdot (\dots)(\dots)$ の部分）における指数関数的な定式化は、<span class="highlight">$r_{cpt}$ がゼロの場合（つまり、$D_{cpt}$ データが全く使われず、$D_{pt}$ データのみで学習する場合）に分布シフトがゼロになる</span>ように修正されています。これは直感的にも理にかなっており、$D_{cpt}$ がなければ分布シフトは起こり得ないからです。</p>
<p>$L_{pt}$ の式の分布シフト項の最後の因子が $(1 - e^{-a_2 r_{pt}})$ となっているのはこのためです。$r_{pt} = 1$ (つまり $r_{cpt} = 0$) のとき、$e^{-a_2 r_{pt}}$ は $e^{-a_2}$ となり、0にはなりません。一方で $r_{cpt}$ の式では $(e^{a_2 r_{cpt}})$ となっており、$r_{cpt}=0$ のとき、この項は $e^0 = 1$ となります。
        論文の記述では「$r_{cpt}$がゼロの場合に分布シフト項がゼロになるように」とありますが、数式を見ると $r_{pt}$ の影響を表す項 $ (1 - e ^ { - a _ { 2 } r _ { p t } } ) $ と、$r_{cpt}$ の影響を表す項 $ (e ^ { a _ { 2 } r _ { c p t } } ) $ が別々に存在します。
        $L_{pt}$ における分布シフト項が $r_{cpt}=0$ (つまり $r_{pt}=1$) の時にゼロになるためには、$(1 - e^{-a_2 r_{pt}})$ の部分ではなく、この項全体がゼロになるようなメカニズムが必要です。
        しかし、本文の記述と数式から最も素直に解釈すると、$r_{cpt}=0$ の時に分布シフトの影響がなくなるように、特に $D_{pt}$側の損失に対する分布シフトの表現 $(1 - e^{-a_2 r_{pt}})$ が調整されていると考えるのが自然です。 $r_{pt}=0$ であれば $(1-e^0) = 0$ となり、分布シフトの影響は消えます。
        一方で、$D_{cpt}$側の損失 $\mathfrak{L}_{cpt}$ の分布シフト項の因子は $e^{a_2 r_{cpt}}$ であり、$r_{cpt}=0$ のとき $e^0 = 1$ となります。この場合、分布シフト項がゼロになるのではなく、何らかの基準値に影響を与えます。
        この記述は、分布シフトの概念そのものが「$D_{pt}$から$D_{cpt}$への移行」に伴う変化を指すため、$D_{cpt}$ の割合がゼロであれば、その「移行」が発生しない、という文脈で理解するのが適切かもしれません。
        </p>
</div>
<h3 class="subsection-title"><i class="fas fa-chart-bar"></i> 📊 この新しい定式化の有効性</h3>
<p>このリプレイ比率を組み込んだ新しい数式の有効性は、論文中の <span class="keyword">図19 (Fig. 19)</span> で示されています。この図は、異なるリプレイ比率における学習ダイナミクス全体を、提案された単一の数式でうまく表現できることを視覚的に示しています。</p>
<img alt="図19：異なるリプレイ比率での損失曲線" src="loss_curves_replay_ratio.jpg"/>
<div class="note-box">
<p class="note-title"><i class="fas fa-eye"></i> 図19の解説：リプレイ比率と損失曲線の関係</p>
<p>図19は、横軸に学習ステップ数 (Step)、縦軸に検証損失 (Validation Loss) を取っています。左側の (a) が $D_{pt}$ 検証損失、右側の (b) が $D_{cpt}$ 検証損失です。</p>
<ul>
<li><span class="highlight">凡例 (Legend)</span>:
                <ul>
<li>紫色の×印 (<span style="color: purple;">x</span>): <span class="keyword">CPT Truth Loss</span> (実際のCPT実験で観測された真の損失値)</li>
<li>オレンジ色の破線: <span class="keyword">KP(100%) Fitted Loss</span> ($D_{cpt}$ (Knowledge Pile) のデータが100%の場合のフィットされた損失曲線)</li>
<li>緑色の破線: <span class="keyword">KP(67%) Fitted Loss</span> ($D_{cpt}$ が67%、$D_{pt}$ が33%の混合比の場合)</li>
<li>赤色の破線: <span class="keyword">KP(50%) Fitted Loss</span> ($D_{cpt}$ が50%、$D_{pt}$ が50%の混合比の場合)</li>
<li>紫色の破線: <span class="keyword">KP(33%) Fitted Loss</span> ($D_{cpt}$ が33%、$D_{pt}$ が67%の混合比の場合)</li>
<li>茶色の破線: <span class="keyword">KP(0%) Fitted Loss</span> ($D_{cpt}$ が0%、つまり$D_{pt}$ データのみの場合)</li>
</ul>
</li>
<li><span class="highlight">図(a) $D_{pt}$ Validation Loss</span>:
                <ul>
<li>$D_{pt}$ の割合が高いほど（茶色線 KP(0%) に近いほど）、$D_{pt}$ の損失は低く抑えられます。</li>
<li>逆に、$D_{cpt}$ の割合が高いほど（オレンジ線 KP(100%) に近いほど）、CPT開始後に $D_{pt}$ の損失が急激に上昇（悪化）する傾向が見られます。これは<span class="keyword">分布シフト</span>が大きいためです。</li>
<li>提案手法（破線）は、様々なリプレイ比率における実際の損失（×印）のトレンドをよく捉えています。</li>
</ul>
</li>
<li><span class="highlight">図(b) $D_{cpt}$ Validation Loss</span>:
                <ul>
<li>$D_{cpt}$ の割合が高いほど（オレンジ線 KP(100%) に近いほど）、$D_{cpt}$ の損失はより速く、より低く減少します。</li>
<li>$D_{pt}$ の割合が高いと（茶色線 KP(0%) に近いほど）、$D_{cpt}$ の学習は進まず、損失は高いままです。これは当然の結果です。</li>
<li>こちらも、提案手法（破線）が実際の損失（×印）を様々なリプレイ比率で精度良くフィッティングできていることが分かります。</li>
</ul>
</li>
</ul>
<p>この図が示す重要な点は、異なるリプレイ比率に対して個別の数式を用意するのではなく、<span class="highlight">単一の数式フレームワーク</span>（パラメータ $a_1, a_2$ とリプレイ比率 $r_{pt}, r_{cpt}$ を含む）で、これらすべての曲線を同時に記述できるという点です。</p>
</div>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-trophy"></i> 本手法の優位性</p>
<p>先行研究である <span class="keyword">D-CPT law (Que et al., 2024)</span> は、異なるリプレイ比率における<span class="highlight">最終的な損失</span>を予測することに主眼を置いていました。しかし、我々の提案する手法は、最終的な損失だけでなく、様々なリプレイ比率における<span class="keyword">訓練ダイナミクス全体</span>（つまり、訓練中の各ステップでの損失の変化）を記述することができます。</p>
<p>これは、CPTプロセスのより詳細な理解と、より柔軟なハイパーパラメータ調整を可能にする点で、大きな進歩と言えます。</p>
</div>
<div class="framework-box">
<p class="framework-title"><i class="fas fa-sitemap"></i> まとめ：リプレイ比率の統合による包括的な理解</p>
<p>このセクションでは、CPTの学習ダイナミクスを記述する既存の定式化に、データのリプレイ比率を指数関数形式で組み込みました。これにより、</p>
<ul class="unstyled-list">
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 単一の数式で、様々なリプレイ比率における損失曲線を表現可能に。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> リプレイ比率が分布シフトとLRアニーリングに与える影響を定量的にモデル化。</li>
<li><i class="fas fa-check-circle" style="color: var(--color-accent1);"></i> 訓練プロセス全体のダイナミクスを捉えることで、CPTのより深い理解と制御に貢献。</li>
</ul>
<p>という成果が得られました。これは、LLMの連続事前学習におけるより精密な設計と最適化への道を開くものです。</p>
</div>
</div>
[Error]: 3回連続で空の出力が返ってきました。
<div class="section-card" id="J._Out-of-Domain_Validation_Set">
<h2 class="section-title"><i class="fas fa-puzzle-piece"></i> J. Out-of-Domain Validation Set</h2>
<div class="glass-card">
<p>📌 このセクションでは、論文の主要な関心事の一つである継続的学習（CPT）において、<strong>事前学習データセット（<span class="keyword">$D_{pt}$</span>）</strong>と<strong>継続的学習データセット（<span class="keyword">$D_{cpt}$</span>）</strong>の検証損失を<span class="highlight">組み合わせる</span>ことで、それらとは異なる<strong>未知のドメイン（Out-of-Domain: <span class="keyword">$D_{ood}$</span>）</strong>の検証損失をどの程度予測できるか、という興味深い問いを探求しています。</p>
<p>特に注目しているのは、<span class="keyword">$D_{pt}$</span> と <span class="keyword">$D_{cpt}$</span> の検証損失の単純な「<span class="highlight">線形結合</span>」（重み付きの足し算）によって、$D_{ood}$ の検証損失を近似できるかどうかです。このアプローチの可能性と、それが常に成り立つわけではない限界点について、実験を通して明らかにしようとしています。これが成功すれば、モデルが新しい、まだ見ぬドメインにどれだけ上手く対応できるかを、ある程度事前に見積もるための一助となるでしょう。📐</p>
</div>
<h3 class="subsection-title"><i class="fas fa-book-open"></i> 背景となる重要な考え方</h3>
<div class="info-grid">
<div class="info-card">
<h4><i class="fas fa-project-diagram"></i> Data Mixing Laws (Ye et al., 2024)</h4>
<p>このセクションの研究アイデアの背景には、Yeらが2024年に提唱した「<span class="keyword">データ混合の法則 (Data mixing laws)</span>」という考え方があります。これは、複数の異なるデータソース（ドメイン）を混ぜて言語モデルを学習させる際に、<span class="highlight">ある特定のドメインにおけるモデルの性能（例えば検証損失）が、学習に使った他のドメインでの性能の組み合わせによって、ある程度説明できる</span>、というものです。</p>
<div class="bubble-box">
<p>🗣️ 平たく言うと、「もしドメインAのデータとドメインBのデータでモデルを鍛えたら、ドメインCのデータに対するモデルの賢さは、Aでの賢さとBでの賢さを上手いこと足したり引いたりすれば予測できるんじゃない？」というようなイメージです。</p>
</div>
</div>
<div class="info-card">
<h4><i class="fas fa-lightbulb"></i> 線形結合による推定のアイデア</h4>
<p>上記の「データ混合の法則」から着想を得て、この論文では <span class="keyword">$D_{ood}$</span> (Out-of-Domain) データセットにおける検証損失 $L_{D_{ood}}$ を、$D_{pt}$ (事前学習) データセットの検証損失 $L_{D_{pt}}$ と $D_{cpt}$ (継続的学習) データセットの検証損失 $L_{D_{cpt}}$ の<span class="highlight">線形結合</span>で推定するという仮説を検証しています。</p>
<div class="formula">
<p>推定式のイメージ: \(L_{D_{ood}} \approx w_1 L_{D_{pt}} + w_2 L_{D_{cpt}}\)</p>
</div>
<p>ここで、\(w_1\) と \(w_2\) は、各データセットの損失が $D_{ood}$ の損失にどれだけ影響するかを示す「重み係数」です。もしこの単純な関係が成り立つなら、実際に $D_{ood}$ で大規模な評価を行わなくても、ある程度その性能を予測できる道が開けるかもしれません。🧪</p>
</div>
</div>
<h3 class="subsection-title"><i class="fas fa-tags"></i> このセクションで鍵となる用語の確認</h3>
<div class="feature-card-grid">
<div class="feature-item">
<i class="fas fa-database"></i>
<h4><span class="keyword">$D_{pt}$</span> (Pre-training Dataset)</h4>
<p>モデルの初期学習（事前学習）に使用される、一般的で非常に大規模なテキストデータセットを指します。この論文の実験では、<span class="badge blue">FineWeb</span> がこの役割を担っています。</p>
</div>
<div class="feature-item">
<i class="fas fa-cogs"></i>
<h4><span class="keyword">$D_{cpt}$</span> (Continual Pre-training Dataset)</h4>
<p>事前学習済みのモデルを特定のドメインやタスクにさらに適応させるための継続的学習に使用されるデータセットです。ここでは、<span class="badge orange">Knowledge-Pile</span> が用いられています。</p>
</div>
<div class="feature-item">
<i class="fas fa-globe-americas"></i>
<h4><span class="keyword">$D_{ood}$</span> (Out-of-Domain Validation Set)</h4>
<p>$D_{pt}$ や $D_{cpt}$ とは異なる、未知のドメインの検証用データセットです。モデルが学習データに含まれていない新しい種類のデータに対してどれだけ汎化できるかを評価するために重要です。</p>
</div>
</div>
<div class="note-box">
<p class="note-title"><i class="fas fa-exclamation-triangle"></i> 重要な注意点</p>
<p>著者らは、この線形結合による近似アプローチが<span class="highlight">全ての状況でうまくいくわけではない</span>、という点も明確に指摘しています。特に、$D_{cpt}$ が非常にニッチで特殊なドメインのデータである場合や、$D_{ood}$ の特性が $D_{pt}$ や $D_{cpt}$ と大きくかけ離れている場合には、このような単純な近似は成り立たない可能性が高いです。あくまで、「<span class="highlight">$D_{cpt}$ が極端にドメイン特化していないようなシナリオでは、有望な近似手段となり得る</span>」という、条件付きの主張であると理解することが大切です。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-flask"></i> 検証実験のセットアップ</h3>
<div class="framework-box">
<p class="framework-title">実験設定の概要 🛠️</p>
<p>この仮説を検証するために、以下のデータセットが用いられました：</p>
<ul class="unstyled-list">
<li><span class="badge blue">$D_{pt}$ (事前学習データセット)</span>: <strong class="keyword">FineWeb</strong> (Penedo et al., 2024) - 広範なウェブテキストから構成されるデータセット。</li>
<li><span class="badge orange">$D_{cpt}$ (継続的学習データセット)</span>: <strong class="keyword">Knowledge-Pile</strong> (Fei et al., 2024) - 知識集約型のデータセット。</li>
</ul>
<p>そして、これらのデータセットでの検証損失を用いて、以下の多種多様な <span class="keyword">$D_{ood}$</span> (Out-of-Domain) データセットの検証損失を推定できるかがテストされました：</p>
<div class="tag-list">
<span class="tag">StackExchange (RedPajama)</span>
<span class="tag">arXiv (RedPajama)</span>
<span class="tag">C4 (RedPajama)</span>
<span class="tag">PhilPapers (Pile)</span>
<span class="tag">Books (Pile)</span>
<span class="tag">SlimPajama</span>
<span class="tag">Open-Web-Math</span>
</div>
<p>これらのデータセットは、質問応答サイト、学術論文、一般的なウェブクロール、哲学論文、書籍、数学関連のテキストなど、非常に幅広いドメインをカバーしています。この多様性が、仮説の一般性を試す上で重要となります。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-chart-line"></i> 図21・図22の解説: 線形結合による推定結果の可視化</h3>
<p>論文では、この線形結合による $D_{ood}$ 検証損失の推定がどの程度うまくいくかを、図21と図22で視覚的に示しています。</p>
<div class="glass-card">
<h4 class="subsection-title"><i class="fas fa-image"></i> 図21: $D_{ood}$ 検証損失の線形結合による表現</h4>
<p>📊 この図は、様々な $D_{ood}$ データセットについて、実際の検証損失（青い点）と、$D_{pt}$ および $D_{cpt}$ の検証損失の線形結合によって予測された損失（オレンジ色の線）をプロットしたものです。</p>
<img alt="Figure 21: Linear combination of D_pt and D_cpt to represent D_ood validation loss" src="ood_validation_loss_linear_combination.jpg"/>
<div class="note-box">
<p class="note-title"><i class="fas fa-search"></i> 図の見方・ポイント</p>
<ul class="unstyled-list">
<li>✏️ 各サブプロットが一つの $D_{ood}$ データセットに対応しています。</li>
<li>✏️ 横軸は通常、学習ステップやトークン数など、学習の進行度合いを示しますが、この図では各点が異なる学習段階や異なるモデル状態に対応していると解釈できます。縦軸が検証損失です。</li>
<li>✏️ <span class="highlight">青い点</span>が、実際にその $D_{ood}$ データセットで評価した検証損失の値です。</li>
<li>✏️ <span class="highlight">オレンジ色の線</span>が、$L_{D_{ood}} = \lambda_1' L_{D_{pt}} + \lambda_2' L_{D_{cpt}}$ という形で、$D_{pt}$ と $D_{cpt}$ の検証損失から計算された推定値の軌跡です。各サブプロットには、その推定に使われた具体的な係数 $\lambda_1'$ と $\lambda_2'$ （論文中では $D_{pt}$ と $D_{cpt}$ の係数として表示）が数式で示されています。</li>
<li>💡 <strong>ポイント</strong>: オレンジ色の線が青い点群をうまくなぞっていればいるほど、線形結合による推定が正確であると言えます。</li>
</ul>
</div>
<div class="two-column">
<div class="column">
<div class="bubble-box">
<p>例えば、左上の「StackExchange」のプロットを見ると、オレンジ色の線が青い点群のトレンドを非常によく捉えています。式は、$L_{\text{StackExchange}} \approx 1.853 L_{D_{cpt}} - 0.546 L_{D_{pt}}$ となっており、$D_{cpt}$ の損失に大きな正の重み、$D_{pt}$ の損失に負の重みが掛かっていることが分かります。これは、StackExchangeのデータ特性が、$D_{cpt}$ (Knowledge-Pile) に近く、かつ $D_{pt}$ (FineWeb) とは異なる傾向を持つことを示唆しているかもしれません。</p>
</div>
</div>
<div class="column">
<div class="bubble-box">
<p>一方、「Stories」のプロットでは、$L_{\text{Stories}} \approx -0.516 L_{D_{cpt}} + 1.409 L_{D_{pt}}$ となり、$D_{pt}$ 側に大きな正の重みが、$D_{cpt}$ 側に負の重みがついています。これは、Storiesデータセットの特性が $D_{pt}$ (FineWeb) に近く、$D_{cpt}$ (Knowledge-Pile) とは逆の相関を持つ可能性を示唆しています。</p>
</div>
</div>
</div>
<p>🔍 図21全体を眺めると、多くの $D_{ood}$ データセットにおいて、この単純な線形結合が驚くほど良好に実際の検証損失の傾向を捉えられていることが見て取れます。これは、継続的学習の文脈で、主要な学習データセット ($D_{pt}$, $D_{cpt}$) の振る舞いを理解することが、他の未知ドメインでの性能を予測する上で強力な手がかりになる可能性を示唆しています。</p>
</div>
<div class="arrow-connector"></div>
<div class="glass-card">
<h4 class="subsection-title"><i class="fas fa-image"></i> 図22: $D_{ood}$ 検証損失の推定における絶対誤差</h4>
<p>📉 この図は、図21で行った線形結合による推定の「誤差」に注目したものです。具体的には、各 $D_{ood}$ データセットについて、実際の検証損失と推定された検証損失の差の絶対値（つまり、どれだけズレているか）を箱ひげ図で示しています。</p>
<img alt="Figure 22: Absolute errors of linear combination to represent D_ood validation loss" src="ood_validation_loss_absolute_errors.jpg"/>
<div class="note-box">
<p class="note-title"><i class="fas-search"></i> 図の見方・ポイント</p>
<ul class="unstyled-list">
<li>✏️ 各箱ひげ図が、一つの $D_{ood}$ データセットに対応しています。</li>
<li>✏️ 縦軸は「<span class="keyword">絶対誤差 (Absolute Errors)</span>」です。値が0に近いほど、推定が正確だったことを意味します。</li>
<li>✏️ 箱ひげ図は、誤差の分布を示しています。
                    <ul>
<li>箱の中央線: 誤差の中央値（メディアン）</li>
<li>箱の上辺・下辺: 誤差の上位25%点（第3四分位数）と下位25%点（第1四分位数）</li>
<li>ひげ: データの広がり（外れ値を除く範囲）</li>
<li>点（プロット外）: 外れ値</li>
</ul>
</li>
<li>💡 <strong>ポイント</strong>: 箱が全体的に低く、かつ箱の高さ（四分位範囲）が小さいほど、推定誤差が小さく、安定して良い精度で予測できていることを示します。</li>
</ul>
</div>
<div class="info-grid">
<div class="info-card">
<p>例えば、「Math」（Open-Web-Math）や「SlimPajama」の箱ひげ図は、比較的低い位置にあり、箱の高さも小さいように見えます。これは、これらのデータセットについては線形結合による推定がかなり上手くいっていることを示唆します。</p>
</div>
<div class="info-card">
<p>一方で、「Stories」や「StackExchange」は、他と比べて誤差がやや大きめに出る傾向があるかもしれません（箱の位置や外れ値に注目）。しかし、それでも絶対誤差の値は全体的に小さい範囲に収まっているように見えます。</p>
</div>
</div>
<p>🔍 図22は、図21で視覚的に捉えた「推定の良さ」を、より定量的に評価するためのものです。全体として、多くのドメインで絶対誤差が比較的小さく抑えられていることは、この線形結合アプローチの有効性を裏付けています。ただし、ドメインによって誤差の大きさやばらつきには違いがあることも読み取れ、これが冒頭で触れられた「アプローチの限界」や「ドメイン特異性」と関連している可能性があります。</p>
</div>
<h3 class="subsection-title"><i class="fas fa-microscope"></i> このセクションから得られる知見</h3>
<div class="content-box">
<div class="process-step">
<div class="step-number">1</div>
<div class="step-content">
<strong>近似の可能性</strong>: 特定の条件下（$D_{cpt}$ が極端にドメイン特化していない場合など）では、$D_{pt}$ と $D_{cpt}$ の検証損失の線形結合という非常にシンプルなモデルで、$D_{ood}$ の検証損失をある程度近似できることが示されました。
            </div>
</div>
<div class="process-step">
<div class="step-number">2</div>
<div class="step-content">
<strong>ドメイン間の関連性</strong>: 線形結合の係数（$\lambda_1'$, $\lambda_2'$）は、$D_{ood}$ が $D_{pt}$ と $D_{cpt}$ のどちらの特性により近いか、あるいはどのような関係性を持つかを示唆する指標となり得ます。
            </div>
</div>
<div class="process-step">
<div class="step-number">3</div>
<div class="step-content">
<strong>予測への応用</strong>: この手法が使える場面では、実際に $D_{ood}$ で大規模な評価を行わずとも、モデルの汎化性能をある程度予測したり、異なる学習戦略が $D_{ood}$ に与える影響を推測したりするのに役立つ可能性があります。これは、セクション5.2で述べられている「$D_{ood}$ の損失予測」や「$D_{pt}$ と $D_{cpt}$ 損失のバランス問題への帰着」というアイデアに繋がっていきます。
            </div>
</div>
<div class="process-step">
<div class="step-number">4</div>
<div class="step-content">
<strong>限界の認識</strong>: 同時に、このアプローチが万能ではないことも強調されています。適用できる範囲や条件については、さらなる研究や検証が必要です。
            </div>
</div>
</div>
<div class="challenge-box">
<p class="challenge-title"><i class="fas fa-question-circle"></i> 今後の研究への示唆</p>
<p>このセクションの結果は、異なるデータドメイン間の関係性を理解し、モデルの汎化性能を予測するための新たな視点を提供するものです。特に、継続的学習の過程で、どの程度既存の知識（$D_{pt}$由来）を保持し、どの程度新しいドメイン（$D_{cpt}$由来）に適応させるかというトレードオフを考える際に、このような$D_{ood}$への影響を考慮に入れることは非常に重要になるでしょう。</p>
</div>
</div>
</div>
</body>
</html>
